Time,Speaker,Text
0:13,Sanyam Bhutani,"Hey, this is Sanyam Bhutani and you're listening to ""Chai Time Data Science,"" a podcast for data science enthusiasts, where I interview practitioners and researchers and Kagglers about their journey, experience, and talk all things about data science.

Sanyam Bhutani  0:45  
Welcome to the ""Chai Time Data Science"" show the show bringing you quarantine content containing interviews with my machine learning heroes. I am Sanyam Bhutani. If you haven't yet checked out the other podcast that I recently launched Chai Time Data Science News CTDS.news please go ahead check it out if you're interested in finding a short data science news podcast. This episode is all about open source and machine learning. In this episode I interview one of the core developers at scikit learn and at the time of recording and associate research scientist at Data Science Institute at Columbia University. Andreas Mueller. This episode is slightly different compared to all CTDS.show episodes. We talked about how Andreas' overview about open source and machine learning and scikit learn itself has evolved over the years how his approach to creating open source API's, his understanding of open source has evolved over the years that he's been active in the open source community. There's a lot of discussion around scikit learn and I thank you all for all of the questions from the AMA all of which have been discussed. We also discuss learning through materials and Andreas' take on the recent developments in deep learning. Andreas has also been teaching a course of machine learning at Columbia. And we also talk about his, his previous work there, and his advice for learners. This is a new format of episodes on CDTS.show. So please do let me know if you enjoyed it. And for now, here's my conversation with Andreas Mueller. Please enjoy the show.

Sanyam Bhutani  2:46  
Hi, everyone. It's really a big honour for me to be talking to one of the core devs from the scikit learn community. Andreas thank you so much for saying yes to my request."
2:57,Andreas Mueller,Thanks for having me.
2:58,Sanyam Bhutani,"Really excited to be talking to you, so I want to jump right into a few questions. These will sort of skip over a few details, because you've already been seeing most of them through a lot of interviews that I found while doing my research. So I'll have those linked in the description instead. My first question would be got involved in scikit learn or during the early days, how is your contribution changed and evolved over the years?"
3:24,Andreas Mueller,"That's a good question. So I mean, I'm not sure if it's even fair to say I started in the early days in early days for adoption, but probably not in development because most of the algorithms were already there. So it was really already a fully fledged package when I started. And but so I think what has most changed is that while at the very beginning, I had added some like small algorithms, but I was mostly always on like the more maintaining and bigger picture kind of stuff and I think what changed is that right now I'm not as involved with the development most of the time, I'm I was more involved, involved with like finding funding and writing the government document, working on the roadmap, finding people to work on the project. And so it's like, I do more of the like organisational stuff and less of the development. I guess that's like a typical thing. In any organisation like, the more experienced you become, the more you go from, like the hands on work to the more like project management work."
4:34,Sanyam Bhutani,You were able to get your ideas translated into real code through managing multiple people. Do you miss the development days?
4:42,Andreas Mueller,"Oh, yeah, it's like development. This is great. And I, I mean, I always try to get back to it as much as possible. It's like I saw a tweet from someone recently. I don't know. Maybe it was Hillary Mason or something like they were saying that oh, my God, how much I miss development and I was like, yes. Everybody that does a lot of other stuff. Like, we really we started for the coding, because that's the thing that we enjoy. And so one of the things I did is I started this project called dabble, like a year ago or something, or maybe even two years now. And so because that's like very early stages, it's, I can actually do like a lot of the coding there, which is very different from scikit learn where scikit learn moves very slowly and like, it's much harder to contribute. So basically, I created myself a little bit of an outlet to, like, do more of the very quick development, which is a lot of fun, I think. "
5:39,Sanyam Bhutani,"Okay, can you tell us more about dabble? I think it's, it's tru to word, it allows you to dabble with projects. Can you say a bit more about that?"
5:47,Andreas Mueller,"Yeah, sure. So it's still very early stages. But the idea is basically to make a more accessible data science Machine Learning Library that allows you to get prototypes very, very quickly. So I guess it bears some ideas from things like pandas profiling, for example where and some like things individualization community, we are trying to find good visualisations automatically. So first, the idea is to get a good idea of your data set, and then automatically run some simple automatic machine learning things. So a lot of scikit learn allows you to build very complex pipelines, and like, basically do whatever you want. Scikit learn gives you a lot of freedom. But for a lot of practical problems, you actually don't need a lot of freedom. And basically, if you run like some gradient boosting, it's probably gonna work out okay. And so, that will include a quick way to tune things like gradient boosting and SVM and random forests by successive halfling. And there's like basically a built in list of classifier tries and regressors it tries and then it focuses more on doing the pre processing for you automatically trying to detect types, doing some visualisation and then hopefully soon also doing some more model explanation like showing you the relevant metrics and visually, visualising them and showing you like partially pennants curves, rotation, importance, this kind of like debugging tools that you want."
7:20,Sanyam Bhutani,"It's it's sort of a tribute to the running to that industry. They really just use logistic regression, which is good enough. So is it in for the industry so that people can iterate really quickly come up with prototypes?"
7:34,Andreas Mueller,"Yeah, it's it's definitely I mean, I'm not sure if it's just for industry like the same signs. But it's used to iterate very quickly because I think, basically, like, everybody has their favourite version of like the machine learning workflow diagram, where you like, start with collecting the data or defining the problem and so on, and you tried building models you put into production, and I think I'm really having to be a cycle is very important. And so I want to create tools that allow you to to make the cycle much faster, so you can try out something and then maybe go back and collect your data. So we have very good tools for like the model, tweaking and tuning. And that's what cycle load is great at. But then people, I think, spend too much time and trying to build the perfect pipeline for a data set, instead of like, maybe thinking about what is the problem I'm trying to solve? What are the metrics I should be looking at? Can I collect new data? And so all these like, all these other steps of this workflow cycle, they get too little attention. And so basically, I wanted to say, well, this actually, the model building part is the easy part. And so you can try to automate this away as much as possible. And yeah, yeah. So basically, if you do if you try to adjust your regression, gradient boosting, it's probably going to be fine. But really what you should think about what does your data mean?"
8:50,Sanyam Bhutani,"So as if, if I understand correctly, it is the iteration pipeline for software 2.2, so to speak."
8:57,Andreas Mueller,"Yeah, a little bit. So one of the things that it skips is productionization. So it definitely doesn't aim. It's definitely not production ready right now, but also doesn't aim to have this part of the cycle. So it's the, like data scientists exploratory analysis, cycle and not like, like there's a bigger cycle, obviously of going to production, doing continuous deployment, doing monitoring, dashboarding and so on, which are also all very important parts. But they are not what I'm what I'm targeting and basically targeting to things that are already there in the Python ecosystem. So the Python ecosystem is not that much used, I guess in the in this more production raising issues, or the SciPy ecosystem, at least is not. And so I'm trying to like collect a little bit more on the things that were the Python ecosystem is already strong, like normal data analysis and machine learning and trying to put them together to like a simple to use coherent piece."
9:59,Sanyam Bhutani,"Which will allow to bridge the gap between the data scientists and the production team, which for many big industries tend to become different."
10:08,Andreas Mueller,"Yeah, I mean, it's definitely still going to be a gap to be solved. But yeah. Hopefully that the people will then focus less on on the endless tweaking of the model and more on the overall process."
10:22,Sanyam Bhutani,"You've also given a talk on this topic of auto ML, what are your thoughts on it in 2020, I'm biased towards auto ML. Would love to hear your thoughts too."
10:32,Andreas Mueller,"It depends. So there's actually there's quite like different flavours of auto ML and so I'm maybe obviously more on this what's now called classical ML sides of the stuff that scikit learn strongly last the deep learning side. And so there's really a lot of interest in auto ML and deep learning and neural architecture search. And this seems like quite an interesting topic, but it's definitely not my expertise and I can't really say much about like, what's happening there. But in terms of the more classical stuff I'm I'm a big fan and friend of the group at at Freiburg, from Frank and Mateus. I'm not sure if you know these guys, they did smack and auto SK learn. And they went about a bunch of auto ML competitions. And yeah, I really loved they work. And so in one of their work, I found basically if they create a portfolio of good algorithms to test, I love this portfolio approach. It's what I implemented double. Basically they found if you just use extra boost, it's as good as trying to learn trying to shoot over all pipelines, it's I could learn that is kind of shocking to me, but also not that shocking. This is definitely this comes with a bunch of caveats in that, like, they only tried a couple of the pipelines and they only gave us so much computing power and they only use this handful of data sets. So I wouldn't make this claim that this is true. In general, but you can solve a wide variety of problems with a very limited number of, of machine learning pipelines. And so, in a sense, this is a win for autumn out. But it's also means you don't actually need that much auto ml. If there's only a small number of good candidates, then selecting among these candidates is pretty easy. And so that's why I like this, this portfolio approach where basically you figure out a list of good candidates, and then you do efficient search over these candidates. And so even if you just do something like hyper bento success of half things already quite, quite effective. And I mean, then there's like, there's very interesting more fancy stuff there. So great matrix factorization approach. And then there's a bunch of approaches that try to incorporate runtime of the algorithms more. That's something that's not fully explored yet. I think it's very important to look into the runtime of algorithms. But I don't think anyone has found a real solution there. But so I guess my main thought is like, in classical about, it seems like you can get away with relatively simple solutions for like, majority of cases. And so auto ML is good, but it's also going to be easy. I should also caveat this by saying this is for getting like, I don't know, 95% there, you can probably spend much more compute to something much more advanced, actually much more fancy and spend a lot, lot more work and get like the extra percent. But one of the things that I am doing with Apple that I think is true for many practitioners is it doesn't really matter how much like the last percent might not matter as much as moving on to the next product. Like the question is, how much impact can you have if you spent more time on this project, whereas another project Often building a very, very complex solution. And spending more time is not as beneficial as building a good enough solution that's robust and then go to the next thing. "
14:14,Sanyam Bhutani,"Yeah, I think auto ML would also insource allow this so that you don't have to just focus on to picking between models. You can also leave that part automation and focus on other stuff that sort of require domain expertise."
14:29,Andreas Mueller,"Yeah, sure. I mean, I guess the question for me is sort of the trade off the trade off off, do you really want to run a really, like big expensive search over everything? Like how much is the benefit? I mean, if it's, if it's free to do so I guess there's no downside to doing it. Like if you have infinite compute, then you can run the biggest model search. But if you do that, and the outcome is that it's basically as good as what you had to start with then. [Yeah] Yeah, I'm not sure. I'm not sure exactly what a trade off is and how much work you should actually actually put into choosing something as good as possible."
15:06,Sanyam Bhutani,"Understood. Nw I want to come back to open source and API designs, I think scikit learn was sight slightly or actually much ahead of its time in terms of design, because it was ahead of the curve for the machine learning. What are your thoughts on creating good API designs, specially for open source or otherwise?"
15:27,Andreas Mueller,"I'm sure so, I mean, I can't take any credit for most of the scikit learn API, but I think it's by being involved in a project is something I very much came to appreciate. And whenever we are trying to create just a new class or a new kind of functionality, we're always really looking for creating easy to use API's. And yeah, I can I can give some ideas about what my strategy there is. So one of them is really use case driven. So think about what do the users want to do with the API? And how will their code look? So this is a mixture between sort of test driven and development driven developer development. So I really like this idea of developer. Documentation driven development. So there's two things that I think about the API. One is, how easy is it to teach? And then the other one is, how easy is it to use once you understand it? And so if you have something that is like super elegant code, but it's impossible to understand, for newcomer, it's probably going to be hard to sell this. So you want something that is a good compromise between being like, succinct and expressive and easy to teach to understand. So in the beginning, no one will have a mental model of how your code works. It's easy for people to build a mental model of how your code works, they will be less surprised by what it does. So having your interface be easy to explain is really important, then obviously you also want it to be easy to use. Maybe one of the things that at least now at scikit learn is something that's very much at the forefront is usually that all the discussions are about API, and not about the implementation. Like scikit learn implements some, like, somewhat [?] like numeric optimization stuff, right? But the implementation is basically always the easy part. The hard part is how to make the interface because the implementation you can very easily change later on the interface. It's very hard to change an open source package. So often, we spend much more time on the interface or on the implementation and so what also what should the behaviour be like? Let's say the user specifies this parameter of commit this combination of parameters. What is the expected outcome does the expected outcome make sense? Like, how do we document this? Again, this is the extendibility. Like if we add a new functionality to to an existing object, we probably need to add a new parameter or a new option. How can we make this option discoverable? And how can we make sure that it plays nice with all the other options? And there's like, and it's like things are really silly. Sometimes, or you think they're silly. There's been a very obvious thing is in the one hot encoder, you often white one to limit the number of categories to like some maximum, let's say, you have hundred thousand categories, but you only want to encode the 50 most common ones, like you assume there's a long tail, you put everything else in the other category. And that's like a very obvious thing to do. And it's something a cyclone should have, but does not have right now, hopefully has it in the next release. But the reason is that the one hot encoder already has so many options. That's very hard. To make sure that like all the different combinations the user can specify makes sense. So what happens to unknown categories? What happens if a user said, well, I want to drop one of the categories? There there's like lots of weird edge cases that you wouldn't think about. Like, how does this like combined with having missing values? And so things that seems simple can be quite gnarly in the implementation details."
19:28,Sanyam Bhutani,The challenge really becomes how do you come up with how would people intuitively approach this? How would you assume others intuition about the sorts?
19:38,Andreas Mueller,"Oh, yeah, but that's right, the documentation, right. So I mean, we we rarely write out the full documentation. But we are all I guess. We know how we would write to a document. We've written enough documentation that we can think about it. If you haven't written that much documentation, just write the documentation. It's not that much work. And like, then maybe have one other person read the documentation at least I mean, code reviews are obviously super, super important for API design. So I just said the model in scikit learn is that HPR needs to be reviewed by at least two other core developers. And so like, if your code is not reviewed, then probably your code is bad and your API is bad. Sometimes you don't really have a choice. If you don't have anyone to review, but like, you should review the code. And you have people like, as like, one, ideally, two people look at your code. And so if you write documentation, so what else can read it and understand it, then then, you know, sort of what kind of mental image that will form maybe you can know perfectly, but if you have a hard time writing, understandable documentation is probably a problem."
20:48,Sanyam Bhutani,"That again, hints back to your documentation driven development as you would say, now coming coming to the evolution of the framework cycle, sort of evolved slowly compared to other frameworks, at least if I may say so and the shift in the community. How has your opinion teens in the shift from classical algorithms towards now deep neural nets? Like really make transformer models that have been coming up?"
21:14,Andreas Mueller,"Yeah, so first of all, just a slight pet peeve of mine. It's like, it's kind of funny that people say this is classical ML and deep learning. So a new thing. You know, it's I can start doing this the other way around, right? Neural networks are much older than like random forests and gradient boosting. [Correct] But anyway, so I guess in the modern form, they're different. But so scikit learn is slow for two reasons. One reason is, well, let's see three reasons. First reason is we don't have enough resources. To review all the code we would like to review mostly. Nowadays, it's easier to read resource more easy to get resource to write code, but it's harder to get resources to review all the code that we want to review. The second is we want to be careful. A lot of people rely on this so we don't want to make any, like, quick, drastic changes, we, and that's a reason to be conservative, right? We're intentionally conservative. And the third is that, like, compared to what's happening deep learning community, there's not that much super exciting stuff happening in like the classical algorithms. So obviously there's like, probably now there's like thousands of papers ever written Europe's icml. But the question is, how much impact will these have for practitioners? And so, it's, yeah, it's often tweaks to existing algorithms. And, like, it's quite rare that it's, there's like a new thing that's gonna be a big improvement that's gonna clearly deliver to practitioners. Like there's stuff that happens to like the grain boosting of plantations, make them bit faster. But nicer, it's cool. There's some stuff in like automatic success of huffing, and that's kind of cool. But there's not like there's not big splashes. There's yeah, for example, the the whole transformer thing is like, still relatively new, and clearly made a big impact, like but but like, I don't know what the last thing is that made a big impact in like classical ML it might be hyper band, which is, I don't know, how old is that now? Five years something like that. Yeah. So I mean, there's there's a couple of things happening in like the interpretable ML space but also there. So I guess like chap is newer than that, and people are quite quite interested in chap. That's also the the sort of scikit learning wants to just add, okay, we just one more reason I should mention is cycling wants to add things that kind of stood the test of time because otherwise, you end up with a lot of code that's not maintainable. If you add everything that beats the benchmark, you will end up with hundreds of algorithms, most of which are out of date. [Yeah] So I actually had a conversation the other day with someone working on Julian ?Shimbun is also Machine Learning Library for c++ started mostly focusing on kernel machines. It's it's a pretty great library, it didn't get as much adoption as I could learn, they might start episodic a little bit earlier about the same time. And so they had a slightly different policy. They added a lot of state of the art algorithms, so algorithms that were just published. And so that's nice because you can reproduce results in current paper so they had a lot of cool stuff about like multiple curdle learning and I used that in my thesis. But the problem is now the person that computed this algorithm has long graduated or went to industry or something, no one understands the code. And no one's really interested in the code because there's something new and shiny out there. And so you end up with a lot of code that's very hard to maintain, it's maybe not even worth maintaining. And so psych alone basically decided we're not going to do this. At some point, we wrote something in the FAQ, it says, we only accept new algorithms if they're three years old and have 200 citations plus. And so this is somewhat arbitrary. But it got us a lot less emails that say, hey, can you implement our fancy new algorithm? Because the answer is just no, here's the criteria. It's very clear."
25:40,Sanyam Bhutani,"There's also I also, unfortunately, spend a lot of time on Twitter. The ML Twitter community is great Denny Britz had shared this tweet I think, if if the authors can share the git log of their new paper and the new sort, it just becomes one line of sorts. For some of the recent SOTA architectures atleast."
26:02,Andreas Mueller,"Sorry, can, can you say again?"
26:04,Sanyam Bhutani,"By sort I mean state of the art. So Denny Britz had shared this tweet that if some authors can share, they can just summarise the paper with a git log of the difference. It just becomes one line of difference for state of the art new model."
26:19,Andreas Mueller,"Yeah. And that yeah, I mean, often it's the small tweaks, right? I mean, even even things have made a big splash, like, a drop out is not that many lines of code, right? It's just it's a it's very hard to say which line is the right?"
26:37,Sanyam Bhutani,"Certainly. Now, coming to your mission, your mission on your personal website is to create open tools, open source tools to lower the barrier of entry for ML apps. How, how much of that goal would you say you achieved? How far are you from, let's see, just checking, checking that box off of your to do list."
26:57,Andreas Mueller,"I mean, it says make it more accessible, right? So you can always make it more accessible, I think. So, I mean, there's been a lot of progress that I cannot take any credit for, right? There's like, so the second huge community that has a lot of amazing work going on. So if ever this books get checked, it's definitely not due to my work. So, but there's things that I don't like. And basically, I wouldn't say we're close to what I want. One of the things that is a pet peeve of mine is that in scikit, learn, it's very hard to deal with feature names. So if you build a complex pipeline, it's very hard to understand what's going on. And I hope we're gonna solve this this year. I probably say that last year already, maybe even the year before. And I also think there's an area are there's like a couple of stupid things in scikit learn like that where that should be be fixed. There's also definitely some friction integrating between scikit learn and pandas and like there's things in visualisation. So I'm doing some visualisation in tableau. And that's based on matplotlib. And I think it's, it's helpful because like, there's not really that many machine learning focused visualisation tools. So if you look at seaborne, seaborne, that's sort of a got a lot of great statistics plots, but actually, it's kind of tricky to the plots that I want to do with it. Because I'm not really target audience. And like, seeborne is expanding a little bit more to like what wide form data and so that might help, but I don't think it's really the rights dried venue to for machine learning visualisations and for like high dimensional data. There's yellow brick, which is doing some visualisations on top of scikit learn, which is cool. We're also now adding a bunch of visualisations to, to scikit learn itself. So there's like, I mean, it's super trivial stuff like this plot ROC curve and pluck a pot AOC and pluck confusion matrix, which are like very easy, but also, again, you need to get the API rights and everything. And, but it's also it's also nice if you don't have to write 10 lines of matplotlib code to get the nice part of ROC curve. A bigger picture issue for the ecosystem is sort of what's the right, right plotting library. matplotlib is going through some restructuring, they want to change the data model. But there's also thing about like, okay, everybody is now on many people are doing Jupyter notebooks. So maybe something that is more directly integrated with the web would be better for interactivity. interactivity is a little bit sluggish with matplotlib. Because of the architecture, I think I'm not sure if it's fixable, maybe it is fixable. But if you look at the interactivity of like, Altera or plotly, or bouquet there, it's like, much quicker. And so then the question is like, well, a lot of these libraries are all built on matlab. Do we want to swap out the back end? And if we want to swap out the back end, like which of these ones do we want to pick? Like, is it a good idea to have as a community move to Altair or move to plotly? or move to bouquet? And so I don't think there's an answer. I'm definitely not the person that's going to answer this question. But I think it's something where, if we don't know how to plot well, then we can't say we are accessible."
30:57,Sanyam Bhutani,"Yeah, that's a great point. Uh, the next question comes from an anonymous Redditor and Redditors love controversies. The question is the biggest controversy you've seen in the scikit learn community?"
31:09,Andreas Mueller,"Biggest controversy? I don't know. Do you mean, would you say between developers or in the community? "
31:18,Sanyam Bhutani,"Let's say, between just in the community."
31:23,Andreas Mueller,"In the community, or there's like, it was this amazing flame war about logistic regression recently. So it was kind of started by a tweet by Zack Lipton. Who's a really cool guy, and I really love his work. But basically, he tweeted that, well, how many papers have run results because they're just regression. It's like a lotus penalised. And I don't know if you saw that. And, yeah, and so that was like, several developers muted the thread because it got so bad. And yeah, so that was fun. So I mean, the main thing is, if you're a statistician, you don't want to just rushing to be penalised, and they said oh the scikit learn developers like they didn't know what to do if they got NANs or correlated features or something like this. Obviously, the point is that we don't want to get Nan's if we have correlated features, which is why we regularise. So if you have a perfect predictive prospective, then regularising makes a lot of sense, like from predictive and from optimization perspective, from a statistical perspective, it makes no sense at all. And so I think this is something that comes up in a couple of places is that people that have more statistics, or inference view, they're like, very put off by scikit learn, because this is really not the problem we want to solve. But in Python, there's also no package that is as mature as I could learn that solves these problems. So there's stats models, and sets one says a bunch of great things but sets models doesn't have that bigger community that big of a developer base, and so so, if you look forward to regression, it's very likely you'll find second lag before you find stats models. But if you're a statistician, then this is not what you want."
33:09,Sanyam Bhutani,Makes sense. You've been involved in scikit learn for a while. When do you think when do you envision 1.2 of the version of the framework coming out?
33:19,Andreas Mueller,"This is also a controversial question. But this is more among developers. When do I envision? So we always say next year, every year, maybe. So the question is a little bit what is the;"
33:33,Sanyam Bhutani,list from the framework in in your opinion.
33:36,Andreas Mueller,"Exactly so the question is, like, do we want to make a list of requirements? Actually, I wrote a grant to NSF that had one of the to do items, figure out how to do a 1.0 release. Unfortunately, it didn't get funded because they said second version development doesn't help computer science research. And so what anyway, NSF has it has interesting opinions on open source. Anyway, so I keep arguing with them, but well, we'll see maybe something comes of it. Wait, so 1.0? The question is, do we want to make a list of features? And then say, once we have these features we want to release? And if so, what is this list of features? Or should we just not care about the list of features and just release? So for me personally, one of the things that I really care about as these feature names thing, so I feel like if I create a pipeline, and I have a logistic regression, at the end, I should be able to figure out what do the coefficients in logistic regression mean? If I don't know what their coefficient the logistic regression mean, that's like, that's a really bad situation that actually you can easily build a cyclone pipeline where it's really hard to figure out if you have like a computer and 100 encoder and like calling transformer and then a feature selection or whatever, you don't even need to be that that tricky. If you just have an imputed in one hot encoder, it can be already tricky, tricky. And so this is something I'm sort of passionate about. But I'm but maybe we also don't want to want to block. for that. There's actually there's an issue on GitHub. That is, what do we need for 1.0 release? But it's a little bit unclear. And probably what it needs is a dedicated push, which is why I wrote this this grant proposal, but I don't think any one of the developers is currently making this push. So basically, if I say, okay, my thing is now I want to do a 1.0 release. And if I spend my energy on rallying the other developers behind that, then we could probably make it happen this year. But or if any of the other core developers does that right, the question but the thing is, this is not anyone's like really high priority, and so It's probably not gonna happen this year unless someone comes along and makes it as their priority."
36:08,Sanyam Bhutani,"Now, this is a question but I see like many okay boomers making a mistake as people skip over SK learn because deep learning is cool right now, just the newcomers, if I may just jump on to deep learning just jump on to transformer models without even knowing what a scalar is, unfortunately, for some of them, what are your views on that?"
36:29,Andreas Mueller,"I mean, it depends a little bit on what they want to do. Right. So I think there's only a very small subset of machine learning problems that are best solved with the transformer model. And, but if you want;"
36:48,Sanyam Bhutani,Like deep learning broadly speaking maybe like;
36:51,Andreas Mueller,"Yeah, so like if you if you want to do like, applications, it's probably a bad idea. If you want to do well, I guess if you want to do research, it depends if you want to do more fundamental research it is a bad idea. But there's also people who build state of the art deep learning, like solutions without like a strong, like computer science background, and so you can do things in the space. And you can probably like, maybe if you're lucky, put, push to say, I've got four words. But if you want to solve problems, that depends on whether it's the right tool for the problem. And so, and again, this goes back a little bit to what I said earlier, I'm building the model is only a very small part of, of the process usually. So if you want to, if you have a problem, and let's say, with logistic regression, you get 90% accuracy and with deep neural net you get, like 9% accuracy on this data set. But probably, if you spend, I don't know how many weeks making it up, or let's say, maybe at least days to make the network work, then probably You're wasting your time because there's an issue in your data set. And you should have probably formulated the problem in a different way. And so by, but figuring this out was also going to be harder for the deep neural network than it is for a logistic regression model. So I think you're missing out and you're slowing yourself down. If you're interested in actually solving problems effectively. If you're interested in winning on Kggle. Maybe this applies less yet, maybe maybe you can just take the someone else's deep learning code and then tweak the parameters or maybe add some idea there and it'll work. But this is sort of depends a lot on what do you want to do. Right?"
39:03,Sanyam Bhutani,This also allows me to segue into another interesting aspect of your journey. You moved from industry back into research. Why did you decide to make the transition? And did that allow you to have an interesting perspective on software in industry versus academia?
39:23,Andreas Mueller,"There's so oh my god, there's so much in this. Okay, when will this be aired? "
39:33,Sanyam Bhutani,"Uh, around two weeks from now."
39:35,Andreas Mueller,"Okay. So then you all know that I'll be joining Microsoft. In the middle of June, meaning I'm going to go back to industry [Okay] And so so that's so I went back and forth. So basically, after a PhD, I went to Amazon for a year then I went to NYU, then Columbia, and I'm going to Microsoft. And so I think one of the things that I really appreciated about academia was the freedom to work on whatever it wants to work on. So I feel like my work on cycle learning, machine learning has a lot of impact. And I felt like even if my things get productionize at Amazon, I've have less impact than if I work on scikit learn at least the kind of impact that I personally care about. And so that's why I went to academia because I had the freedom to work on these problems. The reason why I'm leaving academia again, is basically it's hard to get funding for the kind of work that I want to do, and it's hard to get credit for it. So I'm on a soft money position at Columbia right now meaning that I have to find money. To pay myself and to pay my group. And that's a lot of work. And so either I would have to keep up doing that work indefinitely, or I would have to become a professor to become a professor, I would actually need to change what I do quite a bit and do much more research and publishing. And then I could become a professor in maybe six or seven years. And so that's, um, yeah, so going to industry back again, I guess, no, no, my situation will be different than when I was at Amazon before in that now. I'm sort of more senior. And so now, people will let me do more what I want you to do, and they actually they hire me because of my open source work. And because of my position in the PyData ecosystem, right? They want me to work on these things. Still, I assume that I'll have less freedom than that. What will happen if it didn't go in academia, I can do whatever I want, as long as I find money for it. So there's there's pros and cons to that, like, basically security by a freedom and also somewhat the the kind of access to resources. So at Columbia, I think, I mean, I was actually quite lucky. I have two amazing people working with me and Nikolas, and Thomas fan?, and so I was able to fund them, and they are doing amazing work. So they're all doing all the programming. I'm not really doing any of the programming. Nikolas did the history of gradient boosting I don't know if you saw that. And Thomas did so much work individualization into scoring and debugging and infrastructure. And so yeah, so, but, but actually to people is a lot of resources in academia. In industry, it's nothing. If you look at what industry teams are, they have like 10-20 people working on something and so on. I mean, it's this also makes it difficult, more difficult to coordinate with open source teams. So if you have part of a team that's in industry, part of it is open source. It's it's different. There's like a quite a difference between how the Apache ecosystem works and how the PyData ecosystem works, for example. Um, but still, I think we, it needs to figure out better funding models and I think one of them is becoming more directly connected to industry. "
43:41,Sanyam Bhutani,"Okay, if I may say it, this is a little ahead of time, but Microsoft is going towards a big push making big push towards open source. Will you continue doing open source work? Can you share a bit of that ahead of time?"
43:56,Andreas Mueller,"Oh, yeah, I will definitely continue to do open source work there. And I mean, there will be a balance between my personal project and coordinating between Microsoft and the PyData ecosystem. So one of the big things that my role will be, how can Microsoft product and product on Azure and SQL product and so on? How can they help the PyData ecosystem? And how can they integrate well with the PyData ecosystem? "
44:24,Sanyam Bhutani,"Okay. I want to discuss a area that you were active in until recently, teaching at Columbia University. What courses did you teach there I'm actually midway through your latest 2020 version of the course and would highly recommend it to the audience, but any things that you enjoyed and any mistakes you recognise that students make while doing data science ecosystem, or machine learning courses?"
44:50,Andreas Mueller,"I think there was a lot of questions. So the course basically only ever taught this course also taught a project capstone course and I taught the applied machine learning four times. And so yeah, check it out on YouTube. It's there to 2019 or 2020 version it quite similar."
45:06,Sanyam Bhutani,And this is there on your channel right.
45:09,Andreas Mueller,Yeah.
45:09,Sanyam Bhutani,Link would be?
45:11,Andreas Mueller,"Youtube.com/AndreasMueller, and it's on YouTube, it's all free. There's also slides and you can look at the homework. What I'm doing right now, actually, and I don't know how long it's gonna take me is I'm trying to make the course into a new book. And so hopefully this will be even more accessible. So we will have both the videos and the book. So currently the book that I have out with O'Reilly, the introduction to machine learning with Python. It's very introductory, I think, I like it, but it's a little bit outdated. And it's much more introductory than the class I have on YouTube. And so basically, I want to create something that's a little bit more on the level of my class at Columbia, but also still doesn't require background in linear algebra and statistics."
45:58,Sanyam Bhutani,"Timeline on when that'll come out, when can we expect that?"
46:02,Andreas Mueller,Well ideally I would have it come out before before I joined Microsoft but I don't think that's realistic. So like it depends on what draft version like okay I really want to do this summer [Okay] and my goal is for for this to be freely available so as Jupyter notebooks and as HTML online.
46:35,Sanyam Bhutani,That'd be amazing.
46:37,Andreas Mueller,"okay so but coming back to your other part of your question is like common mistakes and so this is more mistake a song the project's course which is well basically all the things that I said everybody mistake most people make is like not thinking about the bigger workflow tweaking to model too long not looking at the data not doing visualisation well in particular, there were many projects where students were working for weeks on a deep learning solution. And they told them, try retrogression tried rich retroression, tried regression. And then after two months, they're like, oh, we tried retrogression. And it's better than our lstm. And I'm like, yes, that's because you lstm is not working and you have no baseline. Figure out what is the baseline, what what will be the performance if you do constant prediction, what will be performance? If you do like the silliest baseline, I wasn't really saying you should use retroregression to solve this problem. It was like a time series prediction problem. Probably retrogression is not the right solution, but it will give you a baseline to tells you is my complex model actually working or not? And so they jump to something really complicated. That was really hard to make work. And clearly it didn't work because the really simple solution beat it. So that's a very common problem."
47:55,Sanyam Bhutani,"Now this is an issue that I think you've already addressed in I think all of your previous interviews but the gender bias in open source community and even in the tech community widely speaking. What are your thoughts? And how can we cut down on that you have already contributed to so many sprint's with WiMLDS I believe, but what can we, other developers from everyday contribute that that might be helpful?"
48:22,Andreas Mueller,"This is a very serious topic and a very hard, hard problem, I think. And there's there's issues at so many levels. So the issue that I guess I am focusing on is on the developer level or in the core developer level, so we have no female. Oh, sorry. We have one female core developer. Now they'll call it scikit learn right now, I think, but that's like that one out of 20 or 21. It's bad. And SciPy is high ecosystem projects are about as bad. And so one of the things I'm trying to do with the sprint is engage more people as developers. Yeah, I don't think I'm doing as good a job with that as I should. But so probably one of the parts of the answer is mentoring and engaging with people. It's really you get the best outcomes, if you long term engage with people on like a one on one basis and build like personal relationships. And, but there's also obviously other parts of the pipeline that are broken. Like if you look at the CS degrees, it's actually quite interesting of the two programmes I was involved in, you could see that in the like, there's a very large foreign student population. Of course, it's like maybe 50 or 60% of the students in the data science at Columbia and at MIT, you were Chinese, and in from Chinese students, actually the gender ratio is pretty balanced. That's You can look at the next big population is Indian students, there's maybe like 30%. It's less balanced. Maybe there's like 20% women or something, or 30% women. And then if you look at the US students and all the rest of the world, it's really terrible. And it's like, I don't know, 10% or less. That's, I thought that was quite interesting, in that there is such a big discrepancy between the different countries. So it seems to be at least, okay. Obviously, there's not a perfect measure, but it seems to be more of an issue in the US and Europe than it is in China or even in India, which is interesting. But so there's like, yeah, on the CS students data science students side on the developers in open source yeah, so I think for the overall culture, I think maybe what might be important is like, culture shift. So I'm not that involved with a lot of like the coding tech community, but I think there's still a lot of the like, bro culture that you see in Silicon Valley that will definitely disincentivize like women from from joining these companies. And so no matter what environment you're in, make sure it environment environment is inviting. maybe ask people why they don't join the community or your company, or why they leave what the problems are. It's often not obvious. So I've seen this it's like have learned that the the values are different. Partially or the way people are communicate are different and people have do advocacy and diversity tell me this. There's so you might do things to turn away people that you're not aware of, because they interpret something that you do in a different way. And you need to make yourself aware off and you need to change your behaviour that's driving people away. There was a really great talk recently, when I was at my God, I forgot what it's called. Was it the more??? Sloan data science summit? I think it was called that still. Maybe changed its name, but oh, no, no, no, it was the Chan Zuckerberg. Yes, sorry. I go to many data science summit's. It was Chan Zuckerberg has this amazing essential open source software programme where they fund open source projects, both that are like core signs and those that are like biomedical and there was a really great talk about diversity and I, I'll send you the link and you can include it maybe in the description or something."
53:04,Sanyam Bhutani,"Definitely. Now you've already given us so much broadly speaking if I may dare say speak on the data science community's point of view, but what's your favourite activity outside of tech? What do you enjoy?"
53:24,Andreas Mueller,"So this is probably hilarious, but right now I'm really making a lot of bread like a lot of people in the US. Yeah."
53:33,Sanyam Bhutani,"The non pandemic days, what do you;"
53:36,Andreas Mueller,"I actually I actually bake bread and I used to bake bread on non pandemic days [Oh okay] but it's very hard with your working day. So now I really got back into it. That I'm reading and photography, I guess. Sometimes I'm giving a little bit but not that much. Mostly I'm just reading some sci fi novels and and like take pictures of my friends and stuff like that."
54:03,Sanyam Bhutani,"Okay, this this might really be tough question, what's your favourite game of all time?"
54:09,Andreas Mueller,"What's my favourite game of all time? I mean, so I used to play Starcraft one competitively in [Starcraft one?] Starcraft one, man. So this is definitely one of the contenders."
54:26,Sanyam Bhutani,"Let's pick two, one of all time and maybe if we were to pick one today the only game that you're allowed to play."
54:33,Andreas Mueller,"The only game so I really love world builder, like no sorry gene ??? wealth building so okay, no, I can't I can't pick one so I'll have to I'll have to listen to me rant about the games I'm playing and do ask for this yourself. And so I'm for Tyler was super obsessed with city skylines with his like sin cities plus plus, but he wants to have to manage the traffic and the traffic lights, no less this stuff. It's super intricate and it's amazing. Then a light factory recently and satisfactory, which are both factory building games, and research management, which is great. And then one one game that I really enjoyed. That was like me not playing games for like several years. And then I came. So I was like, okay, what is the best game right now? And I looked it up and well, basically death cells one. And so I played death cells for like a month because it's actually it's not a kind of game that I usually play because they usually play more strategy. But it's such an amazing game. It has so many things that are done so well. And I was like, oh my god, indie games have gotten really, really good."
55:39,Sanyam Bhutani,"Okay. If I do ask you one final question, what would be your best advice to beginners who are looking to contribute to the open source or let's say SK learn framework."
55:52,Andreas Mueller,"So one of the things is maybe contribute to something you're using and that you're passionate about. Also, think about the reasons why you want to contribute. community benefits most for people that will stick around. So if you do like a what's called a drive by contribution, so you send a PR and then gets burst and you never come back. That's can be useful, but it's usually not as useful. That someone that sticks around having people that really stick around even if you're like, not that advanced now, or don't have that, like really super strong machine learning skills. If you stick around you will be really useful as someone that can review pull requests and it can like help the community move forward. Also, yeah, pick something you care about. Maybe I would actually say, depending on your skill level, maybe don't pick psychic learn because tech alone is actually quite hard to contribute to. we tackle issues as easy issues and simple issues and good first issues. And they might be good but second learn is moving relative slowly. And so I mean, I'm obviously quite passionate about it. But if you want, you know, a quick return and you want to have something that moves quickly and to where you, then maybe a smaller project or a newer project might be better, like, the way I developed Apple is like 100% different in the way I do. And so yep, pick, I guess, pick what you want to contribute to some somewhat carefully. I mean, maybe it doesn't matter so much for the first pull request. But if you want to get wealth in the community, see what the community is like. If you're frustrated by things being slow, then scikit learn might not be the best for you. Of course, we can be really slow. If you have a lot of patience and to really care about machine learning algorithms, then scikit learn might be great. The other thing is maybe the biggest mistake that I see people make over and over again. Again, is they start with something big. If your first pull request to any project should be something small, and if you're new to open source maybe can be something trivial, though, make sure it's not something that annoys them. So personally, I love pull requests that fix typos in documentation. "
58:18,Sanyam Bhutani,I think that's also how you started initially?
58:20,Andreas Mueller,"That's how I started. Yes. And some projects might think it's annoying, I don't or a fix. I also fix a lot of pep eight right now, we don't really want pull requests that just fix fix pep eight issues, because they break like they break merchantability If Allah???? prs maybe you might have conflicts and so on. So the, you should find something that's very easy, but you're relatively certain that the people want it. Or it can be like a small feature that you really care. Let's say, I always wish they had x and you add, you're asked to get one axe and if they say yes, then you give it but partnered with something small, don't try to add a new feature because it will take a long time, it might not be in scope. And also, it's quite tricky for the maintainers. If they don't know you, and they take a whole bunch of code from you, are you going to come back to maintain the code or not? Basically, there's a great write up, I think, by Brett cannon??, it talks about the box of puppies that says like, basically, pull requests or a contribution is like a box of puppies, saying like, well, you give it to some of them, it might be cute now, but in the long term, you really need to have to take care of that. And sort of the taking care of it is a lot much more work than giving the box of the puppies and so trying to so if you do something small or if you fix something in existing code, that's much more likely to get the trust of the maintainer, then adding a giant feature."
59:55,Sanyam Bhutani,And that is equally important work if if I dare say so.
1:00:00,Andreas Mueller,"Which one I mean, the fixing stuff is like, at least as important. It's, it's much harder to get people to do it. One of the reasons we kind of stopped doing Google Summer of code with scikit learn is, what is two reasons. One, it's very hard to scope projects that are three months long for scikit learn, because most of the work isn't like that, because we don't add big things. And the other thing is that even if they add the thing that are actual bottleneck is reviewing the code, not contributions, we have a bunch of big contributions that are not reviewed lying around. So having a junior developer, create, or even a senior developer create a huge pile of code is not useful to second learn, because there's no one there to review it. And so, yeah, the question is really, what is the what is the value add for the project? And if the project says, oh, we really need this huge thing, but we just no one has time to implement it, but this would be the best. The best thing ever. Someone built this huge thing that maybe you build a huge thing. But maybe don't do as the first thing, maybe do something small to learn the culture of the project, learn the process to learn, like how their ci is set up, what kind of guidelines they use, and so on."
1:01:16,Sanyam Bhutani,"So, I missed out on one common community question that I'd like to ask now. What are your thoughts on rapids or the shift towards GPU based frameworks?"
1:01:29,Andreas Mueller,"Also, good question. So I haven't worked much with rapids. From from what I understand the game, quite quite good engineering team over there working really hard. I think it's a little bit it depends on your goals. So for a lot of algorithms, you don't get as much of a speed up so I think what I heard for gradient boosting is you get like three x speed up, realistically, maybe a five x speed up. And then you can ask yourself, Is it worth having extra hardware to get three x speed up? And I mean, that's just something you have to evaluate for yourself, like, what are the costs and benefits? Like, if I'm doing this on the cloud, it clearly depends on the cloud prices, like if the cloud is subsidised by Nvidia, and they it's free to get diffused. And sure, I'll take a free three times speed up, right. But if it if it's more expensive, and that's not your bottleneck, then maybe it's not worth it. And it depends a lot on the algorithm and our data set. Like if your data set doesn't fit on the GPU, for example, like then, but it does fit in RAM, maybe it's, it could be that it's faster to do it in RAM, then to do it on GPU. And so it depends on the day test set size and it depends on the algorithm how how much you gain? And then it depends on your particular situation. Is that game worth it? So this is the choice you should make as a user, as developer, we need to make a different choice, which is do we want to support GPUs and scikit learn doesn't support GPUs basically, because we don't want to deal with the pain that of like, dealing with CUDA dependencies also would mean that basically rewriting each algorithm to have a CPU and a GPU version. And so if we go out if the rapids guy do that, that's not nobody's doing it. I think I think it's great to have it. I think it's good to have an option to do this. I don't think it's necessarily the right solution for all the problems."
1:03:50,Sanyam Bhutani,It's also a future facing direction of movement.
1:03:56,Andreas Mueller,"Yeah, but I mean, I don't think the trade off I'm not sure how much trade off will change. I guess it depends on how the pricing and cloud providers will change. But I guess the and that will change the price of the harbour potentially what that's like. Yeah, I don't know. I think it's very hard to forecast because there's a certain direction between, I guess how many, like whether Google puts the GPU in all of their, or Amazon puts a GPU in all of their boxes, and it's probably related to whether Nvidia will buy will build new factories, and then we'll write to how how expensive it is, but there's probably a baseline price off billing GPUs that you can't get down. And so actually, I I don't know how, how the pricing on this works, but I think the cost benefit analysis is what makes it breaks it right."
1:04:53,Sanyam Bhutani,Makes sense. Before we end the call. I'll definitely have all of your profiles linked in the show notes. But what would be the best platform to connect with you? I know there's a YouTube channel as well. There's your website and your Twitter handle anything else that you'd like to mention?
1:05:08,Andreas Mueller,"I mean, any scikit learn related things, go to the issue tracker. Or if you just have a question, maybe go to mailing lists. You can probably find my email. So I guess generally, I don't really like answering questions per email, because if everybody sends me an email, then I don't do anything else. So if you couldn't find the different platform to ask your question, that's, that's probably works well. If you if you want to engage about something that definitely send me an email, if you have ideas, I'm happy to email us probably the best way to reach me. But the more I can take away from my inbox, the better."
1:05:48,Sanyam Bhutani,"Okay. Andreas, thank you so much. On behalf of the community for all of your contributions and advance congratulations on the move to Microsoft."
1:05:58,Andreas Mueller,Thank you. Thank you so much for having me. This was this was a lot of fun. A lot of great questions from the community.
1:06:02,Sanyam Bhutani,Really enjoyed it. Thank you for your time.
1:06:11,Sanyam Bhutani,"Thank you so much for listening to this episode. If you enjoyed the show, please be sure to give it a review or feel free to shoot me a message. You can find all of the social media links in the description. If you like the show, please subscribe and tune in each week, to ""Chai Time Data Science."""
