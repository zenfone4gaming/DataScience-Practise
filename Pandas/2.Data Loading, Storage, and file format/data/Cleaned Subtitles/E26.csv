Time,Speaker,Text
0:13,Sanyam Bhutani,"Hey, this is Sanyam Bhutani and you're listening to ""Chai Time Data Science,"" a podcast for data science enthusiasts, where I interview practitioners and researchers and Kagglers about their journey, experience, and talk all things about data science.

Sanyam Bhutani  0:46  
Hello, and welcome to another episode of the ""Chai Time Data Science Show"", I'm really excited to be interviewing another person from hugging face in this episode: Victor Sanh, who's a research scientist at hugging face. This is part two of Second release of the hugging face series. Do check out the interview with Julien Chaumond, who's the CTO of hugging face if you haven't or if you're interested. In this episode, we talk all about Victor's journey into the field of NLP and machine learning broadly speaking, and delve more into research at hugging face especially distil BERT which is the most recent research by hugging face we also talk a lot about open source and research in general, Victor shares his amazing view points for NLP research in this interview, so I'm really excited to be sharing this. Without further ado, here's my conversation with Victor San, Research Scientist at Hugging Face, all about distil BERT and research in NLP, please enjoy the show

Sanyam Bhutani  2:04  
Hi everyone, I'm really really excited to have another person from Hugging face: Victor on the show. Thank you so much, Victor for joining me on the podcast."
2:13,Victor Sanh,"Hi, thank you for having me. It's a pleasure to be here."
2:15,Sanyam Bhutani,It's really an honour to have you i thought i i'd like to use a hugging face transformer model again to generate your intro may read it out if that's okay with you.
2:26,Victor Sanh,"Yeah, like that go."
2:28,Sanyam Bhutani,"So this time I use Distil-GPT-2 last time with Julian I'd used GPT, to the prompt that I gave to the module is Victor Sanh is a research scientist at Hugging Face, and it came up with Victor Sanh is a research scientist at Hugging face Research Institute in New York City, and has led a number of open source projects that have been named for his work. He's also co-founder of open a foundation and is a founding member of international open source society. I o f. And open technology. His interests include computer vision and NLP open technology, cryptography, computer software and robotics. He's also an open source software engineer and researcher. Sounds, right?"
3:16,Victor Sanh,"I love it this version actually, I thought of it is actually true. Like not the open AI part, definitely not. But like yeah started like, computer vision before doing NLP. So yeah, probably it's definitely true. I promise I didn't like to do anything like on myself when training."
3:39,Sanyam Bhutani,But I definitely love to know more about your background before we talk about the research at hugging face. So could you tell us how did you get interested in machine learning? I found out that he also took a bunch of machine learning courses during University Days.
3:51,Victor Sanh,"Yeah. So basically, I have a background in mathematics. So I didn't finish all my studies in maths so I studied Like algebra topology optimization like probability financial maths, like a lot of math. And look so like, I guess, like my first project in, in machine learning was doing computer vision. So at that time I was working on like, a project, like we were trying, we were doing like 3d object recognition. Okay, so we're using like, our RGB D input. So like the D for RGB, like in the D stands for depth, okay. That's the kind of like, sensors you have like, like recent iPhones. They're using like RGB D inputs to recognise 3d objects. So I like that's how I kind of started machine learning. And then like, I did a little bit of machine translation. This was like my first step into NLP. And yeah, like now I'm like more working broadly on NLP like Stack I've done a lot of like conventional AI obviously but yep"
5:07,Sanyam Bhutani,got it you a research scientist at hugging face Can you tell us more about the work that you're currently handling and what is a day in your life currently look like?
5:17,Victor Sanh,"Yeah so I guess like I work on like that's their work on two different things so hugging Face is also building a product so we doing conversational AI, that's how this company started. So part of my time is actually Yeah, like building machine learning stack for products. So I can lead like the the effort, like picking machine learning into products or building new models, putting into production monitoring server, etc. So that's one of my on a slight like, yeah, my name Bro. Now the focus is more like doing research. So what I mean by research is like science in general. So it can be like, contributing to transformers are like Main Library. But it's also like working on for them to research papers, writing blog posts as like a teaching opportunity, like reading papers. So yeah, try to balance Like, right now my focus is pretty light on science. So I try to balance all of it. And depending on like the timeline on the different projects, like try to prioritise all of this"
6:37,Sanyam Bhutani,"You mentioned the Transformers repo. TensorFlow 2.0, or PyTorch, which is your favourite right now?"
6:45,Victor Sanh,"Um,"
6:47,Sanyam Bhutani,it's a tricky question.
6:48,Victor Sanh,"Honestly. Yeah, it's a tricky question. I feel like I shouldn't answer you know, like, just not to like offend anyone. I started mission on in doing time. have actually then I switched to PyTorch. So I guess like I use both okay like a lot of the code like the code we believe is actually in PyTorch that's like how we like started doing things like the vision like on Transformers library is really to be agnostic as possible on like the framework we use either TensorFlow PyTorch the the idea is here's like to bring value to as most people's as possible and so that like the framework is not like they were like to use our libraries."
7:40,Sanyam Bhutani,That's amazing.
7:41,Victor Sanh,"I mean, like we already implemented in Transformers it's not PyTorch Transformers anymore. And like super excited to see like how people use it and like, already people like opening issue so people are actually using the the sense of intimidation."
7:57,Sanyam Bhutani,"Okay. me fan and also user I am definitely more excited about it as well. Now, coming back to the research aspect. So a common question is how does your research pipeline look like? How do you think of a new idea? How do you approach a new problem? Maybe qouting the distil BERT example as an example? Yeah."
8:23,Victor Sanh,"So I guess I get the Distil BERT paper like came out of, like this general trend like in like, in the community, in the space analogy of like training, like, more specifically pre training super large transformers, and these Transformers were getting larger and larger and like a lot of us like, not only me, but we're kind of tired of seeing strain. Yeah. Like for many reasons, like for just one just to sight one's like, how do you handle these super large models into production, so no one can make To use like 1GB file in solution, so yeah. So yeah, like, just from a production point of view, you want to have them be more efficient, more, more like production friendly, something more maintainable, etc. So I guess like, the first tip, like in terms of research is to work on something that really excites you, and interest you. So like, like the most recent one yes, definitely is to Dilbert. Like I've worked on multitask learning before, like other topics to come this way. I. Yeah. So it's really about like finding something that excites you. And the second column is what could be like from a research like from a research scientist perspective, I would say would you be like probably Would you be like excited if you if you were to read like the paper like your contribution? I mean, I like you, you probably gonna write something and implementation in the paper or whatever, whatever like the format is, would you be like proud of like this work at the end? Like, would you be excited about this paper? If it wasn't you who wrote it? Would you be like, Okay, this is a real contribution? I can I do see the trend here. So trying like to find like this intersection between what excites you and what can really bring values to community. "
10:38,Sanyam Bhutani,"That's amazing advice. So you mentioned, you should be excited about the project. One thing that I always get worried about is in a passion project, you might not be able to see when should you end a project or when should you continue exploring because nothing works in machine learning until it does. So how do you decide that?"
10:58,Victor Sanh,"Yeah, that's a good question. Well, like if I had like the perfect answer, I guess I would be great. I don't have it like, well, like, obviously, when you start a project in machine learning, you have to factor in that like the first iterations won't work. Like otherwise you were privileged genius. So just like be aware that like the first iterations, like, won't, like, won't look like the final like the final product, the final model, whatever you call it, so it's really important that you have this in mind when you start. And it's also like a question of determination. So you know that if you're going to fail, like no, that's going to fail several times, multiple times. Yeah. How do you handle like fears like, do you like Do you never run out of ideas from the failures and that's probably like the moment when you should, okay, I should step back that down like from this, like, just take the time like to have a better view on this like, just have a better like helicopter view on this and give clarity. And maybe like in two to three months you will have better ideas that will come like from other research interests, and they can see like your ideas and like come up with like something different to best work with a remote team. Because remote work is in itself a challenge when people are distributed across different time zones. So any tips or tricks their communication. Like communication is at the centre of like, like cuz it's like it's easy. I guess it's easier to collaborate. When you like face to face so we have like, our main office in New York. So like definitely easier like to create like collaborates. But we also like, as I said, we have people in Europe, we have like to be really, I would say intentional in the communication and to be really clear on right away on like, who works on the watch who is leading what, who is like, taking the lead on timeline on like price writing things, etc. So we have to be really intentional in that. But I mean, like we, we managed to, like do we do it pretty well. So we, I think we collaborate like quite well, in the sense that having like this natural like, remote setting like in which like the overlap between Europe And New York so the East Coast is like the morning here in the afternoon like the end of the afternoon in Europe. So what it means that you can do your like I can do my day like my whole day like pretty peacefully without being interrupted every 10 minutes like write something for like the people in Europe and when they get up they have like what I wrote and like my questions and like the points like I raise etc. Then when I wake when I woke up like when I'm waking up like the next day in New York, then I already have like answers. So if you like, you have like a rhythm and dynamic that is naturally introduced by having a remote and not like not necessarily overlapping timezone."
14:50,Sanyam Bhutani,"That's amazing advice, how do you exactly factor in such that the tasks don't become that long. Now, coming back to research, if I hugging faces is like the apple of NLP, maybe not the current one with the kitchen stove design, but maybe the older one true innovators in open source and research. Could you tell us more about your latest innovation: DistilBERT it might be a little technical. So maybe if you could give us a 50 foot overview."
15:18,Victor Sanh,"Yeah. So DistilBERT stands for the field, like using Distillation on BERT. So the idea is compress BERT. So BERT, like the smallest BERT is 12 layers, it's about 10 110 million parameters. Okay? So we managed to compress it to using 40% less parameters. And it also means that the model is smaller, it's also faster. So we like on average grunts twice, faster than like the original Bert. And it actually has a pretty good performance like, like 92% of like yours. Little Bird like on the on the glue edge not on the death set. But like another advantage of that is that it's actually cheaper to train like insulation the idea that you have a teacher and you want to train the students when the student is trained to mimic like reproduce the behaviour of a teacher. So using this technique we can like Train like a smaller transformer model a smaller Bert way in a way cheaper manner, so it doesn't take like as much compute and it's actually also faster to train. [Got it]."
16:36,Sanyam Bhutani,"This is a question from the AMA section by @ logesh underscore guma pathy. Which of these approaches are best for distilling on a task specific use case distilling Transformers on language model tasks, then transfer learning or specific data set or directly transfer learning the transformer target specific task then distilling it"
17:00,Victor Sanh,"Okay, yeah, that's a good question. That's a really good question. I would say it depends on how much data you have. Okay. Yeah, like the first instance on how much you how much data how much how like the volume of the data training data, you have"
17:19,Sanyam Bhutani,"a lot of talking about IMDb, for example. So IMDb sentiment classification."
17:28,Victor Sanh,"Yeah. So, yeah, let's say we're talking about IMDb. Like a lot of like a lot of papers like recent paper, including recent papers, like some of them are actually the iclear submissions. show that the pre training like the they they study like the interconnection between pre training and dissertation, and show that pre training actually super important. And so what we do in the stillbirth is actually with pre Train, consumer language model by distillation. And then we can find to need like, like, classical verbs like, it's also faster, it's cheaper to fine tune. But like this pre training part is definitely super important. So if you have like, just a bit of data free training, so doing transfer learning, basically so I'll do something pre training you find an item next to your specific test. So during transfer learning is super important. And like can like boost your performance. If you have much more data like, like a really big train data set, are we tend to think that like training from scratch can lead to like pretty competitive baseline. Okay. Yeah, nature pretty competitive baseline. Then there's another question like how what kind of architecture you Using. So there's this there's questions of like distilling or like trying to compress and model into another inductive bias. It means that so there is a transformer based model. That's what if you're trying to put it in LSTM. And honestly that for me, I don't have like a really good answer of like, like how it works like, actually does it work? Like, like transferring a transformer to an LSTM, like because you kind of changing industry bias? That's an open question for me today. Okay."
19:42,Sanyam Bhutani,"So Dilbert is not the state of the art across benchmarks, but it's definitely much more lightweight across the Muppets. Sweet. Did you try any inference benchmarks or maybe some intuition on how frictionless will it be to put into put it into production"
20:00,Victor Sanh,"So yeah, what we basically compared is like so different kind of production settings. So both on CPU like having a server on CPU having a server or GPU doing compare the inference times and on average like both on CPU and GPU It was like twice faster than the original Bert. So yeah, we do like and we tested like both like recent architectures like for GPUs, like the hundred or like more more basic, more basic infrastructures like, like Ukg, which is like a really basic GPU. And also like the idea of like pushing like the research directly to the production we also tested when with computation and device so like, we basically tested on iPhones. And yeah, again, like it's like Smaller so like, just like doesn't take as much memory space as like the original and also faster I think like if I remember correctly like six to 7% faster on like iPhone seven iPhone seven and iPhone seven I think we have like a benchmark on like more recent like iPhone 11 so the latest one I don't have the two years in mind that should take like the swift programme ELS repository is one of like one of our GitHub repos very like Adrienne and default let's see like the test on that. The results sorry, I don't have like them. Like here, but I'll have"
21:46,Sanyam Bhutani,them linked in the description for the audience.
21:49,Victor Sanh,super great. Yeah.
21:51,Sanyam Bhutani,"talking a little bit about Iris hugging faces. Of course, it's already shipped these models on iOS which still is completely mind blowing to me even though Julien has already Talk about it. What are your thoughts on edge inference and any research in that domain that's exciting to outside of DistilBERT of course."
22:09,Victor Sanh,"Yeah, yeah. So yeah, they actually a lot of like, recent research on like, how can you compress like this large model that you can be like around deflation quantization, but also around, like how you can tweak architectures like from the beginning. So, how to include more sparsity. more specificity to the training into the transformer model, like a training time. So like, just to cite a few of them. There is Albert like this paper from Google, which is doing really, really great job by compressing that's pretty impressive. Like as a result they have another like, another IP mission."
23:01,Victor Sanh,It's called
23:04,Victor Sanh,"it's called layer drops. Okay? But that's not the real title. The real title is like read reducing number of layers on demand. Like the new method introduced is called layer drops. The idea is like, extend like the dropout, but to the layers of the transformer. That's like the high level pitch. So I'm really excited like to see all these all these works on trying to compress the transformer models like how trying to understand like, what kind of sparsity we can reach were like which layers why we can do that, like does it help to generalise does it help like to train faster etc. And we were excited to like to see all the developments I think like they are like, I counted like just my, just Kim's I just came through like the active submissions in the army. like 20 or 30 submissions are on this topic who can be super excited to see that one of the reasons that all these compressions techniques and methods and studied somehow has been put aside I like in the research community thesis was not like as trendy as doing transformers. And I believe that having these kind of studies can also help us like to understand better to better understand like why Transformers work, what kind of inductive bias what kind of extractor we are, like actually learning in the training. And it can help us like to build like better models to bid analyse these models. Yeah, like,"
24:52,Sanyam Bhutani,"Yeah, for sure, because that's how you get it into the hands of people by putting it on there for not by training it across a huge number of GPUs. And only submitting the inference to that. Yeah,"
25:03,Victor Sanh,"yeah. I think like, Can I be like really honest with you Just real quick? Yeah, like, was the thing that really excites me, like, now like as a, as a community. It's not like really doing research for doing research, what 2019 is super exciting because we're reaching a level of like, product research production. Yeah. Whereas starting to really smooth out like the, the all the advances all these texts, all these new models, like from research to production, and that's something that's really exciting because like, the goal of what we're doing is not doing research. The goal of what we're doing is bringing machine learning to so that we can impact other people's lives. So we can impact like the life of my mom who like barely knows how to use her phone. Like she's made of like, yeah, I'm typing something and like my friends actually trying to autocomplete me. Yeah, that's machine learning. So what we're really doing here is like building the tech building technologies, building the models, the next generation model, so that it's not just for us, like the like machine learning community, but also like people who have no idea of what tech is people who have no idea what machine learning is, like, there's just using consumer products now. So I'm really excited about like this trend of like having research, sharing this research and having these features research really impact other products like products that people really use in the daily lives."
26:46,Sanyam Bhutani,"Yeah, that's definitely a great comment even Jeremy Howard in the faster take was echoes upon this, in fact that the cutting edge of research is slowly shifting from research academia to engineering because what researchers Release phase two is put these in the hands of consumers eventually in the hands of humanity."
27:05,Victor Sanh,"Yeah, but it doesn't like don't get me wrong It doesn't mean that researchers like food production it's like I'm not saying that like I'm trying to make everyone feel safe but I'm just doing I'm just saying that yeah we should keep in mind that the end goal is like yeah building like the autonomous car like building the next generation of like taking your phone yeah kind of thing."
27:28,Sanyam Bhutani,I think you particularly from being from hugging face are in this amazing intersection. This amazing intersection where you can also work on a project and research and shipping to production via open source.
27:40,Victor Sanh,"Yeah, and that's actually how we started like all these are principles efforts where it's like, yeah, like we doing science for the products, the machinery for the product and we quickly realised that all the all the things we were building like it useful it useful For a lot of people like not only like just like the models, the machining parts, but like also like, more like engineering stuff, like for instance, I could work a bit like on a really small library called Knock knock. Which was like so I usually use early stopping when I tried something. So basically, except like a rough estimate, you have no idea when the training is going to stop. So well I was like spending weeks nights and weekends like monitoring my trainings and having no idea when it's going to stop. So I just like do a simple interface, simple library that basically send me like slack notifications when it ends, so I don't have to have to worry anymore. So I just I build it like visit this like for myself and like it's taking place like we're into the project. And it was like it's super useful for a lot of people thought like we really did. And like sharing this kind of knowledge like opens fostering sharing is really at the core of our DNA. And like, it's really like the patient and the culture that drivers like when we're doing open source."
29:10,Sanyam Bhutani,"I'm sure we all are definitely fans of that. But to me are these I'm not sure what your thoughts are on that there's something really satisfying about watching those last four minutes are sometimes I waste a lot of time doing that, just watching our scores go down. Now, coming to another aspect of English speaking face ships, we train models as well inside of the repository. So there's a question by Zeid olofi do you support any pre trained models in other languages like Arabic If no any future plans"
29:48,Victor Sanh,"so we are working on a multilingual version of system verts. So that is not just focused on on English but I'm trying to make work like form in the multilingual setting. So like an accent M or multilingual Bert, like sitting, so it will be available in more than 100 languages. Well, okay. So yeah, like definitely like the there's definitely like a focus shifting or not working just in English and yeah, like trying to either like shift is what we learned in English to like low resource low resource settings or languages. Oh yeah, yeah just like train multilingual does that are like multilingual by construction?"
30:43,Sanyam Bhutani,That's really exciting.
30:44,Victor Sanh,"And yeah, that's really exciting. I'm really excited about like this line of work."
30:49,Sanyam Bhutani,Coming to open source efforts are turning phase there's again a question from the AMA section by storm trooper really nice using 1721 Do you manage a large open source project effectively? And do you have any guidelines for creating machine learning libraries? Maybe you could put a knock knock once for example.
31:12,Victor Sanh,"Yeah, so that's a good question. And yeah, like, managing a really large library is definitely something that is time consuming. Like I'm gonna like it's time consuming. And we have people that I can say that full time on this, like, full time managing the library. Entering to issue is like, integrating new, new stuff. But I guess that the more like the library grows, the more it's about people in the community creating Yeah, and like just for like, just for instance, like we haven't amazing committees, I just I just wanted to shout out that like we haven't made in committee like, just for In this week, so we have two additional, two additional models like one from cells first, to Niki, from sensors like he's one of the main main offer like on the GR, CTR l language controllable language model who can sell first? He basically integrated oil baking cells. We didn't even like help him too. Well, okay. And like, another example recently is like, Stefan, like a German, hidden in Munich. Like he integrated the German version of thirds of German version of death. Like, and yeah, we didn't like, had like little supervisions in that, like, yeah, we have an amazing community. People are contributing, and that's really what we encourage. We like seeing people like gathering and having productive interaction because again, We'd like, our culture in the DNA is really about sharing. We believe that we can gather community around sharing models and not about just about sharing, it's also about like, making it available to the highest number of people. So, like, you have a lower barrier, the, like, lower barrier to, like, get into, like, playing with state of the art model, you know, and, and other things that motivators and like I'm super excited to see all this community gathered gathering around this. Because we have like this open source code. People don't have to spend hours and hours weeks, maybe like trying to reproduce results. Yeah, like trying to re implement things trying to maybe sometimes Like retrain something from scratch, you know, you know a lot of people have like a parrot retrain bird. And so having this model already available accessible to everyone. It's a huge deal, the huge detours and so since people don't have to focus on that is already there you just have like rents create, make sure it's like you have to like to correct the years etc. Yeah, you can really focus on like, the biggest thing. Yeah, focusing on the bigger fish and test new hypothesis like test new models build on Tom like the, what we call the shoulders of giants, you really rely on what people have already built and focus on the bigger question so that also they're like themselves can bring value to me."
34:50,Sanyam Bhutani,"Got it? For sure. And I'd also like to mention like as, as I said, hugging face as an apple of NLP research. I really, really I'm amazed and respect the fact that whenever a paper or anything is released by hugging face, for example, even distribute these the research paper that comes out source code that's completely up to the software engineering standards of any based organisation, along with the blog post, which also walks you through in a not so sciency language in a relaxed fashion. So even people who want to pick it up can easily go through that and build on top of it very easily."
35:27,Victor Sanh,"Yeah, something I guess that's something at least I think is driving us is the opportunity to have a community as open as possible. And one aspect of that is having both researchers and, like practitioners, I would say, like in the same space, using the same tools and building upon the same tool. And one practical aspect of that is also teaching Yeah, like, it sounds really like super spheres but like, how can you like make sure that people who don't know yet a lot of things that that machine learning or like have little experience that maybe like the more software engineers right now, but they want to play with machine learning models, how can you make sure that our library is as easy as easy as possible for them to use but also make sure that like the bigger libraries has enough complexity for researchers to play with, like to get into the code like I don't know like change the self attention and add a new model here like an add this when you change the layer norm, whatever, like not breaking. Yeah, definitely. And like teaching is definitely part of it. So either we like, like, releasing like blog posts. To explain like stuffing, machine learning and we only have like a backlog, I'm always thinking like, what can be the next blog post, but they see like in a really science way. Most like, close sharing knowledge. Knowledge is not just about like writing papers. You want to make sure that people can also like get into the community and contribute like as soon as possible. So yeah, teaching like pushing like the layers and the best practices in terms of like both software engineering and machine learning engineering. Yep."
37:41,Sanyam Bhutani,"I'd like to give a shout out to your medium profile and even the hugging face publication as they call it, do, check out the complete publication. Each and every post is definitely amazing there, even if it's an older one, I'd highly recommend everyone to go through every post"
38:00,Victor Sanh,"Yeah, yeah, like some old, old post."
38:04,Sanyam Bhutani,"Now, you mentioned this briefly that. And it's definitely the case that not everyone in the community has access to a good amount of compute power, or at least multiple graphics cards. What are your thoughts on people developing applications on the small or maybe a single machine? In the NLP domain? Do you think it's possible for them to produce competitive results now that we have this crazy competition in terms of compute power?"
38:33,Victor Sanh,"I think the question, the question is here is on what, what do you want to be competitive on? Like, do you want to be competitive on the same thing at Google? Same together fair, Microsoft, Amazon? Do you want to be competition for training because like when you're independent researchers, I I honestly think I honestly think it doesn't make any sense like to try to compete with Google Like you have, I guess, like the like, the easiest answer here is, yeah, definitely define what you want to work on. And if you're an independent researcher, it doesn't make sense to work on the same thing as Google. Or as fair, like people have much more compute than you even if you have the same idea. Like you probably I'm not sure you will have like the cash burn like I'm not sure you will have like the human resource like to develop your ideas. Yeah, so yeah, it's really about what you what you're working on. And then there are so many things you can work on like different than treat training like they are like a lot of interesting research. Like from a research point of view. There are a lot of research that it definitely doesn't require as much compute like to fine tuning is already like, fine tuning is already Enough and yeah, like all there like for practical for practitioners like I guess like the like the credo is here is build something when you're affectionate do something like you want to push something out something that excites you and it doesn't have to be really sciency research oriented thing like oh no no dog"
40:28,Sanyam Bhutani,not hot dog app
40:30,Victor Sanh,"yeah that's a super good example actually like yeah, that's a super like fun something that actually at least makes me Make me up and yeah like pick like yeah something that excite you like you taking pleasure of building something like using machine learning and that doesn't mean to say like yeah which you can be competitive like without having access to like crazy amount of control Just because you'd like, don't want to compete with Google, it doesn't make any sense. Like, yeah, let's like this lottery trainings like to the big tech companies, it's fine. And I really, and I like this trend we have like in the community right now that sharing is also like, restaurants responsible. It's a risk Constable, things to do. Like in terms of like, openness of the community in terms of like impact and diverse environments. So yeah, don't compete with Google. It doesn't make any sense."
41:34,Sanyam Bhutani,"To also quote an example that I just remembered this paper by Sebastian ruder to tune or not tune way compares, fine tuning is important or not, I assume that wouldn't take up as much of compute by but it's definitely in a good area of experimentation, if anyone would like."
41:53,Victor Sanh,"Yeah, definitely. I love this paper. I love reading his paper. I think it was at HCL this this summer."
42:01,Victor Sanh,I don't
42:01,Sanyam Bhutani,"remember, but I definitely think it was from this year."
42:05,Victor Sanh,"Yeah. Yeah,"
42:07,Sanyam Bhutani,"it's coming to online courses. So Julian had this post about the two to four n, Stanford NLP course. What are your thoughts on online courses to get a break into the field, maybe your favourite online courses and any tips to a person who maybe wants to work at hacking phase but is looking at online education?"
42:30,Victor Sanh,"Actually, like in our team, we don't we have a few people who doesn't have like, we don't have like really formal learning. Yeah, like, for instance, like Thomas like has a background in physics, like in quantum quantum physics, I think. Yeah, he has a background in quantum physics. As for the audience, he's leading the research team right hugging face. Yeah, Nia Thomas is leading the science team at Hackensack And he didn't have like a really formal like he like he didn't get in pH increase learn like an NLP machine learning and I think he's doing fine"
43:18,Victor Sanh,"yeah so I guess like it's definitely possible like to I'm not gonna say like just talking about like online courses I am I am speaking about like how can you like get into like machine learning without like, like super formal background in machine learning super formal education in machine learning it's definitely possible there are like so many courses like reading papers is one way buildings building something is definitely the way I recommend. Don't just do something definitely read like, like, read as much as possible like online courses are great definitely because they definitely pinpoint to the They pinpoint to like, good references that list and like two main topics that actually useful so that's a really good way to get in because it gives an overview of yeah what's happening why like why this definitely what these kind of developments and what like pre seed was like what's the actual trend conformers Sarah so yeah, unlike currencies like great building something definitely I definitely couldn't build it something that needs something that excites you that makes you laugh and you want to spend your all your weekend building that."
44:44,Sanyam Bhutani,"Yeah. Got there during the state of the art hot dog or not hot dog this week. What's your favourite online course recommendation? maybe one or two, any minimum"
45:00,Victor Sanh,"Definitely like the Stanford like stack of the definitely the Stanford NLP calls. Cuz I thought I led by Christopher Manning. I definitely love this one like maybe if we can set another one that a lot of like good block paws, like subnets and router height has amazing blog posts. Yeah. I really like the podcast like Matt Jaffna at Alinea. That's a really good way like to just to sweeten just now and when I don't remember his family name Sorry about that. But yeah, like co hosted with Matt young no and when and yeah, I mean, that's a great way like to like to hear the offers of the papers and like, discuss like, what are the positives Whatever the limits of the of the work and watching the future in the future works."
46:06,Sanyam Bhutani,Yeah. So this has been an amazing conversation my final question to you is what best advice do you have for someone who's just getting started into the field to machine learning or NLP? broadly speaking?
46:27,Victor Sanh,"The most reasonable advice I would say is not trying to do research at first. Okay, so, so I'm like definitely buyers to read during research was like, that's pretty much why I do. And I do realise that that's definitely not the most reasonable advice to give home to research wingstop like, start by the basic like, read courses read, like blog posts that moles have a more like overview of things that don't get into, like, technical details, like without any explanations or technical details that are not necessary here. So yeah, read the logs, like, start where you are and like, yep, build something, build something like, it doesn't have to be really complex. But that's something that we really like, look into, like when we hire for instance, like, what, like path open source projects have, you know, like, what kind of contributions to major libraries have gone on and I mean, like, there's definitely a trend like in like industry, when you hire is like when to like, okay to resume is like, how do they send out? Like, did they build something like contribute to like libraries to today, like, research or whatever? So"
47:56,Sanyam Bhutani,this has been an amazing conversation before we end the call. What would be the best platforms to follow you and follow your work.
48:03,Victor Sanh,"Oh, yeah, first, thank you very much for having me. It was super exciting to talk with you and discuss all these super exciting topics. Yeah, the best, the best. The best platform is definitely Twitter. And my handle is a son. So maybe you can make it difficult to spell it. But yeah, definitely, definitely Twitter is the way to go."
48:29,Sanyam Bhutani,"Okay. Alright, thank you so much, again,Victor, for joining me on this show. And thanks to all the contributions to the NLP and machine learning community."
48:39,Victor Sanh,Thank you. Thank you so much for having me.
48:42,Sanyam Bhutani,It was it's really an honour to be talking to you on this. So thanks again.
48:47,Victor Sanh,Thank you. Have a good one.
48:55,Sanyam Bhutani,"Thank you so much for listening to this episode. If you enjoyed the show, please be sure to give"
49:00,Sanyam Bhutani,"Thank you so much for listening to this episode. If you enjoyed the show, please be sure to give it a review or feel free to shoot me a message. You can find all of the social media links in the description. If you like the show, please subscribe and tune in each week, to ""Chai Time Data Science."""
