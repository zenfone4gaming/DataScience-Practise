Time,Speaker,Text
0:13,Sanyam Bhutani,"Hey, this is Sanyam Bhutani and you're listening to ""Chai Time Data Science"", a podcast for data science enthusiasts, where I interview practitioners, researchers, and Kagglers about their journey, experience, and talk all things about data science.

Sanyam Bhutani  0:45  
Hello, and welcome to another episode of the ""Chai Time Data Science"" show. In this episode, I'm privileged to be interviewing one of the wise and senior Kagglers who have retired from their life and working on Kaggle, just kidding. It's it's my privilege to be interviewing Dr. Christof Henkel on this show. We talk a lot about his previous assumed life of having worked as a construction worker. You might be surprised to watch the video in case you're tuning in from the audio, please go to YouTube and check out the video stream. Jokes aside, in this interview, we talk a lot about Christof's Kaggle experience how he got interested in Kaggle, his Kaggle journey, his Kaggle pipeline, and his recent second position solution to Google's quest competition. Again, you can find the links to their write up and the competition in the description of this podcast. We also discuss Christof's, new job at rapids where he's just joined as a senior deep learning data scientist. Christof shares many advises around approaching Kaggle competitions and deep learning which is his area of known expertise. So I'm really excited to be raising this interview. Quick reminder to the audience, please remember to enable the subtitle. If you're a non native English speaker, it'll help your watching experience on YouTube. Without further ado, here's my interview with Christof Henkel. All About Kaggle, data science and Google quest q&a enabled second position solution. Please enjoy the show.

Sanyam Bhutani  2:35  
Hello, everyone. Dr. Christof Henkel. Christof, thank you so much for joining me on the podcast."
2:41,Christof,"Yeah. Hi. Very, very glad to be here."
2:44,Sanyam Bhutani,"Likewise, I'm not sure if this is Christof or you're his son. Can you confirm or deny because I've seen Christof's image on Kaggle?"
2:54,Christof,"Yeah, I'm the real, I'm the real Christof, yeah. I think it should be quite obvious that I'm not like over 70 years old."
3:07,Sanyam Bhutani,I think you're makng a lot of fans.
3:13,Christof,"And I know that I even fooled some of my my fellow Kagglers and they were really surprised to see how I looked like in reality because like there's a lot of people will think that I'm like a very old man who I don't know. All this love to data science and now in this in this retirement spent this time on Kaggle, but that's not true. And if you look in the discussion posts of mine, I heavily use emojis which I think is quite unlikely for for someone, but;"
3:54,Sanyam Bhutani,Maybe it was fake to overlay a video no one will ever know. 
4:00,Christof,Yeah. 
4:01,Sanyam Bhutani,"Okay. Jokes apart, Iwant to talk about your background first you have a PhD in math and have a traditional academic background, how how did data science start to come into the picture for you was it was it a part of your research days?"
4:17,Christof,"Not really. So my my topic was about modeling of stochastic processes with applications to financial markets. But during the PhD, I made the first step towards data science I would say by using our to do the simulations of my thesis. So they I say, discovered my love for programming and my love for modeling and simulation. So that was the first important step I would say."
4:52,Sanyam Bhutani,"Okay, and you followed a traditional academic research but what made you pick math as a career and take up research."
5:02,Christof,"That's quite funny because I started to study math rather as a coincidence. So during my service service, I just wanted to enroll in any subject which was easy to enroll, just to be at university and to have time to orientate what I wanted to study because I didn't know at this point. So I enrolled in math because there was no restrictions. And it was quite easy just to to start with this."
5:35,Sanyam Bhutani,So you find it easy.
5:37,Christof,"No, it was the the enrollment process was easy. So there is no restrictions. No, no no tests to get on board and so on. So I just took math. Yeah, because I wanted to be part of the university system and then see what I want to study. But then I took my first my first semester and it went went quite well. And then also, I also had a lot of fun. So I stick with that. Yeah, and the funny thing is I also never imagined to do a PhD. And only in the very last year where did the master thesis I discovered that is self driven work, where you work on something for yourself and you are independent and just write your own thing and research in your own area is really, really a nice thing. And I really love to do that. So yeah, so that's the very end of my studies. I then just realized, okay, a PhD could be fun and then so;"
6:55,Sanyam Bhutani,Your Kaggle profile does look that of a person who's completed a PhD. 
7:02,Christof,Okay.
7:05,Sanyam Bhutani,"No. Okay-no more jokes, I promise. But how did you find your traditional background helpful? Your background in math helpful once you got started in your data sciences?"
7:19,Christof,"Yes, I would say math has different fears. So I think for for machine learning and deep learning more specifically, I think linear algebra and optimization theory helps the most. And my focus was on probability theory. But still, you get like the basics of linear algebra during your studies. And, yeah, also, I think machine learning and deep learning specifically, has also a lot to do with probability and statistics. So yeah, it's helpful. And I must admit, one very helpful thing is probably the confidence to read research papers. So I think to be very good at competitions, you also need to keep up with state of the art results and to read like most recent research papers, and during the PhD, you gain a lot of confidence in doing so. So reading new stuff, having, I don't know, complex formulas to understand and this kind of stuff, and I can really imagine that this is quite difficult for beginners. Not to do technically but rather from a confident point of view."
8:38,Sanyam Bhutani,So is it true for your kids that you read a math paper and around your head the code starts to pop out? Are you at that expertise now.
8:49,Christof,"That's a bit extreme. But yeah, I know how to code the things I need."
8:58,Sanyam Bhutani,Okay and then you found your passion for data science. Was it through Kaggle? Or earlier than that?
9:05,Christof,"No, it was through Kaggle. Of course. "
9:09,Sanyam Bhutani,Okay. 
9:10,Christof,"So, yeah."
9:13,Sanyam Bhutani,So what what made you sign up for your first competition on Kaggle? How did you get started on Kaggle?
9:20,Christof,"That was right, let's say in the last few months of my PhD, and I was finished with writing everything and just taking care of the bureaucratic stuff. So preparing for my defense and preparing the final presentation and this kind of things. And finally, I had a lot of free time. So I used to work like 30 hours a week and do a PhD on the side. Yeah, I started to watch YouTube videos around deep learning because it was quite famous at this point of time. And and by the I somehow came across Kaggle and directly jumped into my first competition. "
10:06,Sanyam Bhutani,Okay
10:06,Christof,"Which was quite challenging already. Yeah. And since then I'm, I'm hooked on Kaggle."
10:13,Sanyam Bhutani,How was your first competition experience like?
10:17,Christof,"It was really tough. So I directly jumped into a real one, so to say so no playground competition, I just like directly jumped into deep learning for speech recognition. And I had no clue about audio. I had no clue about deep learning. I had no clue about TensorFlow, and was quite funny. I never used Python before. So so a lot of new things to learn. So I learned TensorFlow together with Python, together with deep learning approaches and everything that that comes with it and of course, I know, objectively my result was not too great. After two months of very hard work. I was like, play 300 out of 1000 something. But I learned an immense, immense, valuable things there. Yeah, and since then I'm yeah, continued competing and continue to become better and better and to work on myself to improve. "
11:31,Sanyam Bhutani,That made you accept the challenge of Kaggle and you to code the challenge full time. 
11:35,Christof,Yeah.
11:38,Sanyam Bhutani,"Did you expect it to be an easy experience your first competition, or?"
11:43,Christof,"No, it's not it's not easy at all. I don't know anyone who says that Kaggle is an easy experience. Because there's like, even those competition which seem easy, they sometimes have either some twists that makes them or they are so easy that it's easy for everybody. And it's hard to find. It's hard to find an edge against other competitors, which makes them hard. So there's no such such things as easy competitions I will say."
12:18,Sanyam Bhutani,How do you now balance the time spent on competitions with life and work? Now that it's confirmed you're not a retired construction worker. 
12:29,Christof,"Between Kaggle and work I guess we'll talk about that later anyway, because I just joined the the rapids team and there you have a very high synergy of both, my private life I think it's anyways important to every now and then, and I'll make a step back. So to have some time to think about problems and to solve that. Let's say that ideas yeah just are created in your head that might take a little bit of time. So a lot of really interesting things are like things that pop into your head while showering or something similar. And the same is for like to go for for a run or you have some some day off and you go skiing or something. I think that's really important to have less pressure during the competitions and to be like more at ease with yourself and to have a fresh, fresh mind to think of new things."
13:38,Sanyam Bhutani,"I had a very interesting experience. I remember Giba mentioned during his interview that sometimes during your sleep, you get ideas and I told the same thing to SRK: I'm fortunate enough to sit right next to him at office and he said no, maybe they don't work. Most of the times those ideas."
13:55,Christof,"In the one in sleep, don't work but the one from the shower. They are really good because, at least for my for myself"
14:06,Sanyam Bhutani,How do you think the things that you've learned on Kaggle has impacted your professional life?
14:17,Christof,"I think it's, you learn a lot of self discipline, I would say, and problem solving, of course. But also a lot of strategy and a lot of how to deal with pressure I would say. So that's our aspects that also important for my, for my professional life."
14:42,Sanyam Bhutani,Also good meme posting skills.
14:45,Christof,"Yeah, yeah. Yeah, that's actually true. I think it's like the social interaction on the on the Kaggle platforms are posting discussions, answering questions, discussing different concepts I think that's really important also in your professional life so to to to explain complex things in an easy manner so even beginners understand that that's that's quite an important skill I would say. And a lot of people in the data science community lack the skills and complained that it's hard to explain the ideas to management or something similar. So I think it's really really important skills to to explain complex ideas in similar in the simple way."
15:36,Sanyam Bhutani,"Now, I want to talk more about your approach to competition and I'd like to use your recent two gold finishes. But before that, what sort of competitions do you usually enter what competitions do enjoy working on?"
15:50,Christof,"So I tried to do competitions that are very different to the ones I've done before. So to enlarge my myself gearset and enlarge my skull. So I started with audio but then I did NLP, computer vision. I really enjoy hybrid competitions. For example, there was one about Vito product demand prediction, which was from the, from the architectures quite similar to the pets adoption prediction. And then both you have some some yeah, something some some profile, let's say if it's a product profile or pet profile, and you have various just you have text, you have tablet features. And it's really interesting to combine all this different aspects and to build so I guess I'm known for building my deep learning models. And, and I love to build like deep learning models that have different components. So you have one component which handles text, you have one component which handles images and then you have to somehow integrate all the different aspects. And that's quite challenging and makes a lot of fun in my opinion, because you have a lot of freedom in what you do. And you can try a lot of cool things, so."
17:18,Sanyam Bhutani,"This also question from the AMA, what number one mistake do you think most people make while training deep neural networks? One mistake or one thing that they miss?"
17:34,Christof,"I think networks, they're like, a lot of small things that you can do. Not not necessarily wrong, but not optimal, like pre processing, for example. So most people just standard scale their variables, which is fine as a starting point, but not optimal for most the same as with handling NAs or clipping values, all this kind of stuff. So there's a lot of small things that you could do better, I would say. And it's really hard to do better because there are no not much education around that. So you wouldn't find much tutorials how to build very good neural network for tabular data. "
18:25,Sanyam Bhutani,Yeah. 
18:26,Christof,"That's unfortunate, but good for me. Because that gives me a little edge in competitions. Now, what else? I think often people think to bake in terms of neural networks. So instead of starting small and then trying to to grow, they start with something fancy and big and then you have a lot of  problems that comes comes with size of the network, I would say."
19:05,Sanyam Bhutani,"Okay, now coming back to Kaggle, what type of go to step in working on a problem and do you again, start with small iterations or we have pre written code that you bring into the competition."
19:19,Christof,"Both, so the first thing is ability very fast and simple baseline. So the fastest, the better, just to have one, to have the full pipeline already from pre processing, over training, our post processing if necessary, until submission. And then the next step is to set up a solid cross validation scheme. So I think that that's the most important thing for Kaggle competition is to have a valid local cross validation which somehow matches your the leaderboard because Otherwise, how can you optimize something if you cannot, if you cannot relate to the world, right? So that's the most important step. And I put I emphasize this one so much that I spent for some competitions is the end the huge amount of time until I get the cross pollination right. "
20:21,Sanyam Bhutani,Okay
20:21,Christof,"Because in my opinion, it doesn't make sense to optimize something that won't be reflected on leaderboard. And so for example, one of the recent two competitions, so the TensorFlow 2.0 question answering, I spend, I don't know, weeks to reconcile the competition matrix because they are the sponsor are quiet and transparent about it. And everybody had a really, and there was even a buck in the matrix. So I spent like, I don't know, two, three weeks to figure it out. And then I finally found a buck and they changed the metric of the words So I just started again. Yeah, so sometimes it takes for ages to just get a solid cross validation scheme. Sometimes it's quite simple. Sometimes it's, yeah, it's right from the beginning. But sometimes it can be really hard. And then I spend a lot of time on this. So that's my first step. And as soon as soon as you have some, some solid cross validation scheme you can use then I start iterating ideas. And for this, I like to scale down. I know the whole architectures and models. So I use a very simple model and just iterate through whatever I can find basically, and just add to it, it's a the second half of the competition. I start with slower, slower, larger things."
21:55,Sanyam Bhutani,"Okay. For the beginners who have absolutely no idea of how to create a good cross validation framework who just copy for most of the times, what based advice do you have? How do they learn better ideas, cross validations?"
22:09,Christof,"I think that's difficult because it's really depends from competition to competition. So normally there is, most of the time there's one, this minimum one discussion post about cross validation and cross validation leadership. Everything that's posted in this discussion post and also, yeah, try to if you have a big gap between your local cross validation the leaderboard, try to understand where this gap comes from. So normally you have some kind of leakage between your train and validation set. So can be something simple like having same I don't know if something some competition about users or something having send us us in your training and validation set can create some some leakage, which is, can be quite obvious, I would say. But sometimes it's more difficult. For example, when you have, I wasn't an image competition where you had very similar images, but not exactly the same. And it was really hard to find the similar images and then because you don't want to have in similar and to put them in one side or the other."
23:33,Sanyam Bhutani,"So we we're all fans of you all awesome results from competitions, talking about the recent two wins quest Q&A. Did you find this problem to be easy and relevant to your previous competitions? I know you just entered the quest competition during the last two weeks which which is unheard of for someone finishing in their second position. "
23:53,Christof,"Yeah. Okay, I did a lot. I did some competitions previously related to NLP where I used to three already before the flow q&a, the first two boards were to pre transformer time where there was like, LSTM the state of the art. And then the one before was like, from jigsaw, you need to identify buyers and comments and so on the day I already used transformers so that's what it was a big advantage and I already use the hugging phase repository, which which helps a lot and I made already some some mistakes which I then it's a good learn from. So that give me a head start. But I've never done anything related to question answering and two texts span prediction because what the TensorFlow one you needed to predict an answer span so really find the correct start and end token in Wikipedia article. So that was something new for me also how to use huge inputs. So normally the transformers you have like a maximum input of a specific size, which is way smaller than the whole Wikipedia article you need to analyze and then you need to come up with some some clever way in in going through the whole article. That was also something interesting to learn. Yeah, and then I thought, naively as I am that it's a piece of cake, so just enter the question answering the Google quest competition afterwards because it's, you also have question and answers. You also use Transformers;"
25:50,Sanyam Bhutani,and you were bored. nn
25:52,Christof,"Yeah, and it's like, piece of cake. But at the end, it was not it was like Really, really different. And yeah, I needed to learn a lot of new things. I haven't used the spearman rank metric before that created some some interesting things, there was a lot about post processing, which was quite different was also very different was the size of the data. So for the tensor flow one you had like 100 thousands of documents, and you train for an A one model train for 30 to 40 hours. And the the Google Quest, like 6000, which creates, yeah, quite different problems, because you you overfit immediately and you have a lot of other things to deal with. So, yeah, you can, you can always see, reuse your pipeline, and reuse the things you've learned, but there's always, always something new even if it seems like gets it's the same."
27:03,Sanyam Bhutani,"So how did your approach ready for the question? Because you had two weeks to the final deadline of the competition, and yet, your team finished second in the position. amazing result because I'm sure it was a team effort. But if you could tell us more about the approach."
27:20,Christof,"Yeah, of course, it was a huge team effort. And we were really lucky with the with the team. And I knew in picking the respective people what what their strengths are. And I, you know, really looked for a team, which has all the necessary skills and and I was yeah."
27:44,Sanyam Bhutani,How did you convince Philip and Dimitri to not use the team names Zoo.
27:49,Christof,"It was a simple player, so so he made a instead of needed a simple vote. He had like 10 amazing teams  names, so I suggested and then this one one bias. "
28:08,Sanyam Bhutani,Okay.
28:12,Christof,"Yeah. Yeah, I think the main thing that differs from other competitions if you only have like this limited amount of time is you need to be very efficient. And really, let's say trust that your team members will do their part. So for example, Philip and Dimitri concentrated on the post processing and I didn't even touch it. So I concentrated on modern architectures. So, yeah, you need to trust your teammates, that's quite important. And you also don't have time to try to fancy things I would say. So there was that's quite quite interesting story because there was a paper by Facebook research and they publish the pre trade model, which is called part. And I hate it on my radar. So I even posted in the external data threat because I wanted to have the option to use it. But then I didn't have time to implement it because it was not ready to be used by the standard hugging face library from it. So yeah, I went on with simpler things that are easier to implement. And if I would have had more time, I of course would have checked it out because it was like a promising architecture and I didn't. Yeah, so it was quite funny because the first team saw a post I made in the external data thread, where I gave a link to a paper from from Facebook. Yeah, and they implemented the model And it was the best performing model, which is quite funny because if I hadn't posted this link, they wouldn't have had it on their radar. And yeah, I didn't have time to implement it for our team. So that was quite unlucky, I would say. And if we have had more time and not only the two weeks I definitely would have would have checked this one out too. So you're not only you have not time to try to things that are too new you rather concentrate on things that you confident that will work. And you're always yeah, you you also think about what's the most promising things you can implemented short time so not what are the promising things in in generally, but what are the ones you can also basically implement in a day or two to have it done within within a given time, so that was different to other competitions and other competitions. I take a lot of time and in reading domain specific papers, reading the background, check out, get repositories related to the domain and so on. And then in this competition we didn't have the time for for that kind of stuff."
31:17,Sanyam Bhutani,So there's this prevailing sentiment that you need a huge cluster of GPUs to win a gold winning solution. Can you speak to that sentiment and share your hardware setup for the previous two competitions?
31:31,Christof,"Now so and generally I don't think you need a lot of compute power if you're not competing in that computer vision competitions and the like this huge energy competitions like the TensorFlow one, but on the other hand, I might be a little bit biased what means huge, because I have I have a desktop PC with three graphics cards. Three GTX 1080 ti from Nvidia. So;"
32:07,Sanyam Bhutani,Savings from your previous life.
32:12,Christof,"So, for me, it's not that much but for I, it might seem much for other Kagglers, I would say. But it's not that I have a dg x or something of this size and the previous one I also use a little bit of AWS compute power. So just squeezes squeezing more experiments in this two weeks. "
32:40,Sanyam Bhutani,Okay.
32:41,Christof,"Because otherwise it would have been difficult but swala for doing this stuff parallel, not not faster as well for doing this stuff in parallel, especially when training k forward schema where you have like five folds which are identical and you just want to sort run stuff. But for my normal experiments, I normally use my desktop and that works out quite quite well. "
33:08,Sanyam Bhutani,Okay.
33:09,Christof,"Yeah. So you interviewed the the Zoo. And I really enjoyed this interview too. And I fully agree with the dots opinion that having restricted hardware, yeah, enables you to, or forces you to think about creative solutions that are simple. And also to think about implementing in an efficient way, computationally, which is also a skill that is important from my point of view. It also forces you to think about efficient iterations through your experiments. So for example, use very small models in the beginning and so on that that all starts basically by having hardware restrictions and by not having unlimited computational resources."
34:07,Sanyam Bhutani,Okay.
34:07,Christof,"So I think it strengthens your your skill set in doing more efficient, efficient models. "
34:15,Sanyam Bhutani,"Okay, now, I'd like to discuss about your gold winning second position team solution if you could give us a very high level overview, I'll definitely have the write up link in the description for those who want to check out the complete detail later, but if you could discuss the very high level overview of it."
34:33,Christof,"So, for those who are not aware the competition in one or two sentences was about multi multi layered and multi classification. So you had like a Stack Exchange post questions and then related answer and you need to binary classify. Are you like classification policy but rather regression between 30 different targets somewhere rather question related something like how good the question is written, how good the answers written, how good the answer matches to the questions. What type the question is office it and instruction or is it not a question at all? So there was one like one target question is not really a question. Yeah, and you had to use NLP methods to analyze the text and to predict those those 30 targets. Yeah, and right from the beginning, it was clear to us that proforma transformers will perform best. So we based our solution on on an ensemble of transformer models. At the end we had five different ones with slightly different architectural tweaks but quite similar yeah, and they used, they have different pre trained backbones, which we then tweaked a little bit, change the architecture a little bit. And then used, let's say specific post processing in the making the predictions closer to, to what's a good performing, performing target. So the metric was really about the ranks of your predictions. So it was experiment rank correlation with a specific tweak on ties. So if you predict the same, the same prediction that squat differently that if you have like, really close but not the same predictions, and you can improve the metric a lot by by handling those those ties correctly. And so I would say our solution is like twofold one thing is the the the modeling itself and one thing is the post processing and we were really aware that there's a huge shake up potential which you also could see after the competition so there are a lot of teams moving up and down and we we are we we stayed quite stable. But we also spend a lot of time in figuring out a way that robust in the on the private leaderboard also. So really happy that that worked out. "
37:36,Sanyam Bhutani,"Okay. I also want to discuss another aspect of you're a seasoned Kaggler, but many beginners struggle with this thing of, should I need to do x y z goes before I can do Kaggle before I can learn about transformers. What advice do you have for them because there's also this aspect of modeling very fast, which you can only learn on Kaggle when should they jump on Kaggle, should they first go back and do all of those schools or should they jump on Kaggle right away?"
38:04,Christof,"Now from for me, it worked out quite well to directly jump into a heart problem for yourself because there you learn the most. And I, friends of mine who just joined Kaggler I always advise to do so. So just pick something that you are interested in. Yeah, and try to try with the with the heart with the hard things, things that make you feel uncomfortable. A bit, not not too much you wouldn't be you wouldn't start something where you need like, I don't know, high performance cluster to participate. Something that's doable, but you shouldn't have the ambition to perform like in the top, top region but rather to do your best and to start with something complicated for you. And I also think it's really helpful to find team mates from who you can learn. And also from who you can learn efficiently. So it doesn't make sense to team up with the unknown with a very experienced person with whom you can't keep up with but you ideally you would team with someone who's slightly, slightly either better or he had different different strengths than you are, so you can learn from them."
39:33,Sanyam Bhutani,"In hindsight, or maybe you could give me a few examples. How has Kaggle impacted your professional life? Are you taking away any learnings that are helping you there off Kaggle data sciences?"
39:48,Christof,"I think like 99% of my off Kaggle data science comes from Kaggle because I had no data science experience before Kaggle. So, basically 99% of my, of my experience comes from Kaggle."
40:07,Sanyam Bhutani,This question of mine originates from the fact that now many people start to talk about Kaggle is not equal to data science and no one of them is a Kaggler. I'm trying to get the opinion of the best Kagglers against it.
40:19,Christof,"Yeah, I think there's like, often people start with statistics, data science analytics, and then come to Kaggle. But some directly jumped into Kaggle from their physicists or so, that in chemistry often have like PhD and all of our Kagglers they start to have contact with data science in general."
40:45,Sanyam Bhutani,Sometimes constructions also. 
40:47,Christof,"Yeah, yeah, yeah, it's, it's, you know, there's one girl she's like a lingual linguist. So she studied linguistics and then was interested in doing NLP things and then started to compete in a peak competition and they had all she had all this this domain knowledge from linguistics. "
41:09,Sanyam Bhutani,Yeah
41:09,Christof,"It was quite a fun experience for her to have like this contrast in in what linguists think about how language should be processed or language is set up basically. And then you have like this this Kaggle guy so just throw transformers also just works really great, so."
41:34,Sanyam Bhutani,"Yeah, what best advice you have for someone who's just starting out on Kaggle?"
41:41,Christof,"Yeah as I mentioned, Kaggle was something difficult for yourself and try to read everything you can get on the platform. So read all the discussion pause. Read also kernel questions and posts. Try not to copy too much. So it doesn't make sense to just copy some some, the best performing public kernel and then try to tweak a few hyper parameters and to improve that way because I think the kernels which are public are already tuned in their hyper parameters so much that they are working quite well on the public leaderboard, which doesn't mean they will perform good on a private leaderboardr, but rather get started with your own ideas. And if you run out of ideas, try to peek in public kernels to get some more inspiration. But try not to copy to copy too much. Rather take I don't know two three days longer to implement your own thing than just like copy everything from probably one. It might be no, not as efficient for your first competition, but in the long run, it's way better to have come up with things, things by yourself"
43:01,Sanyam Bhutani,"I think that that's also useful to beginners who struggle with starting with a blank notebook. That's that's where the copy foking works all but if it becomes a habit you can actually never start with a blank notebook and might not be able to tackle a real data science problem at work with your inspiring for. Now, speaking of coming to your new role, you just joined the rapids team at Nvidia. Can you tell us more about your work at rapids? And what tasks are you working on? Did they give access to you or to a large number of GPUs? That your reason for joining?"
43:37,Christof,"No, no. Yeah, I'm really excited to have joined rapids. They are building a team of Kaggle Grand Masters and I guess all the team have the same role description or the same, the same job. It says many presets but the the main role is to to use rapids in machine learning competitions, which is great for people who love to do machine learning competitions. And by doing so, the idea is to enhance rapids and to create more more functions within rapids. So for those who don't know, rapids it's it's a collection of Python packages with which you can compute directly on the GPU. So it's there's like Q-ML, which is like Sk-learn, but on GPU is cudf, which is like pandas but on GPU. And the idea is to have the functionalities of those two packages escalon and pandas but the speed of having GPUs at your at your service and I think that will become more and more important also on Kaggle because now as Kaggle moves towards inference to be done in kernels, it becomes really, really important to have efficient running code and to have fast running code. "
45:11,Sanyam Bhutani,Yeah. 
45:12,Christof,"So I think in the next competition, where you need to do something with tabular data, rapids will give a huge edge for those who can use it. Yeah, and and our, our purpose is to to use rapids in these competitions and to, on the one side act as a power user who have any users this framework to identify box which might be in there, but also give indication what what features are missing what you would need to add. There's a few too people working on especially on deep learning on tabular data. So, as you mentioned, there's a lot of mistakes you can make with table, deep learning for tabular data, a lot of different pre processing, post processing. And yeah, and they are enhancing the rapids framework for also handling this things more efficiently. Because if you think about it, ideally, you would not only do the modeling on GPU, but also the pre processing and the post processing and have one workflow. So one example is loading and a one gigabyte data frame to pandas takes five minutes. So it's really slow load and and then and then you do something you do some pre processing, like, I don't know, label encoding normalization or something which also takes forever. And then at some point, you have your model set up and then you transfer everything to the GPU, and then do your modeling and the ideas from references you've directly loaded. data into the GPU, which takes 20 seconds instead of five minutes, then do the the pre processing there, which is also a factor of 100 faster because you're directly on GPU and then also do the modeling there. And you don't have this, this copying back and forth between CPU and GPU. So it's like, it's just like wave be faster and way more efficient as it compared to how people are doing data science nowadays. Yeah, and I'm quite excited to, to play a big, big part in this in the future."
47:38,Sanyam Bhutani,"I'd also point the audience to in another interview that I did with Even Oldridge where we discuss this in a much more depth, but I think the end envision with rapids is if you could just replace import pandas as PD with import rapids sub module as pd. All of the codes should be able to function with all of the sub functions of pandas. And with the huge PDFs that you mentioned."
48:02,Christof,"Yeah, yeah, exactly. So the idea is to have the functionality of pandas and escalon, but the speed of GPU and then you can build an end to end pipeline with it."
48:14,Sanyam Bhutani,Awesome. What's next for you? Will you continue to Kagggle? Or will you focus more on work or will you Kaggle from work?
48:22,Christof,"I think it's it's the same thing now. So my job is basically doing machine learning competitions and using using rapids permit for that. And right now, I just entered one computer vision competition. Yeah, and, and I'm really interested in improving the pre processing and post processing for computer vision. So there's a lot of things in pre processing, you can do like augmentation and so on, which is currently done on on CPU, which, in my opinion, doesn't make sense to do it not on a GPU. And there's also a lot of things you can do with respect to post processing in computer vision. So I know that for example, metric learning where you try to find from a test image, you try to find the similar image from train and take the labels of that one. So you need to find similar images, and so on. And this is normally a post processing step that also happens on CPU nowadays. And it's also really, right now really inefficient and slow. And it would be really awesome to also put that on GPU, so so that's what I'm currently working on. So I think that that's how it's meant to be you, you you compete in the competition and you you, yeah, you try to find out what's missing and where's room for for improvement and where you can do more things on the GPU."
49:55,Sanyam Bhutani,"I'd also point the audience to another interview that I did with James Dellinger about the Dali library will, again a framework by video, which allows you to do to be documentation on the GPU with the same end goal that we're talking about."
50:10,Christof,"Yeah, that's also on my my to do list so I joined rather recently, and want to do as check out the Dali library and see ya see if it already has everything I need or if there's anything, anything you would love to have for, for using it for competition."
50:32,Sanyam Bhutani,"Awesome. Now, before we end the call, what would be your best advice to someone who's, a single advice was just starting their machine learning journey?"
50:46,Christof,"Yeah, don't be scared, I would say and jump right right into it. Just try your first Kaggle competition. Even if it seems very, very difficult. Just there's always people who answer questions in the forum. So if you have questions, if you have problems with your cross validation, just just put up a post. And normally people are really, really open and transparent and answering."
51:14,Sanyam Bhutani,But also make sure that you do your homework before posting the question. Don't just ask all of the Google questions right there.
51:21,Christof,"Yeah, this shouldn't be something that you can easily Google and it shouldn't be something that already some someone else answered the day before or something."
51:32,Sanyam Bhutani,"Yeah. Awesome. Before we end the call, what will be the best platforms to follow you and follow your work? I'll have these linked to the description again, for those of the audience who want to check it out."
51:43,Christof,"It's a LinkedIn account, which is my Christof account. And then I have a I have a Twitter account, which is purely from my my data analyzes, which is, yeah, pure Kaggle related. So I just post related things there."
52:03,Sanyam Bhutani,"Okay, awesome. I'll have both of those linked in the description. Christof thank you so much for joining me on the podcast and thank you so much for all of your contributions to the Kaggle community."
52:13,Christof,"Yeah, thank you to all. I'm really really proud that I was invited to your podcast. "
52:18,Sanyam Bhutani,It was really a privilege to have you on the show. Thanks so much. 
52:22,Christof,Yeah.
52:30,Sanyam Bhutani,"Thank you so much for listening to this episode. If you enjoyed the show, please be sure to give it a review, or feel free to shoot me a message you can find all of the social media links in the description. If you like the show, please subscribe and tune in each week to ""Chai Time Data Science""."
