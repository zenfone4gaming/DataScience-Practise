Time,Speaker,Text
0:13,Sanyam Bhutani,"Hey, this is Sanyam Bhutani and you're listening to ""Chai Time Data Science"", a podcast for data science enthusiasts, where I interview practitioners, researchers, and Kagglers about their journey, experience, and talk all things about data science.

Sanyam Bhutani  0:46  
Hello, and welcome to another episode of the 'Chai Time Data Science"" show. In this episode, it's really a privilege to be talking from someone on the other side of Kaggle, the CEO of Kaggle Anthony Goldbloom. Anthony has a background in economics and has been working on Kaggle, working on the other side of Kaggle as the CEO and co founder for almost over a decade now, which of course we talk all about in this interview. We talk about his journey as the co founder of Kaggle,  and even Kaggle's journey broadly speaking over the few past few years. How Kaggle has evolved, how their perspective, how their team's perspective on Kaggle and the team itself has evolved from the start until the point even after it got acquired by Google. This interview shares a lot of insights about the behind the scenes work that goes into Kaggle while hosting a competition, broadly speaking, or even putting out a new course on Kaggle learn with also I'd highly recommend to the audience that you should definitely check out. We also discuss Kaggle's future plans and what's next and exciting for us as frequent Kagglers. I hope you enjoyed this conversation as much as I did. And it was really a privilege for me to have Anthony on the show. A quick note to the non native English speaker audience. If you'd like to enable subtitles for the video of this interview, assuming you're watching it on YouTube, please do so the subtitles are not auto generated, those have been manually fixed and I hope those will improve your watching experience. The written version of this interview or the blog post version will come out in a few days or in a few weeks from this release. You can find the link to the website where it will be posted if you want to read it in later. For now, here's my interview with the CEO of Kaggle, Anthony Goldbloom. Please enjoy the show.

Sanyam Bhutani  3:07  
Hello, Anthony, thank you so much for taking the time to do this. It's really a privilege for me to be talking to the CEO of the home of data science on the ""Chai Time Data Science"" show."
3:18,Anthony Goldbloom,"Thanks for having me. As I mentioned to you, I'm an audience member. So it's cool to be on the show."
3:24,Sanyam Bhutani,"It's an honor to have you. I am a fan of Kaggle, as you very well know, but unfortunately, I didn't get to witness the progress of Kaggle over the years. I was I was in high school, actually, not even in high school when it started to progress. Could you walk us through what led you to create this platform and then maybe we can discuss how it progressed over the years?"
3:44,Anthony Goldbloom,"Sure. I mean, my first job out of college was as a statistician, I'm Australian originally and so I was working at the Australian Treasury and then the Reserve Bank of Australia. I used to do things like forecast GDP, inflation, unemployment problems that had relatively small datasets and also very messy datasets, you know, measures like GDP, it's not really clear they, there's a lot of, their Composite Measure, it's not really clear that there's a lot of signal and, and they're very hard to forecast. And I think I was first exposed to the idea, the idea Kaggle started just as machine learning competitions. And it's not an idea that we invented. It has a long history, probably made most prominent by initially the KDD Cups, and then the Netflix prize. And I had seen the KDD Cups, and I always thought that was a very, really, really nice way, a nice way to decide who gets to speak at a conference, right? So KDD was a prominent applied Machine Learning conference. A lot of talks, who spoke at the conference was decided in the traditional way where you would submit a you know, your your working paper or your your paper, and it was either accepted or not, that they always reserve some number of slots at the conference for those who had done well, and whatever the competition they ran for the year was and I would like that because it's an objective way to decide, you know, it gives people a chance who haven't published in the usual places and, and in the usual ways and it gives them a chance to share their ideas. And I found that attractive and it was something that I wanted to bring it to, make a concept that it wasn't just us for the KDD Cups that was used more broadly. And that was the motivation behind, that was sort of where the initial spark on Kaggle came from."
5:38,Sanyam Bhutani,"Got it. I also want to point out when the Kaggle had started, machine learning wasn't the so big thing AlexNet didn't come out at the time. Were other platforms very prominent at that time?"
5:51,Anthony Goldbloom,"Therewere a couple. So firstly, a lot of the KDD Cups have been hosted on just like one off websites. There was a Polish website called tunedit.com actually don't know whether they still exist or not. I think it was attached to a university, maybe the University of Warsaw or a Polish University. That was the only platform that I was aware of that, that repeatedly hosted competitions. But it didn't seem to be a very active development. It seems to be more of a side project of researchers who maintained it. And it's certainly the case. Kaggle's timing was very, very lucky, as you say it was two years before AlexNet. That being said, machine learning was was still capable of powerful things. When Kaggle first started, in one, the very the word datas while machine learning it wasn't nearly as prominent as it is today. Data science as a field was sort of becoming a, data science as a word and as a field was starting to bubble up. It's becoming a little bit more prominent. And one of the exciting things that Kaggle did is, it brought together people from many different backgrounds who are actually trying to achieve fairly similar things. So you had people who are buying from magicians, you had statisticians, you had econometricians, you had data miners, all these people had different job titles. But were trying to achieve similar things. And it was interesting, because in the early days of Kaggle, they all had their pet techniques, you know, support vector machines, logistic regression, linear regression on generalized linear model modeling more more generally, self organizing maps, you know, there's many, many, many very different different techniques. And I'd say probably Kaggle's first achievements, was making it very clear what worked and what didn't work. And in the very, very early days of Kaggle, the thing that dominated was the combination of clever feature engineering and random forest. And so it's super interesting if you look at the very early competitions people tried all sorts of things. And after one or two competitions, it became very clear very quickly that random forest was the way to go. And so I'm proud of that contribution that we went from no one really having a very clear sense for what worked and what didn't, to helping all these people in all these different fields have a have a good sense for what sort of techniques they should focus their attention on."
8:23,Sanyam Bhutani,"Did you anticipated getting to this stage, now that you mentioned that people of different backgrounds are coming together, did you expect that to happen? Has it surpassed your vision yet? Or has that happened already?"
8:35,Anthony Goldbloom,"It's certainly the case the Kaggle's a lot bigger than I ever imagined it would be. My early goals were I wasn't really enjoying my job as a, as a statistician or an econometrician. I felt like I was forecasting something that was inherently unforecastable and so I thought that this website would be really, really fun thing to work on. And I was hoping that it could support me. That was the initial goal. And I think with every level of success, you become more ambitious. It's like the human condition. And so, you know, then it then it when it reached that level of success, then there was another target. And then another target and another target. And as always, there's always another target, right? Where we're a species who are never really completely satisfied, right."
9:32,Sanyam Bhutani,"It's definitely for our better that you're not satisfied, so Kaggle keeps getting better. Did you always have the mindset of being a founder, so to speak, was you, were you always interested in doing the startup or did this eventually turn into a full time job?"
9:50,Anthony Goldbloom,"It's interesting, I was probably not super exposed to Silicon Valley startup culture. So it's not like I had this long term vision that I was going to be an entrepreneur or, or a founder. I think that it's certainly the case so that I definitely had those sort of instincts during high school. I had a small business video, videoing and editing Bar Mitzvahs in my hometown. And then during college, I built a little company that made basically mp3 players for for retailers. So the idea was, I just used to buy very, very cheap secondhand computers, install a very slim slim down version of Linux on them and turn them into mp3 players. I thought it was crazy that a lot of stores were still using cassettes. When the iPod came out you could have 10,000 years old or whatever was in your pocket. So I was kind of always tinkering around with these things. They also although I studied statistics, econometrics in college, I had a program programming job in college, which I absolutely loved. And so, I don't know is is, is I probably had the tendencies without maybe had I been born in the Bay Area I might have gone straight to being a startup founder. It took me a little bit longer. Being in Melbourne, Australia where I really wasn't surrounded by that culture."
11:29,Sanyam Bhutani,"Got it. Coming to your current day, what does it, what does a day in your life currently look like? Do you still get to do sailing or are you continuously working with the team having Chai with the Grand Master sometimes?"
11:44,Anthony Goldbloom,"So my day to day at work it's it's a lot different than it was. My day to day, yeah, I don't know it's changed a lot over the over the last, I don't know 10 years. And initially, I used to do I coaded the website. I did the data science behind the competitions. I tried to sell the competitions to customers, trying to trying to do it, do a lot of it, myself and then over time, you know as the team has grown, so, Ben Hamner? owns product and engineering and Julia and Will and the team are in competitions and so the things that I used to do and now handled by people who are much much more capable of doing them at a much higher level. I remember the early, people complain about leakage now. You should see the competitions that I set up they had horrible problems with that. And, and so you know, a lot of a lot of the things I tend to focus on are, I guess like areas areas that are newer and and, you know, we're trying to spin up. So, you know, one of the one of the big questions for us at the moment is how, you know, what's the, what should the interaction be with the rest of Google? One thing you might have noticed is a proportion, the proportion of competition that we run from other Google teams is much higher. So that's a very obvious place for us to integrate with other Google teams. But you know, what, what other areas where Kaggle should be looking to integrate more with Google? I think there's a lot of exciting possibilities. Google is a very, very strong AI and machine learning company. And so there's a lot of AI and machine learning happening around Google. What are the kinds of areas that are a good match for, you know, what, what are the kinds of ways we can collaborate with other Google teams that you know, good for Google and good for the community. And so that's that's an area where I'm spending some amount of time now."
14:00,Sanyam Bhutani,"Got it. So, like you were talking you still you're still responsible for the vision of the company. Can you speak to how you've observed the vision change over the years and especially since the acquisition by Google, because you remain the CEO even after Kaggle got acquired? "
14:16,Anthony Goldbloom,"Yeah, I'd actually say that a bigger turning point for Kaggle was before the acquisition. With the launch of Kaggle kernels. This was in 2015. And it was really the first time we moved from being really, from a community focused standpoint, primarily focused on machine learning competitions to starting to expand beyond. And the idea I think there are a bunch of people involved. But then having, you know, noticed that Docker was coming up as a really powerful, you know, containerization and Docker specifically, was starting to emerge and sort of spotted the idea that you could make more easily and portably host reproducible data science environments. And so that's where the idea and we noticed people trying to share a lot of code on the forums. And so that was sort of, I think, our first big move. That was the, you know, really a very big move for us because a move from, move ended up moving us from just machine learning competitions to you know, I think if Kaggle is actually a fair bit broader than that now, and being a more broad based community for machine learning. We have competitions. But we have, well, Kaggle kernels, which we're actually in the process of rebranding Kaggle notebooks, and we have the data sets and actually just a stat for you. I think it's somewhere in the order of five or six times as many data sets are downloaded from our data sets platform than our competitions platform today. So, you know, I really, although many of our most active, most intensely active users come through the competition platform, we have a huge number of users who aren't not touching competitions. But rather than using Kaggle in other areas, either. We have almost a million people at once viewing notebooks that were authored on Kaggle, which is a large number. So to put put that in context, it's a million people coming, who don't want to start their projects with a blinking cursor, but wanting to see how other people have approached similar problems to the one they're tackling. And then we have many, many hundreds of thousands of people downloading data sets from Kaggle on a monthly basis. And these are people who are using Kaggle. You know, we now have a very, very large data set repository with 25,000 data sets and growing quickly, and so these these are people who when they're searching for external datasets, find Kaggle a very useful starting point for looking for those datasets. So I'd say that's one of that's probably the biggest, that the launch of the competition, the kernel's and the launch of data sets are really the two biggest changes that have happened to Kaggle. And they happened in 2015 and 2016. The Google acquisition was 2017. So came after those two, the launch of those two products."
17:19,Sanyam Bhutani,"Can you speak to datasets? Because at the time of recording data sets was introduced just two three months ago, what led you to adding it as an independent tier, people can actually go out and become Grand Masters even in that tier if they want to."
17:33,Anthony Goldbloom,"Yeah, well, actually datasets, we first posted public data sets on Kaggle in August of 2016. And the way we did it was really, really fairly subtle. We just put them in the competition listing and it's like hey you can compete in competitions. But we've also got these six data sets. For those who just want to play around and write notebooks or kernels on data sets. Why dataset in particular got a lot of attention, Ben put up Hillary Clinton's emails. And so a lot of people, this was around the time of the controversy during the election around Hillary Clinton storing emails on a, on a, on a private server and those emails, I forget through some FOIA requests were released publicly. And so a lot of people were doing NLP, etc, on those emails. And that demonstrated to us that data sets were, yeah, a very powerful direction for us and a good good place for us to invest. And I think it was in January of 2017, that datasets became its own, like tab on the top of the site. So data sets have been there for a while. And I remember, I remember the goal initially was can we get 10 data sets a month, and I think we're probably up to 10 data sets an hour, now I'm not quite sure what the stats are, but we certainly get a large, large large number of data sets. Data sets as a, as a progression as a part of progression, that came in maybe a month or so ago. And the motivation behind that is, you know, one thing that Kaggle has always been accused of is not being a very well rounded data science credential, you know, competitions, train test for a certain skill set, but there's a lot of other skill sets involved in, in doing data science well, and in over time, we've elevated notebooks and, and then discussion into being part of the progression system. And the latest is data sets and I think they each, each of them play a bit of a different role. So competitions is fairly obvious. It's a very nice metric for figuring out who's good at training very accurate machine learning models. Discussion I look at as, and discussion on notebooks get a lot of controversy. But I think on the whole, they're really wonderful. Yes, there's a little bit of abuse. But I think the level of abuse is relatively under control. And they provide more value than they do, you know, suffer from abuse. Discussion in particular, I think is very valuable in you, on the whole, if you look at the most uploaded or the top ranked discussion, people in the discussion ranking, there are some of the most admired Kagglers, right. And in order to get a good discussion ranking, you have to be technically strong to say things that are insightful, you also have to be helpful and a clear communicator and I think a lot of what it takes to be a good data scientist, machine learning like those are very nice attributes to have. Very often, notebooks require in order to get a lot of attention and outputs, they require you to write clean code. That's not the case for a lot of for competition winning solutions. They don't have to be clean code, right? They just have to be accurate. So notebooks tests that. And then a huge part of being an effective data scientist machine learner is putting data sets together. And so in adding that as a progression step, the goal is to create a more well rounded profile for data scientists and also where data scientists who are strong at different parts of you know, the the process of being a good data scientist sort of show their credit show their abilities in a different area."
21:46,Sanyam Bhutani,"I think it also speaks to the fact that every data scientist is somewhat working on a different task. It's not a well defined profile as of now. What's your stand on the debate that constantly at least comes up on Twitter that Kaggle is not real data science. And I have to say it's not usually from Kagglers, people who haven't been on the platform, do you think there are some aspects that are yet to be improved? Or what's your stand on that question?"
22:12,Anthony Goldbloom,"If you, I think there's some, some validity to the statement for sure. If you think about what it takes to tackle a data science or machine learning problem, you have to start off by translating a business problem into a, into a machine learning problem, then you've got to actually solve the machine learning problem, then you've got to productionize it and then you've got a monitor, deploy. And after you've deployed, you've got to monitor the model in production and make sure it's continuing to perform well. My view is that Kaggle is weakest on the last bit, the deployments and model monitoring. We also get a lot of, people often say, you know, we're not so good in the problem set up because you, you don't get the experience of setting up your own problem and mapping a business problem onto a machine learning problem. I actually push back on that a bit. Because I think one thing that Kaggle is very useful for is giving, giving. If you compete in many Kaggle competitions, for example, we've got this new new thing called Data Set tasks, which kind of define tasks on data sets. One thing they do is they are examples of places where people have mapped business problems on to machine learning problems. So if you see many, many, what it looks like to have mapped a business problem into a machine learning problem many, many times means you're much more well prepared for when it's your turn to do it. So I actually think we do. We are somewhat useful there. The place and this comes from Vladimir Iglovikov. I hope I'm pronouncing his name right one of our Grand Masters who works at Lyft. He tells a story of where Kaggle has been very useful for him. What he has found is it, he's much much faster than people who don't compete on Kaggle are getting to a good answer, so he talks about a problem lifter been grappling with for you know quite some some months and he's a very well credential people. He took the problem home over the weekend and had a really good solution by the end of the weekend and that that's what I, you know that that was a useful contribution he was able to make at Lyft and and that's what Kaggle trains you for. At least Kaggle competitions, I should say trains you for trying to, for picking up a dataset, getting familiar with it very quickly and getting to a good answer very quickly. And so that's it. If you look at the skills, I think we're weakest just to summarize my answer, I think we're weakest in helping people learn about model deployment model monitoring. And I think we're strongest in helping people get a good feel for when they get a new problem, how to get to a good answer very quickly. And I think that's a very, it's not everything, but I do think it's a very useful skill."
24:58,Sanyam Bhutani,"I'd also like to say but there's no platform that offers this broad array of everything where you can even go and learn on Kaggle learn. Then go ahead and start with starter competitions, eventually work your way to featured competitions, as they will call initially. Can you speak to Kaggle learn, how did that idea come into play? And what are your next plans?"
25:21,Anthony Goldbloom,"Yeah, actually, before I do, I want to add one more comment. Well model, model model deployment and monitoring is an area where we don't have a lot. We are hoping to, we are trying to fill in gaps over time, right. And so Kaggle learn is a fabulous opportunity for us to educate people on, users on deploying and monitoring models. So it's something, it's not something I'm committing to us doing, but it's certainly something we speak speak about. So it's a gap we're aware of, and it's a gap we would like to feel. Coming back to your question on the origins of Kaggle learn, really, it was very opportunistic, Dan Becker who was a Kaggle Master, maybe, I don't think he was ever a Kaggle Grand Master. I forget that he had worked;"
26:08,Sanyam Bhutani,During the time when Grand Master wasn't a tier.
26:10,Anthony Goldbloom,"Yeah. Yeah. Right. In any case, Dan Becker worked for Kaggle. Uh, I'm, forgetting exactly when. And then he left to work at galvanize helping with air and data robot and both those companies helping with their data science education, curriculum and efforts. And then he ended up, we started chatting to Dan again maybe a few months after we joined Google. And one of the reasons we lost Dan was he was very interested in data science education and being at Google we were better able to yeah, find something like Kaggle learn or something focused on data science education. So Kaggle learn really was completely, you know, Dan set the vision for it and came up with the idea that ultra short form data science education where they the goal really wasn't to teach you absolutely all the theory but rather, just to give you enough to, to know what code to write so that you can go off and do your own projects on Kaggle. So the idea is just to get people from zero to one, and just enough, start tinkering yourself. And and, and that was was and is the goal with Kaggle learn. Now we have Alexis is leading that effort. So I think;"
27:41,Sanyam Bhutani,Matt as well has joined the team. I think it's it's been a while since he's joined?
27:45,Anthony Goldbloom,"Matt's not with us. Now so it's Alexis is, Alexis owns it and and she's got very exciting things planned for. Yeah, for 2020."
27:58,Sanyam Bhutani,"I'd also like to mention to the audience. There are some workshops that keep happening. I think we can follow those through the newsletter and through the YouTube channel, the workshop efforts, and even the paper reading sessions."
28:11,Anthony Goldbloom,"Yeah, so the workshops, the idea behind the workshops, just to explain is Kaggle learn courses involve a lot of work. We have checked, we put set exercises and have checking code and they're really, they're quite heavy to produce. And we wanted a way to produce content, you know edu content in a much lighter way. And so every now and then we create these workshops and the idea is more they're more of an ad hoc spur of the moment kind of thing and, and, and they don't have all, they don't have all the infrastructure requirements that a Kaggle learn course has, you know, we can all learn we've added functionality more recently like completion certificates, progress tacking and and things that have a much heavier infrastructure emphasis. And so the workshops are a good way to, to be able to push things out in a lot of light way. There's a if there's a cool topic that someone somebody wants to run a workshop on."
29:18,Sanyam Bhutani,"Okay. Now, before we talk about Kaggle community, I'd like to talk about the team behind all of this community. I think it's less than 100, at this moment, could you speak to the team? How did you;"
29:30,Anthony Goldbloom,"Yeah. We're about 50. And actually, fun fact, we were always much, much smaller than people people ever thought. We were only 717 people. I was always really proud of that, that we were able to get a lot done as a really small team. And one really nice thing about joining Google is that, you know, we have we have more resources, and just the caliber of Google engineers is very, very strong, and so scaling our engineer team, you know, one of the bottlenecks it always been scaling the engineering team at the caliber we wanted was very, very difficult by area is a is a hard place to recruit engineers and Google is exceptionally good at it. And so it's we've been able to grow the team. The maybe one of them, the things that Kaggle did very early on, which had its pros and cons was we really only hired out of the community or primarily hired out of the community. But the problem was that we didn't only need people doing data science, right? We had data scientists doing marketing, data science doing, data scientists doing competition, sales, and so on and so forth. Yeah, yeah. I mean, education, I think is more closely more closely lines up with what a data scientist, what some data scientists want to be doing, have the skills to be doing but in other cases, maybe that's not such a good fit and I think one of the things that has changed over the years is we sounds like an obvious thing to do, but we have people who are specialists in their various areas, you know, running their various areas. We have somebody running marketing, we have somebody who is a marketing person, we have somebody running a web developer, right, because he who is a developer, advocacy person, and so forth. So it's, it's, it's, yeah, that's, that's a big change in the way the team runs. And the way we're organized is, we have all of product and engineering, reporting into bin and then and that's broken up into competitions, engineering, data sets, engineering, notebooks, engineering, community engineering, which is all the glue that sticks things, you know, keeps things together. And then we have outside of that we have marketing DA Kaggle Learn competitions, data science. And competitions, project program management and program management is the, you know, the actual signing up of competition customers. So that's that's a rough outline of the team. And I think it's about 35 in engineering and and around 15 in other functions."
32:19,Sanyam Bhutani,"You just outlined all of the areas of Kaggle, do you have any one area that you think is underrated right now that you wish had more recognition? Or do you think it will recognize?"
32:30,Anthony Goldbloom,"I mean, one of the areas one of the things that I think has the most potential is is data sets. I fundamentally believe there should be a place in the world sorry, unrealized potential is data sets. I fundamentally believe that there should be one place that you go to find public data. And there's been many, many companies and projects that have attempted to build this. I think that Kaggle's data sets platform is further than anyone has ever gotten before. But that's not very far. We are a long, long, long, long way from being so compelling that we're the first place you go to find data sets. But maybe they illustrate this point, you know, if you want to find a public data set, your best bet currently is a Google search. Right. And that is not a wonderful experience. And I think there is an opportunity to create a dramatically better experience. But I gotta tell you, I'm pretty excited. Like, there's no part of Kaggle that I'm not excited for. So let's take notebooks for instance, the dream with notebooks is that nobody ever wants to start a project with a blinking cursor. So wouldn't it be wonderful if we could have an example of how you approach just about any dataset or use case that you could ever want to approach right? So that any project you're ever looking at on any kind of data set, you're the first thing you want to do is just do a very quick quote unquote literature review on how other people have approached that problem so that you're not starting from scratch. With competitions machine learning, we have a very powerful functionality now in being able to run competitions in notebooks. And so that gives us way better support for things like time series which have always been hard to do, you know, I would love for us to you know, you can imagine some future and this is not something sort of actively being worked on but some future where we have like an evergreen stock market prediction, crop crop competition and evergreen weather forecasting competition, you might have noticed we just launched this Connect X competition. So this is our first step into things like agent based reinforcement, learning style competitions. We launched the GAN competition last year and so, as I, machine learning and I, I move into these are really, I moved into the these new areas, I think it's very important that competitions can take Kaggle into these new areas as well. And so I think there's a huge future there. And then with Kaggle learn, you know, obviously, there's a lot of exciting areas is GaNS and reinforcement learning and some of the developments in natural language processing and so Kaggle learn, giving people an on ramp, a very, very, very quick on ramp to all these exciting new areas of machine learning, is, you know, we've got to move run fast in order just to stay up with where where AI, AI and machine learning is, but we really want Kaggle learn to be, you know, if you want a, if you want something that just gets you from zero to okay, I know what code to write, we want Kaggle learn, to be that place."
35:51,Sanyam Bhutani,"Okay, do you have any favorite moments from the community where it like, you had any aha moments from the community or any wow moments, from the members of the community?"
36:02,Anthony Goldbloom,"Yeah, I'd probably divide them into two. One is I always like the kind of touching moments like I remember a forum thread, for instance, where CPMP wrote a, you know, a tribute to Chris, you probably know who Chris Deotte. And I just thought it ws a really nice moment. You know, the two other great Kagglers, you know, one of the really great Kagglers, the most admired Kagglers, paying a tribute to another of the most admired Kagglers."
36:32,Sanyam Bhutani,Even though they're competitors on the same.
36:35,Anthony Goldbloom,"Yeah, exactly. So that for me, I don't know that, for me was like a best of Kaggle moments. And that those sort of interactions I find really nice. I also really enjoy their Kaggle days conferences because I get to meet a lot of it. I see the names and the aliases and it's really nice to get to know the people and their backstory, and to the extent that had was had an impact on their lives and careers. That's always very nice to hear from maybe less touchy feely moments of pride. I say I'm proud whenever Kaggle has has a role, I don't think that Kaggle is a place where well, I know it's not a place where things are, it's really a place with where invention happens. But it's a place where invention spreads. And, and so, you know, if you think about, you can read lots and lots of papers. And it's very hard to keep up the paper these days very hard to keep up with what real and every paper says my method it's been, I benchmarked against these six other techniques, and this is the best. And I'm really proud of the way Kaggle helps sort out what really pragmatically works. And I mentioned earlier that random forest quickly came to prominence on Kaggle and it sort of cleared up that it was the best way to tackle most machine learning problems and then 2012 which we had Geoff Hinton and George Darwin a competition. Same years AlexNet, using neural networks and then the competition immediately after another of Hinton's grad students Vladmir won that competition using neural networks. And that sort of helped spark the interest in neural networks. In 2013, people started playing with GPUs, particularly when Theano was released. And we helped spread the the power of GPUs more broadly as a way to train neural networks in 2014, extra boost took over from random for us. And so it became an image recognition it was obviously neural networks on structured data, it was XGBoost and then, at that time, natural language processing, sometimes neural networks and sometimes that has obviously changed but sometimes nerural networks and sometimes Old Style information retrieval approaches with extra boost, 2016 there was a medical imaging competition that was won by units, which are now become the standard way of doing segmentation. So those are the things and units, I think were originally published it at a medical imaging conference. So I think I think we have played a nice role in helping them spread to the broader machine learning community. And so now they use for, you know, masking for autonomous vehicle, use cases and so forth. So, you know, I'm proudest of the role we've had, I'm really proud of the role we've had in helping good ideas spread. And then more recently, giving more access to datasets, allowing people not to have to start with a blinking cursor and also the the impact we've had on people's careers. I think a lot of people will tell you that Kaggle's had a meaningful impact on their careers and that's something I've also it's very tangible than I'm proud of. So yeah, I guess a lot, a lot of high moments for sure."
40:04,Sanyam Bhutani,"I can really say that a major portion at least, or almost all of the things that I, the little things that I have learned of data science are from Kaggle. I think many people miss out on the point that Kaggle learn is not just the point where you get to learn but also the fact that you get to apply something, just reading a paper is not good enough, you get to apply that to a competition. Coming to the tiers, so when you introduce the Grand Master, did you expect that to take off into a direction that the Grand Masters get recognized as the heroes of the field by default, and if you have any plans of reintroducing another tiers, the maybe the April troll from earlier the April Fool's troll from earlier?"
40:46,Anthony Goldbloom,"Yeah. With tiers, the the person behind it is Miles O'Neill, who runs our design team and we're very lucky because Miles as it happens, has a background in designing board games and so;"
41:03,Anthony Goldbloom,Okay.
41:05,Anthony Goldbloom,"Is is his past. You know, we're very lucky to have somebody who happens to have that background, who was sort of well, well, yeah, well positioned to design the progression system. He, I think the week that progression system launched, it was maybe one of the more unpleasant and stressful weeks he's had. Just because it's it's a very hard thing to launch, you know, that there's often the kind of thing that takes a while to figure out whether it's going to have a positive impact or not. And, and I think there were a lot of, when I think the first progression system, maybe I didn't remember maybe I designed it, I kind of, maybe I didn't, I figured how it was designed. It was it was not terribly well thought out and Miles put a lot of thought into the updated progression system and I think we're very lucky to have somebody like Miles design it, certainly it's had a really nice impact on the field was interesting when we launched datasets progression. It was met, as a tier, it was dramatically less controversial. And not to say it wasn't controversial, but some of the early changes we had made were a lot more controversial. Definitely. Definitely a lot you know, where sensitive and and disturb like many others, because Kaggle credentials have become pretty well well known and well regarded. We have a responsibility to try and make sure we preserve their integrity and so issues like pride plagiarism, of course, are problematic and not not saying we're perfect, but we certainly do our best to keep those keep those sort of things, keep the tears is and they're the rankings is and the credential as pure and as as well as meaningful as we possibly can. Since it has become an important part of data science, machine mining infrastructure."
43:16,Sanyam Bhutani,"Can you speak to any further actions that you, the team is looking into to reduce plagiarism and even cheating in the competitions of the very few, but quite a few instances have come up any few steps you are taking us on the team?"
43:32,Anthony Goldbloom,"Yeah, what we've got for plagiarism particularly. We've got a couple of hosts that we put out with guidelines on what's acceptable and what isn't. And, and then we have report functionality and we go through, we go through the the, the threads that are reported, and we we sort of cross reference those against the the guidelines that we've posted. There are definitely wise that within our guidelines that people are still able to, I don't know if abused, the system is quite the right word, but get high rankings in ways that are maybe less pure than we would like. But on the whole, if you, you know, I've mentioned this before, I think discussion is probably the most controversial tier. When I go down and I scan the list of the Top 10, it really rings true as a very strong top 10 list and people that really are admired and you expand that out to the top hundred and that holds true to a bit of a lesser extent, but it's still largely a case like if you if you look at the top hundred, I'd say 95 would be people that are very, very, very widely admired in the community. And so, while the, the, the instances where somebody has gotten a high read, write, write writing in a way that are ranking in a way that people look at us a little bit gray exists. They're small enough that that, you know, we think that the system as a whole is doing a good job and to be honest while the ideal might be zero, I think it's it's just not possible. "
45:20,Sanyam Bhutani,Yeah.
45:20,Anthony Goldbloom,"So I think we're not perfect I think. I think we're overall decently happy with the or we feel like we're dealing with the the importance of our credentials in as, in a responsible way."
45:37,Sanyam Bhutani,This question is from Russ Wolfinger from the AMA - What is the most difficult competition controversy that you have encountered and how did you handle it?
45:46,Anthony Goldbloom,"Yeah, the most difficult. I think there are two that come to mind that the one that I see one that is definitely top of mind, I think it is still the most uploaded discussion post. It's related to the TSI competition we ran in 2017, from Emory. We ran a competition with TSA. It had a $1 million prize. And the idea was to figure out as somebody goes through TSA's US airport security, and someone goes through a scanner. You know, you go like this airport security. Someone goes through a scanner, can you detect whether or not they have a threat object on them, was their idea of the competition. The controversy was that because it was being run by a US National Security Agency that could not pay prize money to anybody, any winner that wasn't in the US. And we, we negotiated pretty hard originally, that the idea was that they weren't going to allow anyone from the New York, not in the US to participate and we think we, we really struggled with that concept. You know, Kaggle is supposed to be global, and we want to give lots of people opportunities. And then we faced a tough decision around whether or not to host a cup once once that got, you know, their legal team was able to, it's not like they're doing this out of a place of, you know, wanting to shut out part of the community or anything like that. It's a, they're bound by US regulation, right? "
47:29,Sanyam Bhutani,Yeah.
47:29,Anthony Goldbloom,"I think their legal team found a way where it was possible to allow people from outside the US to participate, but not to win prize money. And so Kaggle faced a really tough decision on whether or not we can still host the prize. And we decided to host the prize even though it meant that a good portion of the majority of the community were not eligible for prize money which is a clearly a crappy situation. The reason we decided to do it and I still think it's I, even with the community pushback, I think it was the right decision was because we figured this competition existing and the status of being out in the world was better than not being out in the world. And so that's why we decided it was better to host it than to not. You know, maybe given our time again, we might have looked at doing something like you know, prize money, rather than, you know, maybe that's the mistake here was, perhaps we should have said, look, we're better off with zero prize money than than a million dollars, only payable to people in the US. I'm not certain but that could have been a mistake on air?"
48:55,Sanyam Bhutani,"Coming to the other point of this conversation, can you tell us about any favorite competitions of yours or maybe any favorite few. And what roadmap are you looking at for the future of competition? We already have, like you mentioned gang competition, agent based competition, then you have analytics icon competitions coming up."
49:15,Anthony Goldbloom,Yeah. So favorites. Yes. There's a few ways to.
49:21,Sanyam Bhutani,That's a tough question.
49:23,Anthony Goldbloom,"Yeah. I mean, I probably the ones that are most rewarding are ones where I feel like you've been involved in a very large breakthrough. So the competition we ran with Merck, around taking chemical compounds and predicting which ones will be toxic, and which ones wouldn't, that Geoff Hinton one is obviously a bit of a high point. It's an important moment in machine learning. Actually, Kaggle, and that competition was on the front page of the New York Times as part of the an article about the deep learning revolution. And so that was that was pretty exciting. For sure. We ran actually even before that in 2011 a competition with the Hewlett Foundation, around trying to take High School essays and grading them using an algorithm and when we first looked at the problem, we thought our there's no way an essay, can can grade a high school essay. I'm sorry, an algorithm can't grade a high school essay. It actually turned out that the winning algorithms were pretty accurate, which was a big surprise. And I think, if I remember correctly, Will Sikorsky who's now runs the competition's data science team finished in the top three in that competition. So it also brought us a bit closer to Will which has, he's been with Kaggle since 2012. So that's been really nice. That was a nice thing that came out of that competition as well. See, yeah, I mentioned the competition led to unit so this is a couple of units reading. So this is a competition to take MRIs and, MRIs yeah, and measure ejection fraction, which is a, was a way to measure heart failure. Actually, the diabetic retinopathy competition I think we ran in 2014 or 15, with the California Healthcare Foundation was one of the first times that medical imaging which has become a hot field have been applied to AI and it was very strong results, saying I get a lot of pleasure out of the ones that have helped bring about, you know, some of the changes in the industry. And then more recently, I had a lot of fun. I was competing in a competition run by Recursion Pharmaceuticals to take to figure out which si RNA was injected. Sorry, which, what was injected? Yeah, which RSNA was injected into a cell."
51:52,Sanyam Bhutani,I think Bojan found you on the leaderboard and he did mention this on Twitter.
51:58,Anthony Goldbloom,"I had a lot of fun. Was a bit of a rough period because, you know, I'd be at work all day and then come home. And it's like, just one more submission one more idea. And it was like I wasn't sleeping very much. And I was constantly thinking about how do I improve my submission? I competed in that competition, because it's kind of really curious to see how well auto ML. Very curious about one direction. I'm excited about or curious about for machine learning is direction that these automated machine learning tools, you know, a lot of time is spent tuning, putting together architectures and tuning, learning rights, and so on and so forth. I was kind of curious, you know, how far away from the point where some of this stuff can be automated, so it's with a cool chance to compete in a competition and try out a new technology. So I really enjoyed that one."
52:52,Sanyam Bhutani,Was this the first competition that you competed in? Or?
52:56,Anthony Goldbloom,"Yes, sort of."
52:58,Sanyam Bhutani,Okay. 
52:58,Anthony Goldbloom,"I've competed in Actually, we haven't done this in forever. But maybe in 2011-2012-2013 we used to run these one, one day, 24 hour hackathons. And I used to compete in those. Actually, I remember we did this one of these 24 hour hackathons. And I think this is before Will join Kaggle. I said to him, Hey, do you want to team up? And his answer was, along the lines of, I'm not teaming up with anyone as hopeless as you. So that made me very determined to beat him, which of course I did not do."
53:40,Sanyam Bhutani,"Can you share a little bit of behind the scenes on what goes into organizing a competition, we have guides of how to compete. I am a bad sample of having competed in a very few number of competitions, but what sorts of legal aspects, all sorts of different things I believe would come into play while organizing one."
54:00,Anthony Goldbloom,"Yeah. I'll give you a bit of a high level. But maybe in a future episode you want to have Will or Julia or someone from that team, that will give you the complete inside out. "
54:09,Sanyam Bhutani,Okay.
54:10,Anthony Goldbloom,"How those how competitions around but, we at this point Kaggle has a fairly large pipeline of interest for competitions so whereas in the past we used to reach out to companies and try and encourage them to run competitions. That's, that's not really something we need to do now, we're decently well known. And so we get a lot of requests. We get a request from a combination of teams at Google and obviously the outside world as well. Ideally, as we launch competitions, and this is hard to do, Well, the first thing we try and do is screen for is this likely to be addictive, they haven't is there enough data, is the data labels is likely to have signal and so on and so forth. So there's some amount of pre screening. This is done by Julia's team, which is the PGM team. And so they'll, you know, do our first screening if it if it looks promising, then we'll ask for a sample of data. And then someone on Will's data science team will take a look at the data. And, and then we'll often do several iterations. With the company posting the problem, or the team posting the problem, to get it into a shape where it's a good competition. A lot of the things we will do is we'll look for, you know, I think this is something we're getting better and better at, detecting leakage. You know, the team is very experienced to putting competitions up and so have a pretty good sense for where the vulnerability, the leakage might be. I know it might seem like, it might seem like there's a lot of leakage in competitions. I think you'll find that was much less today than there was, say 12 to 18 months ago, the team's really put a lot of emphasis on it and has got a really, really good good sense for like the number of instances of leakage so they could detect it before our competition goes out, was really high. So even if you see it's sort through every now and then it's nothing compared with know what was originally in the data set. For sure, and so that's that's a big emphasis and a big thing that the competition team does and it's very important functional competition data science team because even if a company is experienced or team is experienced at tackling a data science problem, they're not trying to attack their own problem in a way that Kaggle users are and, and so it's it's making sure that the competition is not attackable, is a big priority. I remember you know, Wendy from our team often gets a you know, she talks at meetups etc. And she has this line shelf and open up when she's talking about how competitions work. She says, you know how many of you have thousands and thousands of people scrutinizing your work very closely. "
57:12,Sanyam Bhutani,Hehehe.
57:15,Anthony Goldbloom,"It's, it's definitely a tough role. Being a competition data scientist is a tough role because you're one person trying to find the vulnerabilities in the data set. And on the other end, you've got, you know, sometimes many thousands of very determined very smart people trying to attack the same data set, so they have a really difficult job. They do a really nice job of it."
57:35,Sanyam Bhutani,"Now, zooming out a bit, Kaggle has become the default home of data science. You have research coming out of Kaggle machine learning frameworks coming out of Kaggle. Even during the initial days, Forbes had recognized your effort towards this. What keeps you going still because now it's the default home of data science. And what do you plan in the next few years for Kaggle and yourself?"
58:00,Anthony Goldbloom,"I think a couple of different areas where I think we can do a lot better. I think that Kaggle is not as important. We are not, one of our big focuses in 2020 and beyond is becoming more and more important to data scientists daily lives, right. So I think that we have the biggest positive impact on those who are very, very, very heavily engaged in Kaggle. But not everybody can compete in, you know, I said to I competed in machine learning competition for recursion, and I was exhausted by the end of it, right? It's very hard. Not everybody can, can spend as much time on Kaggle, as many of the Grand Masters. And so you know, that a lot of the benefit, I think there's tremendous benefits to spending a lot of time on Kaggle, but it's just not realistic for everybody. And so, one of the things we want to achieve is make Kaggle more useful to more people who only are able to interact with the site and a lot of white way. Put differently, the way we frame the goal internally is we want the majority of the world's data scientists using Kaggle on a monthly basis. And that could be to reference notebooks that are written on problems that look very similar to problems they're tackling. So you never start with a blinking cursor. But you always start with somebody else's notebook, as inspiration for a problem you're tackling. Let's say you're looking for public data. The first place you go to look is Kaggle data sets. Right? So I think we're, we're extremely valuable, we're extremely well, I think we're very valuable to people who spend a tremendous amount of time on Kaggle. I'd like us to become a daily weekly monthly habit for not just those who are very active on Kaggle, but, you know, all, the majority of the world's data scientists. Does that make sense?"
59:50,Sanyam Bhutani,"Yeah, now my final question;"
59:52,Anthony Goldbloom,"Maybe, maybe just to pause on that for a second. This is one thing I think StackOverflow have really achieved, there's probably not a week or a month that goes by where a software engineer, and it, probably a machine learner as well as not on StackOverflow, right? "
59:53,Sanyam Bhutani,Yeah.
59:58,Anthony Goldbloom,That's something we have not yet achieved with Kaggle. And that's something I'd like us to reach for.
1:00:12,Sanyam Bhutani,"Now, my final question in this amazing conversation to you would be what would be your best advice as the official head, if I may, of Kaggle to any new Kagglers or any novice Kagglers that are signing up today in 2020? "
1:00:25,Anthony Goldbloom,"Yeah, I guess I feel somewhat unqualified to answer the question. You've interviewed on this podcast, far more talented people than me but I guess one vantage point I have is I've seen a lot of people go through Kaggle and be very successful through Kaggle and probably the best advice I have, the biggest trap that I think someone falls into is trying to do things too complicated or, and I think it's fine to start simple. You know, maybe you're tackling a binary prediction problem. And the first day you tackle that problem, you submit all ones. Okay, so it's not going to do very well, right? But. "
1:01:03,Sanyam Bhutani,Yep.
1:01:04,Anthony Goldbloom,"You're on the leaderboard. And maybe the next day you write on if the if-then-else statements, and you go from all ones to some improvement. And and then the next day maybe you train a human linear logistic regression model in Excel, I don't know this super simple. And if you can make very, very, if you start very, very simple and, and then work up to more and more complexity over time, you'll find that and, and initially, don't worry about your leaderboard score, just try and make an improvement every day. Every day you come back, you do a little bit better and a little bit better. And I always say that between somebody who knows a bit of coding and a bit of a mess statistics, and the cutting edge of machine learning is probably only a about six months worth of work, right. And so if you can, it's not like there's a huge volume of things that you need to know. And so, if you can, if you can have that goal of starting extremely simple and making a little improvement each day and a little improvement each day, after, after 190 days, you'll probably end up a very good machine learner. And so that's, that's the number one advice and the trap that I see people fall into is the converse of that I'm not going to submit until I have something really good. I'm not going to submit until I have something really good. And that's a trap that I think a lot of people fall into and they become discouraged because it's quite hard to, even the best Kagglers you know, very often are starting with very simple things. So, you know, start simple and build up and and iterate as opposed to, you know, waiting to submit it until until you have something very good and now the other message is that Kaggle is not only machine learning competitions, you know, some people find that concept intimidating or, and there are other ways to learn as well. Data sets, you know what now we have tasks on data sets. So if you're looking for a structured problem that isn't in a competition format, and have a look at the tasks that other users have set out on data sets, you know, look for a data set that you find exciting or interesting and reviewed, read the kernels on it. Think about what problems might be interesting to tackle on that data set that other people haven't written a kernel on or write that kernel. Or not. Sorry, I got to get used to calling them notebooks. "
1:03:41,Sanyam Bhutani,Hehehe.
1:03:41,Anthony Goldbloom,"Yeah, those are some of the ideas I have. But yeah, start simple and build up. Don't try and solve solve everything all at once."
1:03:48,Sanyam Bhutani,"Awesome. Before we can do the interview, do you want to mention any platforms where we can follow you and your work. We'll have your Twitter handle linked in the description, any other platforms?"
1:03:59,Anthony Goldbloom,"Yeah, I mean today the extent that I post publicly it's only on Twitter. Moderately active tweeter, tends to come in bursts and I have time I'll tweet something or respond on Twitter and busy and then I don't so, I wouldn't say I'm super active anywhere. "
1:04:17,Sanyam Bhutani,Okay. 
1:04:18,Anthony Goldbloom,"But to the extent that I am, it's, Twitter is probably the best place. "
1:04:22,Sanyam Bhutani,"Okay, we'll have that linked in the description. Anthony. Thank you again for doing this interview. And thank you so much for creating Kaggle for all of the data science community."
1:04:31,Anthony Goldbloom,Thank you very much for having me.
1:04:39,Sanyam Bhutani,"Thank you so much for listening to this episode. If you enjoyed the show, please be sure to give it a review, or feel free to shoot me a message. You can find all of the social media links in the description. If you like the show, please subscribe and tune in each week to ""Chai Time Data Science."" "
