Time,Speaker,Text
0:13,Sanyam Bhutani,"Hey, this is Sanyam Bhutani and you're listening to ""Chai Time Data Science"", a podcast for data science enthusiasts, where I interview practitioners, researchers, and Kagglers about their journey, experience, and talk all things about data science.

Sanyam Bhutani  0:46  
Hello, and welcome to another episode of the ""Chai Time Data Science"" show. In this episode, I am really honored to be interviewing one of the best and most active current Kaggle commentators. Dr. Philipp Singer. Philipp is a senior data scientist at UNIQA Insurance Group and he has obtained a PhD in computer science. He's been a distinguished researcher and of course a distinguished Kaggler. His research endeavors include several scientific honors, including best paper award at the renowned worldwide web conference. In this interview, we talk all about his journey into Kaggle. And data science broadly speaking, Philipp has received medals in all of the competitions that he's entered including four gold and silver, of all the five competitions that he's entered. So we of course, talk all about his journey into Kaggle. His approach to competition approach when teaming up and the distribution of efforts in a team, his Kaggle journey what com-competitions interest him, and how he approaches any competition in general. We also talk about his recent gold finish on the IEEE CIS fraud detection competition, where his team title the zoo landed another gold medal, and we discuss all about the solution. I'd like to thank everyone who submitted the questions via the AMA section and of course to Philipp who agreed to the AMA section. For now is my interview all about Philipp's journey into data science, Kaggle and his Kaggle experience. Please enjoy the show.

Sanyam Bhutani  2:45  
I'm really excited to have Dr. Philipp Singer on the ""Chai Time Data Science"" show. Thank you so much for doing this interview, especially the AMA section."
2:53,Dr. Philipp Singer,Thank you for inviting me. I'm glad to be here and hope I can answer some questions.
2:59,Sanyam Bhutani,"You hold your PhD in computer science and how if I'm a traditional computer science academic background, could you tell us how did data science start to come into the picture for you? And how did you get started with data science?"
3:14,Dr. Philipp Singer,"Um, yeah, I studied I studied traditional computer science. It is rather called the studies was executive called software engineering, software engineering and economics. So it was kind of a bit of a mix of computer science and some economics courses but was heavily focused on computing size. And since I studied in Austria, and I did a bachelor's and a master's program basically. And in our master's program, we could kind of choose between a few different topics. At that point in time, data science was not a term yet, I guess. It was it was rather like there was some stuff in the area of like, a little bit of artificial intelligence. I'm a little bit of, I don't know, data mining and those areas, so there were a few courses which I attended."
4:06,Sanyam Bhutani,This was around 2010 if I got; 
4:09,Dr. Philipp Singer,"Yeah yeah, exactly, exactly. It was around that time. Um, and yeah, and then I attended a few courses from one of the professors there. And it was in the area of data mining and what you would know now it is called data science. And then I did a few projects. I did the master's thesis in that area and my master's thesis I I studied some some data on Twitter. So I was like, crawling with the API, analyzing some some topic of things there, and so forth. And that's a bit how I got into it. I would say [Okay] um, yeah. [Got it] It was it was kind of kind of, like the institute was called knowledge engineering or however you want to call it and then from there on, I decided to do a PhD in that Institute and that kinda is when also the whole field kind of exploded. And and I got into the field."
5:10,Sanyam Bhutani,Got it. You're currently working as a senior data scientist at the UNIQA Insurance Group. Could you tell us more about the problems you're working off Kaggle and where does it in your life currently look like?
5:21,Dr. Philipp Singer,"Yeah. Around two and a half years ago, I started working as a data scientist there. It's it's very large, it's basically largest insurance company in Austria. There are a lot of also outside things in Eastern Europe so it's it's kind of also spread across Europe. I mean, maybe to give a bit of background how it is in that field in in Europe and specifically Austria, like you cannot compare it to anything that is in the US or the this in, gone over there ever, like so. It's very slow. Companies just try to get into the field I would say. "
6:04,Sanyam Bhutani,Okay. 
6:06,Dr. Philipp Singer,"So it's it's kind of hip and nice to have a data science team and to have and to do something with machine learning artificial intelligence. There is every where a lot of consultants running around doing this things. I think we all know that. Yeah, so we are a very small data science team. Basically, we have freed free data scientists. dimitri.us is called.on calories is also my colleague, I'm always collaborating with him on character as well. And what what we try to do is basically and enable the business units within the company to do some more sophisticated stuff with data that enables them to do some better business decision making it also improve the processes internally and externally. So that means we need to talk a lot with business units, we need to talk a lot with with them and try to figure out what kind of problems they're facing. And then we try to bring some value to them by basically implementing certain models or other tools around data, and then tried to also put that into production. So we even though we are only three people, we tried to be an end to end stop shop for them. So that means starting from getting the requirements implementing it, and also what we need to also do ourselves is like deploying the models and putting them into production. And because there are no processes around those things yet within the company, so that's what we try to do."
7:51,Sanyam Bhutani,"Got it, for reference for the audience. This usually is a multi team task and involves multiple people from different teams working on different tasks, most of the time."
8:00,Dr. Philipp Singer,"Exactly. Yeah, and I mean, yeah, that that's some some there are a lot there, of course, a lot of lot of challenges you need to face. But at the same time, you also have some kind of early early adopters advantage. And you can also steer the strategy in that area bit, which is also something something nice. And also you learn a lot of new stuff you you don't learn if you're just modeling or something like that, because data science in itself is like a very broad field. And that means it's not it's not only about modeling, but it's, it's a huge part, of course, but there are other things like putting things into production, talking with business units and those things around it, which is also something where you can try to learn things if you try to do them. "
8:53,Sanyam Bhutani,"Okay. And all of these tasks again, are handled by the three members in the team."
8:57,Dr. Philipp Singer,"Yeah, I mean, of course, there. There are IT departments and so on, but they are then rather responsible for, for example, let's say we develop a model, we developed some API around it, we put, we deploy that API. And then what they just need to do is that they call this API from the front end systems or something like that. This is not something we can do everything before that, and it's something we try to do. And that's where a lot of work needs to go into. Yeah. [Got it] And the nice thing is, I mean, the nice thing is that we don't have any kind of topical topical focus. So we also tried to work with different types of topics. So in an insurance, you can think about automatic claim management, you can think about pricing, you can think about risk management. So we try to also diversify therapists."
9:49,Sanyam Bhutani,That's really interesting now talking more about your research path you took a CS as a career path and you were a renowned researcher in that for the audience and I'd like to mention that you have multiple research papers including award from the best as as the best paper of worldwide conference. Could you tell us more about why you chose to initially take up the research part? And did once you found data science reduced? start finding any parallels between research and data science?
10:23,Dr. Philipp Singer,"Yeah, as I said, I kind of started to like it at the end of my masters and then it was kind of natural for me because I had a really really good supervisor, my master's thesis and really good mentoring him basically. And then he, he offered me to do a PhD which was, which was for me, kind of a natural transition done, took it. And in the PhD, I learned a lot. I learned really a lot of lot of things. Maybe even more than during my studies, I would say [Okay] so I researching itself is like very you learn a lot of things outside of hardcore programming outcome modeling, like how to approach a problem, how to do experiments, how to, how to think critically about things, how to also criticize your own work, how to never be satisfied with something, which I think kinda important things. [Yeah] So, and also writing papers, communicating the work I mean, in research, it's very, very important that you also communicate your work so you're not doing it for yourself, you're not doing it for your business of whatever you you always need to convince others that this is something that is valuable. Which is also something very learned a lot. Of course, it also has a lot of downsides. Like you write the paper for months and then it gets rejected and, and it's it's, it's also no easy thing. Some people get really frustrated But I was really lucky that I chose a few topics which kind of fits. Then I had my best paper award, which kind of also was a boost towards GaNS, I had a really good supervisor. So it's not always as nice as I had it, I guess. But it worked out for me. Yeah. Then it was kind of the decision for me to not proceed in academia. So the next step would have been to go into assistant professorship to apply for grants more and for for something, so I did a short postdoc and then it was would have been the decision to either continue there. And then I decided I wanna, I want to try out something else and I want to try out to go into industry and have a little bit more impact with what I'm doing. [Okay] So that that was kind of kind of the idea because in research, you develop a lot of things and Sometimes they never get used sometimes. Yeah. [Yeah] It's art and my hope was that the industry at least you can you can put some things into work and really bring value to to people. That's not always works out that sometimes works out and it's still it's still as a researcher kind of tedious process. [Yeah] But I guess I guess it's it's, it's another side of the coin, which is also nice and I learned different things as mentioned before."
13:31,Sanyam Bhutani,"On your website, you also mentioned that you offer mentoring and teaching services. Is that still the case or?"
13:38,Dr. Philipp Singer,"Yeah, I mean, that's I'm not actively doing that at the moment. But this is something I would like to do to some degree because this is for example, something I miss from from academia, where it was also variables also teaching at university where I was also mentoring other PhD students, master students. Is this something I really like to do and which is something I would like to delve into again. So this is I'm something I'm, I will be happy to do more in the future."
14:10,Sanyam Bhutani,"Okay, so for the audience, if you'd like to reach out to Philipp we'll have his website linked in the description. But you're also an indirect mentor on the Kaggle forums, which we'll just talk about. Before that I have a question from the AMA section by [?] They asking, given your experience, both in academia industry, which experience of both helped you achieve such track record with Kaggle competitions?"
14:34,Dr. Philipp Singer,"First, let me mention that this person was actually was a master student I supervised in university so he's kind of connected. We kind of connected on Kaggle again, which is also quite nice. Yeah, I think I think from both sides, you can learn things in academia it's, it's you, as I mentioned before, you need to learn about how to experiment how to to be very precise in what you do, how to think critically how to think critically about what others put out, you, you also need to defend yourself a lot. Like if I if I present the paper at the conference, you you get like 10 questions if like you have accounted for this assumption in this statistical mefferd. And it's just really loud what you're doing. So this is not the case like in in, at least for me, in industry like an industry, I need to convince people that it works and that it brings some value. [Yeah] But how I expected with at least in my company, I'm sure that's different in other companies like there are not that many people out there who can really charge what you're doing methodological events because they are kind of more focused on the business. So if I can show them or this model brings that kind of value, then they're satisfied. How exactly do it. And if I have in my logistic regression accounted for all this statistical assumptions is a different question. But they don't go. So I mean, that's kind of the two perspectives, but bringing them together and being also not always happy yourself and trying to understand the concepts and trying to put them together is I think, something that can be valuable. So on kegley, I guess, I guess specifically, from from research, what what helps me is how to do experiments, how to incrementally improve on what you have, and how to build hypothesis that you then try to evaluate and hypothesis can be, oh, maybe if I add this to my model, it works and then just roll it out and test it and evaluate it. [Yeah] And add more from industries like what works that works. It's it sounds simple, but yeah, I mean, I mean, on Kaggle no one will judge me in the end. I mean, some people judge you how will you do it but if if you will, The price and if you do whatever, you don't need to convince them that this method is is is valid. And this would have made more sense, and it's still something you should think about. But that's kind of a bit, bringing those two things together."
17:15,Sanyam Bhutani,"Got it. Could you tell us more how you started on Kaggle? And what made you sign up for your first competition, which had an amazing result that I just mentioned in the next question."
17:27,Dr. Philipp Singer,"We we at work we can do to the reasons I mentioned before, we kind of focus a lot on the things around modeling, we still model but also based on the things I just said it's mostly you can stop when you have nicely working light GBM model, with certain features that has this kind of accuracy. Everyone is happy and it's not worth until you have kind of really the business needs to improve on that. So so in order For us to be a bit more on the state of the art area we decided oh, let's let's do some things on the side let's let's also let's also research and and try out things and then we got the idea because timidly was already doing it before to just start with some kind of competition. We have a lot of textural problems at work. So that's why it was natural for us to start with some NLP, which was the competition and we didn't have too much experience with deep learning, which was another thing so it started out as us trying to learn something and starting to get in touch with other datasets outside of work that we can play around with and then hopefully also transform those things to our work problems."
18:51,Sanyam Bhutani,Got it.
18:52,Dr. Philipp Singer,"Yeah, then we got kind of hooked. Um, we started we started out doing really well and then yeah, we won that competition and from that point on I guess we were addicted to it and so yeah."
19:12,Sanyam Bhutani,For the audience I'd like to mention you absolutely knocked it out of the park. So I actually wrote down the percentages to sound cool but you have 100% medal success rate with 80% gold medals and 20% silver medals in all the competitions that you took part in; how and did you expect it to be a such an easy experience or any challenges that you faced while working?
19:37,Dr. Philipp Singer,"Definitely did not expect it to be easy and it's not easy. I mean, it's not like we spent one two days and then the results are there you need to you need to dedicate still a lot a lot of time, I think to it in order to be really on top. So that's that's what we we mostly did for the competitions. I found it, I thought, I mean, I signed up to keggers, seven years ago or something like that. I just did one random test submission, I guess at my first competition, which I just signed up to test how it works two and then dropped it because I thought it's impossible to do well, because there are all these great, great people, all these great carriers, all these people I look up to. And then we just started and it worked. And now and now now we do well. So I mean, maybe the message is just don't never be afraid to touch something. Because even if if you don't end up at the top, there are other things you can you can learn on Kaggle. And it's not it's not only about winning. [Yeah] Yeah."
20:51,Sanyam Bhutani,How do you balance time spent on competitions with life and work?
20:55,Dr. Philipp Singer,"Yeah, I mean, it's hard. Probably. I'm probably the wrong person to give good advice. Because it's very hard. I think my girlfriend is not the happiest person if I do a submission, But no I mean I mean i have a full time job and that means I need to spend most of my time during the day working for some other problems. In my spare time and also if it's something that can bring a bit of value to work we can also spend a little bit of time at work but mostly it's in spare time like now Saturdays Sundays it's usually some Kaggling time during the night we we try to we try to optimize our approach more and more to need less time to spend on some tedious things but rather like already know how to do them and and hopefully benefit from that in future competitions but it's not it's not that trivial because every competition it's is unique on its own. [Right] Yeah. I think I think if you're a good data scientist, what we try to do is champion the competitions later on. So not directly in the beginning, because they're a lot of things are shared on Kaggle. You can figure out a lot of things later on and then don't spend that much time because I'm not the person if I start with it, I cannot stop. So I need to I need to go on. We actually did a mistake now we started this new NFL competition and it goes on for two more months. So maybe we should have waited with that. Yeah, but I mean, it's it's so fun to compete on Kaggle. So it's hard for us to to not do anything. But I'm probably I'm open for advice in that how to balance it better. I think it's fine. I think it's fine."
22:51,Sanyam Bhutani,For beginners do you advise jumping in later or starting during the initial days because that's when also some of the learning happens in my opinion.
22:58,Dr. Philipp Singer,"Yeah, I mean, It depends you can, I think there are two different different ways of learning one is to try to do stuff yourself and try to figure out your yourself things which is trying to be blind of what other people put out and try to figure things out yourself. I think this is very crucial. So don't always copy the public kernel and go from there sometimes really start from scratch, figure out your, your, your own pipeline that you're trying to use for setting up cross validation for doing the modeling for doing some feature engineering, doing explorative data analysts and so on. So you, everyone should do that. I think the other thing still is that we live in an information age and there is so much information out there and not making use of it is stupid. [Yeah] So you should also always try to read on the solutions of previous competitions. You should also always try try to figure out when a hint on the discussion forums is important because I think that's a very crucial part that a lot of top characters can figure out quite quickly, like someone posting something about, oh, this feature I have found that this feature is important that maybe people should get into it and most kind of ignore that because and top guy just kind of can make the connection between that and what what they should do. And so that's that's a very that's another important part and so, so also be involved during the competitions if you should start out early or late. I guess it depends on how much time you have. [Yeah] I personally, until now, don't like you do too many competitions at the same time. Basically, we always only did one competition kind of focus on that one into because because then then do you have more focus on something and I mean, but some top guy just tried to do four or five competitions at the same time, which is apparently also valid approach. Maybe we'll go into that I'm not sure at the moment. [Okay] Yeah. So it's kind of you can do both. I think both things available in and definitely try to learn what others others say in the end and also during the competition."
25:23,Sanyam Bhutani,"Got it. In hindsight after you start your own Kaggle and achieving good results, maybe learned a bit from Kaggle as well. How did this affect your professional life? Did you have any takeaways for your off Kaggle data science pipelines?"
25:38,Philipp Singer,"I mean, definitely in the area of modeling, I mean, we learned a lot in terms of specifically deep learning. Also, more about how to how to how to do hyper parameter tuning properly, how to do a blending properly, how to feature engineering some things properly. So definitely that transforms to my off camera work. So, I mean, I learned on camera, how to use pytorch. I learned how to use cameras are located, how to use certain frameworks, all those things I can use now now at work and; "
26:20,Sanyam Bhutani,Okay. 
26:21,Philipp Singer,Yeah.
26:22,Sanyam Bhutani,"That's really interesting. Congratulations on another gold medal. I'm sure it's no surprise for you, but your team, it was your team titled, the zoo, just finished sixth on the IEEE CIA's fraud detection competition. Before we talk all about your solution could you will maybe help me said this is by giving us an overview of the competition and what was the challenges."
26:47,Philipp Singer,"In this competition, it was about detecting fraud in credit card transactions. So we had structural data with credit card transactions mark the zero bond Bond was brought to you and you need you need to predict whether a transaction is fraud."
27:03,Sanyam Bhutani,"Got it. Could you maybe tell us your first go to step-steps, maybe broadly and also for this competition when you got started and how did you approach the problem?"
27:14,Philipp Singer,"In this competition, we actually started late, we started maybe two, three weeks before the end. So usually, I mean, most of the people will tell you that the most important thing is to get a baseline model. Think about in the beginning, already some kind of cross validation set up. So let's say you discuss a bit you've you think, okay, maybe for this problem, we go to a five fold cross validation, we shuffle or if no shuffle, whatever, then you set it up, you try to generate some simplistic features from the data, maybe just taking the data as it is. Usually, the interval or data would be using some light up a model with basic capability. meters and getting a baseline out and then trying to improve from there. That's always what we tried to do to, also, always one of us, prepares the kernel for it. So we have a kernel that we can share, and then have a code base that we can all work from and then go from there."
28:20,Sanyam Bhutani,Got it. Could you maybe also describe your high level approach? And how does your machine learning workflow look like after you have the baselines?
28:31,Philipp Singer,"Yeah, I mean, there are many things that that play a role, first of all, is like trying to trying to find the appropriate features, trying to trying to properly I mean, a very important thing usually is to think about what are you actually trying to model and what is the objective you're trying to model and in this case, it's easy because it was it was easy because it's area under the curve with just a binary outcome. It's kind of white of trivial that you go to all optimizing hour. Yeah, then so you need to decide on that then you need to decide on the CV. Usually you need to think very critically about the CV setup, because that's very important for you to do proper experimenting. You can start with with one thing, but then you should also think about it during the competition. And sometimes we change our CV set up during competition if we figure out some problems with with it along the way. And then we try to do different modeling. We usually start out with doing some light GBM modeling, specifically, of course, on structural data. But then we also try to do other models, like neural networks, try out some cat schools, some logistic regression, whatever fits the problem. And then we also have another pipeline that combines the models for blending and for stacking. So we try to not runway, any models that we fit during the competition, because sometimes even going back to some old models can be available, [Even the BERT ones?] yeah, so what we try to do is usually that we set set set up some either some cloud storage or something that all team members push the models to in a structured way. So we have kind of like a certain setup that, oh, you you push your CV predictions there, you push your test predictions there, you push your accuracy of that model there. And then we have another script that kind of picks up everything and blends it together. So that's a nice workflow that we figured out specifically if you break with others because then you have one centralized storage people push the models to and then you can do easier blending and mixing and and keeping the old models is also available because sometimes you you need to go back a few steps and try to pick up on them and also special typically for blending and for stacking sometimes having weaker models is also beneficial. [Yeah] And so not throwing them away is I think, at least at some point it makes sense to not throw everything away [Okay] that's that's that's kind of the rough workflow I mean then it's very, very specific for for each competition and also of which team members we have. [Yeah] Yeah."
31:26,Sanyam Bhutani,"To talk a little bit more about CV so CPMP the Grand Master of discussion mentions that if you have a good enough CV setup, you essentially get unlimited submissions in a way could you maybe share more CV tips and how does a good CV for you look like?"
31:44,Philipp Singer,"I mean, a good CV in perfect case is something that has correlation with publicly the port because then you're experimenting, you can trust your CV at least for the publicly the port. [Yeah] So if if your CV increases your publicly the board increases. It's perfect. I mean, you need to trust your CV. So you need to trust that it is not overstating something. So for example, for example, if you have a leak between faults and your CV is overstating something, and this doesn't translate to publicly report, you need to carefully think about it. So, for example, in the previous fraud competition, you had, you had certain things that, like there was some something about we can talk about that later a bit. But there was something that user IDs and then if you haven't different folds different or the same user, you kind of overstate the CV in that fold. Now, just as an example, or if you have some time based in time serious problems is oftentimes very difficult to have a proper city setup because you always have leaks, leaks in the sense of that. The training folt leaks to the to the to the to the CV fault. And then you might have some overstatement or understatement so but that's that's very important because that tells you something usually about the nature of the problem and and what you can also expect from privately the board and how to how to go a bit into the direction to also do well on privately the court. But I think in a nutshell in a nutshell, it's about correlation to publicly report conceptual correlation to privately report. I mean, you can think about that and try to think if this will correlate, and, and not trying to overfeed to your city setup because that can also have like, if you do some, let's say you do some optimization on the blending weights and you overstate your city, which might be also problematic."
33:48,Sanyam Bhutani,Which will lead to a shake up as the Kaggle community calls it.
33:52,Philipp Singer,"Yeah, yeah."
33:54,Sanyam Bhutani,"To talk more about your current team. If you could also tell the story behind the team name the zoo, you've used it on multiple competitions. And if you could tell us more about your team, and all of your amazing results have been in a team setup. So why do you decide to team up and your team for this competition?"
34:11,Philipp Singer,"Yeah. The, as I mentioned before we started at work and at work we have we have some cloud setup that we work in, and we have virtual machines in there. And all of them are named by animals. [Okay] So, that's why we decided oh, and we have on our ball in the in the office, we have kind of the pictures of the animals that we named the machine set and the the IP addresses and whatnot. [Definitely] Yeah, that's, that's why we decided oh, it might be a cool name to call it the super folders machines. That's that's how the original idea was born. It has it has other implications, actually, that we only figure out afterwards because I mean, there is this. There's this notion of the model. So in in In deep learning and immersive learning, they have if all those and it's called model su we try to diversify a lot in competitions like doing different things and not focusing on one thing like doing NLP computer wishon structural data and so on. So it kind of also fits there because we have to work with a lot of different types of models. Yeah, that's a bit of the backstory, and then we kind of stick to it because it works out in the first competition. Yeah, actually now in NFL Dmitry got got mad when five minutes we were not called the song so I had to quickly change the;"
35:40,Sanyam Bhutani,I'll try using that and see if that gives me a gold medal. Maybe?
35:43,Philipp Singer,"Yeah, no, I mean, yeah. It's good to stick to something. It also it also builds a bit of the brand, right? [Yeah] People process to and yeah, why we team up I mean, it's it's usually Dmitry he's got thoughts on Kaggle. And me. As I said, we work together, we collaborate really well together, it always works out, I still have to do a solo competition for a final goal. But teaming is so much fun to me. And it's also I tried to team a lot because that's a different kind of learning, I think on Kaggle is that you can learn from other people how they approach problems. "
36:27,Sanyam Bhutani,One of the Grand Masters actually said that it's one of the most underrated kind of Kaggle training.
36:33,Philipp Singer,"Yeah, yeah, like in our second competition, we teamed up with keepa, who is like, top top competitor. Yeah, exactly. Like we learned stuff there. Then we team up with other people. We teamed up with Christopher, aka Dieter recently learned and you learn new things. So I think this is really underappreciated. And this is another way of learning because you can learn how people approach problems and you can learn how to solve problems and how they think and then try to incorporate that into your work. And it also usually, it's really beneficial if you have some kind of different opinions and different viewpoints. And so it has been always really valuable. And it's also fun, really fun to me to work with others. Yeah, but at one point, I have to do a solo competition, or plenty of them until I will. But at this point in time, it's still fun to me to collaborate so."
37:31,Sanyam Bhutani,Got it. How does your workflow look like in a team situation? So how do you track your ideas? How do you distribute your work and how do you track all the experiments and the results and also the ideas
37:45,Philipp Singer,"We have a slack channel. We for example, we have some cloud storage as mentioned before for for storing results we have we usually set some some sheet up to track experiments and looking then write it down. This is something I tried to explore we try to exploit the moment how to improve that. So for example, trying out some some tools like how is it called banded biases or some weights and biases like that. We try some stuff out there, maybe something sticks, maybe not to try to optimize optimize the experiment tracking a bit. For now, it's usually talking about it in slack, having a Google Sheet or some kind of sheet where we track experiments right down to the right on LP write down what this experiment was doing. We we try to also visualize that. We try to we try to talk a lot. Maybe if you ever team with me, I'm a very, very talkative person in slack. Sometimes people don't reply to me for hours and I'm just doing a monologue. But I'm rather a person who I like to communicate ideas quickly, even if they don't make sense and that's also nice in a team because our can pick it up, think about it and then either get neglected or you follow up on it. And sometimes also, like, if it might happen that someone is on the road for a week or so has an idea posted to slack and someone else can quickly pick it up and try it out. So talking a lot is I think, another really interesting thing about teaming. You can [Yeah] and we try to diversify of course, things. It depends a bit on how much time we have in the competition, how much we diversify. But for example, in the previous competition, we we tried to split up quite a lot of things. So Christopher's doing more neural networks we were doing, I was responsible for blending. So one person is responsible for blending, I was responsible for 15 CAD models and light GBM models. Dmitry was mostly focusing on feature engineering and getting the user IDs we will talk about also doing hyper parameter tuning. Eric was also in morning feature engineering and Max on your network so we tried to split it up a bit it's nice if if all people are on a similar level because then you can you can generally you can really split it up um yeah, sometimes it's also not so easy to coordinate if you have more people and I think now the limit is anyways five it was eight before some us but it's it's a different it's also different learning like it's how to work in a team how to split things up. As I said before people try to forget that there is more than learning how to fit the light up a model and more different yes, you can try to pick up from burking such an environment."
40:43,Sanyam Bhutani,You mentioned all of these tasks. How do you decide which stars which person will take away is that a natural flow or do you actually do a call and do I do.
40:53,Philipp Singer,"With discuss it usually on slack just I mean, a few things are really natural like I haven't seen a competition for, for example, not fitting a bit of different models is important, or they are blending or stacking is not important. So those are usually usually things that you can split up like, it doesn't make sense that five people were complaining if it's something you have out of the box anyways in your scripts, and you just try to think about how to properly do it. And then someone does it regularly. Because, for example, the workflow is all your work during the day and then at night, you start with you urine one plan, submit and the next day you work on some models, then you blend it submit and so on. So it's kind of a natural flow. There are other things of course, you need to discuss and decide on who does what and how to split up things. It's always not everyone has same amount of time. So I mean, yeah, so someone has more time at the moment than that person tries to do more. Hopefully, the others can can can be a bit more occupied with other stuff. Yeah. Yeah. I mean, one thing in in theming is to me important is maybe maybe as a suggestion to others, it's, you don't need to have time old all the time. It's rather being honest about when you have time, like telling people in advance away will be on vacation. And this week, it's already enough that people can plan up around it and try to try to set things up."
42:28,Sanyam Bhutani,"That's really great advice. Finally, if we could talk more about maybe a high level overview because it might be too technical of your good winning solution of the competition, and the decision making process behind what led to you picking the architectures and the decisions?"
42:46,Philipp Singer,"Yeah, as I mentioned, the task was to predict credit card fraud on a single transaction, as it turns out, and Kaggle has provided us with a lot of features and also a lot of features that were handed crafted already by the hosting Research Institute or company. So, they had some from in their house build features that they provided us. [Okay] One major thing in this competition was that it was possible to identify a customer in the data. So, one customer makes multiple transactions and then you try to identify what which transaction belongs to which customer, but they did not provide us with the customer these directly, which I find a bit would have been better maybe to provide it. [Okay] So people spend, I guess, at least 50% of the time of the whole competition to reverse engineer those user IDs and those customer IDs. And that was very, very crucial because you can think of if you have a customer with a thousand transactions, it's natural that that customer at least has majority of the same kind of fraud or not fraud. So it helps a lot. If you have, if you have some kind of notion of a customer, you can also build aggregate features of the customer, customer. You can you can think of other ways. So I think that was maybe the most crucial part to figure that out and tend to get well working user IDs. Because then you can add this as a feature or you can do aggregate features. And that helps makes it. [Yeah] We started doing it. We we started to team up as four people and we had some decently working user IDs. But then we merged with Eric, we had some better ideas and then it really helped us because then we could fine tune the are ideas and fine tune then crafting, Dmitry and Eric did most of the work there. It was really like, some complicated things like figuring out which feature of of best, which is the company means what so for example, the cumulative sum of previous transactions over the last 90 days and then try to make sense out of that, how you can reverse engineer the user go to. So as we've been most tabular competitions in the past, there was to meet this guy, kind of some data legal have ever want to call it so at least it was some kind of possible you needed to solve to do well here. Thankfully, at least, the user IDs did not overlap that much to test it them, which means that in test data, of course, you can still identify customers, but you can identify and much less or lower number of customers that you also have been trained to do. So the impact of having those customers is still crucial. It's not as heavy as in training data. Quarter it also public in public read about big data, because publicly the board was timewise, closer to the training data than the private test data toolkit. This is something we figured out this is also you have to think a bit. Why is the public leaderboard score lower than your training safety score generally, this makes sense because you have a larger time gap, you have lower overlap the training data, and then you can try it, try offering all in private, this will be even even more the case. So that led us to not only develop models that work well for this customer IDs and overlapping customer these but also tried to develop models that work well on new customers that you have never seen before. So that's kind of we split this process. blend into into two parts we have we tried to find models that work well on this, what we call overlapping parts of customers that we haven't tested that we also know from training data. So you can call it known customers and unknown customers which come newly to the system and which we have never seen before. I think personally, I don't know it, I think the business value is higher for the unknown, unknown customers than for the non customers. And yeah, I would imagine I'm, I was asking on kadal, the competition hosts what, what really the business goal was, and no one answered, but some, like cgmp will have winners called maybe they can figure out  what was really the case, I think that sometimes just just a little side step, I think that sometimes a bit missing on character that hosts don't communicate what they really want to do which the model because then maybe people would focus a bit more. I mean, you still try to win, but at least maybe try to figure a bit more what the whole stream anyways. So we had two types of models. When we knew in intestate, which part is a non customer, which is an unknown customer at least roughly from our roughly calculated user IDs. And then we blend the different models for those different two parts together and kind of just put them together and that that's our solution. In a very rough nutshell. We use different models we use like cheaper models, we use capitals models, we use neural networks. Those were the three types of models we used. And for those two different parts we use, for example, neural networks we only use for unknown cause customers are not for non customers and capitals only for known customers and not among customers. So it was it was kind of the thing was to think about how to properly set up the city, how to properly how this translates to lead a port, and how the privately the bot will look like. Usually what we try to do is that you have two submissions, we tried to do one more stable self sufficient with very robust plans. And usually we try to do one submission which is more a gamble based of what we think maybe the privately the part will be okay. In this in this competition, the more robust solution of us was better in but the gamble was not too different. So we trust we just tried to we just tried to even improve unknown customer part a bit more because we expected it to be more unknown customers in test like it didn't for all."
49:40,Sanyam Bhutani,"Okay, has that been the case for the gamble submission in all of your previous experiences as will or has the definitive one we know better score?"
49:53,Philipp Singer,"Mostly it's more mostly it's the more robust submission that is better in this in this competition the robust submission is kind of we couldn't decide on all of our top top plans we had so we just took the top seven plants or something and just again blended them together to one single thing. So we couldn't decide so we just took yeah, it's kind of robust because you combine and usually the better working single models then kind of push this corals to blend up anyways so it's kind of a robust thing. In the past I think the gamble the gamble worked definitely in earthquake prediction where we got first place we are we did gamble of trying to sample the training data based on how the test data looks like. But they are we think, I think we even had to gamble submissions because we had the advantage of being low enough on public leaderboard so that we could justify doing gamble submissions, but still one was more gamble and I think the more gamble one was better, but I don't remember. Otherwise, I think it was more most robust."
51:09,Sanyam Bhutani,Interesting. There's a common prevailing sentiment as well that you need an absolute huge amount of compute power to win competitions. Can you confirm or deny that? And what would you recommend to someone who's looking at bagging medals on Kaggle competition?
51:27,Philipp Singer,"I think it depends on the competition. If it's a GPU competition like NLP, like computer vision, it definitely plays a large role;"
51:38,Sanyam Bhutani,Especially Google and the YouTube ones.
51:40,Philipp Singer,"Exactly. I mean, I just wanted to say look at the Google competitions, you have 200-300 competitors, four of them at the same time, and it's basically now with the new chip you requirements specifically are limitations. Okay? It's basically impossible. Another another thing that that people sometimes try to forget, it's not only about fitting the model ones, but it's also about experiment like Yep, if I, if I join a computer vision, or any any type of of competition I want to run during night during day, a lot of kernels, the trail different ideas to try out hyper parameter tuning the tryout, whatever. And then if you're limited by the resources, that's a big role because you cannot explore as many ways in your development of the models as you would like to like, like to do so for example, sometimes, we set up some hyper parameter tuning kernel fork that 10 times and just run it overnight. 10 times if you have if you have more more team members, you can do that even even more. And you know, I mean, with CPU, it still works. I mean, you have two cores per kernel. It's not perfect, but you can still perfectly do it before four cores, right? But in GPU, it's basically possible. Yeah, I mean, you have 30 hours per week. [Yeah] Which sometimes you need to run long Colonel eight, nine hours and then you can do free experiments but week. [Yeah] Which which is hard. I personally have a machine at home. I have not not defensive machine. I've won GPU RTX 2080Ti. Not much, but it allows me to at least to the more simple computer vision,  experiments, experiments and stuff at home. Others use cloud platforms. I mean, there are people on Kaggle who have access to some large large clusters. And of course, of course, naturally, this has some advantages. You still need to do your work. It's not like just having the computing power is enough to fail. But I mean, it definitely helps because you can you can just start it and let it run and you don't need to CPU always changed manually and then restart. So I think it has advantages."
54:05,Sanyam Bhutani,"What would you envision? In terms of advice to someone who has a good amount of money to invest in, in setting up a machine? What would be your suggestion to them? What machine should they look at?"
54:18,Philipp Singer,"Yeah, and personally, I have always been a huge fan of cloud nowadays, I'm more leaning towards building something yourself. I mean, both things are nice, but actually, cloud costs are not that cheap. If you want to do still do a lot of, of experiments and 15. It really depends, but I would I would stick to some customer, like normal customer GPUs from Nvidia, I guess so some artists to a TTI or something, maybe get one or two, get some AMD processor on top. So either some rising or some Frederick were and then build something your own. It's a Something I mean, it always was awesome a bit of my hobby to hardware and building my own my own rig. And so for me, it's fun for others, it might not be that fun. And of course you have to pay for electricity always need to think about that. The thing runs mostly the whole time. So it has disadvantages, definitely. But it's, but I guess there are a lot of blocks and a lot of resources out there to figure out how to properly do it. I kinda like it. It works out for me."
55:33,Sanyam Bhutani,"I'd like to drop a plug. So I actually interviewed Tim Dettmers, who has one of the best blog posts in terms of recommendations so far. "
55:40,Dr. Philipp Singer,Yeah. I think I followed that event when I was building mine. Yeah.
55:44,Sanyam Bhutani,"So if you're interested, do check out the interview of Tim Dettmers as well. "
55:48,Dr. Philipp Singer,I will definitely.
55:51,Sanyam Bhutani,"In hindsight, I'm sure many of the experiments for this competition broadly also would have failed. So you run a lot of experience when you do not make it in, do investigate that as well. And any thoughts on why didn't those work?"
56:06,Philipp Singer,"Sorry, what what do you mean, again, what kind of experiments"
56:08,Sanyam Bhutani,"So while you're modeling or running experiments for the Kaggle competition, many own deliver as well that you'd expect. So you'd expect a model to do well, but maybe doesn't do investigate that as well after the competition ends, maybe. And any such examples of this competition?"
56:25,Philipp Singer,"Well, the most important thing is, at least sometimes, or at least mostly, if you have time to try to think why it didn't work. And then try to make sense out of it, like, oh, I have a brilliant idea about doing some pre processing and then I do the pre processing and then the model works verse on TV and on it'll be that happened to us much more frequently. And for example, the computer vision thing we were doing with with Katina disease, the aptos competition, so and then you try to you mean that's critical thinking and that's that's also important to craft a good solution is in order to understand why something went wrong and or what what frequently happens is all this works well on TV and this works really bad on publicly report and that that's actually a good thing because then you can really try to better understand how the tested the looks like and maybe also critically think about your CV setup, which might overstate something and so forth. There is not always time to explore everything so so of course, I tried to be tried to also after competitions to make sense out of a few things by reading other solutions by merely trying out a few subs after the competition ended and trying, trying to make sense of it."
57:53,Sanyam Bhutani,Got it. You're also for the audience. I'd like to mention you're also very active in the discussions you were in the top 10 on Kaggle in the discussion. Why is that important to you? Why not just keep working on the competitions and going?
58:06,Philipp Singer,"I mean, I got topping in discussion by posting, so I don't think this is too valuable."
58:16,Sanyam Bhutani,"Also, your discussion that I really learned from [Yeah, yeah] solutions, etc."
58:21,Philipp Singer,"Yeah, I, as I mentioned that like always in research to write up my things to do blog posts to do papers. So it's kind of natural that I also tried to do that here, we actually posted at least the description of our solution of every single competition, and I will talk, or I will encourage others to do so as well, because as we mentioned, this is something that other people can learn from. And it also gives you gives you some critical responses sometimes because, for example, someone might see your discussion and seeing, oh, we did it differently. Maybe you should try out this next time and so on. So it's also kind of valuable gives yourself also kind of retrospective of what you have been doing and, and a good summarization of what you have been doing. Maybe I think it would be also make sense for people who are not top 10 to do some write ups because i think i think what what oftentimes happens is that that people who are not on top of the leaderboard still figure out very, very important things and cannot put it together. Maybe sometimes they are, they cannot make quite full sense out of it. That sometimes happens that they posted during the competition, and then some other people make sense of it and put it together and it gives a huge boost. So for example, in the competition that happened, but maybe there is other stuff out there that is not shared yet and which someone might have figured out or someone might, might might might contribute something. Do it. And also post your questions your solutions afterwards. If you want to get feedback, I'm sure people are always give feedback on your solution, maybe give hints of what to do better."
1:00:13,Sanyam Bhutani,"I definitely agree with that. So for the first competition I entered, which was the quickdraw doodle challenge, actually got into the top to 35%, which for me wasn't bad at all. And I created a submission discussion, which was my first goal discussion as well, that these are the things that I did. This is how I approach the problem. Do you agree to disagree, and all of the grandmasters actually jumped in they gave a lot of suggestions which, which is what led me also to getting addicted to caffeine. So definitely agree with you on that."
1:00:43,Philipp Singer,"Yeah, definitely."
1:00:45,Sanyam Bhutani,"This has been an amazing collection of advice from me what for my final question, what would be your best advice or maybe a few tips for someone who's just getting started on Kaggle and machine learning broadly speaking?"
1:00:58,Philipp Singer,"Pick some problem pick some competition and just try to solve it to some degree of satisfaction. I mean, I always have learned the most if I do stuff myself and try to try to craft a solution myself. And so because it's very important that you also get more fluent in what you're doing and don't need to think about every single step all the time, because then you can advance what you're doing and can and can also develop more and more robust solutions. So, I mean, I guess that's the standard advice kegel is really perfect for that. And I love it for that, that it offers so many competitions that you can just jump in. And you can also do late submissions to competition so It even gives you a way, a way to measure yourself without any pressure on on previous competitions. Don't be afraid of anything. I mean, everyone starts out somewhere. Not everyone has a huge background in the area and it gets it gets more hyped area. Every day and people try to jump in and I think you can build your own a good portfolio and and and it's it's important, what I really like is that you also delve into different types of topics. Don't focus on something try to be more diverse because then might help you in different aspects of your professional career. "
1:02:23,Sanyam Bhutani,"Makes sense. Before we end the call, could you maybe tell us what would be the best platforms to follow you and follow your work apart from Kaggle?"
1:02:31,Philipp Singer,"Um apart from Kaggle um, I guess either LinkedIn or Twitter. My Twitter account is @ph_singer. I think."
1:02:42,Dr. Philipp Singer,I'll have it linked in the description. 
1:02:44,Philipp Singer,"Perfect. So follow me there.  That's it, I guess."
1:02:49,Sanyam Bhutani,"Okay, thank you so much, Philipp, for all of your contributions to Kaggle and even the community and best wishes to you for becoming a Grand Master very soon."
1:02:58,Philipp Singer,"Thank you. Thank you, time flies by, one hour gone. Thank you. Thank you."
1:03:14,Sanyam Bhutani,"Thank you so much for listening to this episode. If you enjoyed the show, please be sure to give it a review or feel free to shoot me a message. You can find all of the social media links in the description. If you like the show, please subscribe and tune in each week, to ""Chai Time Data Science."""
