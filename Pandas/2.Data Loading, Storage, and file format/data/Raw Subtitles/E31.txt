Sanyam Bhutani  0:13  
Hey, this is Sanyam Bhutani and you're listening to "Chai Time Data Science", a podcast for Data Science enthusiasts, where I interview practitioners, researchers, and Kagglers about their journey, experience, and talk all things about data science.

Sanyam Bhutani  0:46  
Hello, and welcome to another episode of the data science show. In this episode, I interview Even Oldridge who's a senior applied research scientist at Nvidia currently working on the rapids AI team we talk all about even's journey into the field even has been working in the data science field for over the past decade now even is also one of the faces and my peers from the fast ai community, we talk all about his journey into fast ai, his thoughts around the course best and many best advises for people just taking the course and following the top down way of learning new things. We also talk a lot about his current work at rapid ai. What rapids is currently working on including multiple projects, cudf, cuml, cugraph, and all of the little projects that sit inside of the rapids ai repository. You can find the links to them in the description of this podcast if you want to check them out. We also talk about research and applied research. Open Source, fast ai & Deep learning all of these themes in this single interview Rapids AI is working on many interesting things. If you want to check those out, you can find links in the description of this podcast. A quick note to the listeners this interview with all of the future ones will have properly checked and re-uploaded subtitles. So if you're a non native English speaker, please enable the subtitles on YouTube if you're watching it on YouTube, and I hope that helps you experience along with the written version or the blog post version of this interview which will come out in a few days or in a few weeks from the video release. So you can find a link to the website where this will be released, in the description in the description of this podcast as well. For now, here's my interview with even always all about applied research fast.ai and rapids.ai, please enjoy the show.

Sanyam Bhutani  3:12  
Hi, everyone, I'm on the call with Even, Even thank you so much for joining me on the podcast series.

Even Oldridge  3:17  
Thanks for having me Sanyam, I'm super excited to be a part of this. I've been a fan since it was a series of blog posts. And it's always great to meet with another fastai family member.

Sanyam Bhutani  3:26  
I've been a fan of you through the fast.ai family as I call it, because you're not really known to the person you see the little icons, but it's an honor to have you on the show.

Even Oldridge  3:35  
No, I'm super excited to be here.

Sanyam Bhutani  3:37  
So you're currently working as a senior applied research scientist and Nvidia and you have been working in the research and data science domain for almost a decade now. Could you tell the listeners How did you get started in data science and how did deep learning start to come into the picture for you?

Even Oldridge  3:54  
Yeah, so I was lucky enough to go through an engineering degree and get interested in machine learning and AI very early on, I can remember in my undergrad taking a course in neural nets and ended up almost doing it-uh- was a graduate level course. And I almost did a Master's under this prof I was super interested in this obscure project, which was this like it was called the robot kinetico. And somebody was building a robot brain for cat essential kit. And they were building it the hardware they were building it on with these, this sort of piece of hardware called a field programmable gate array. It's like a programmable machine as they they're still around. They kind of lost the general purpose computing more to GPUs. But they, there was a really interesting kind of hardware platform and I there was a great professor nearby that I ended up going into my masters at there. But from there, I ended up I took a year off after my masters and wanted to take a break. I did a bunch of photography and actually was working as a photographer and from that

Sanyam Bhutani  5:01  
available somewhere online. Can you go check it up?

Even Oldridge  5:03  
Yeah, I have I have a few was the photography. I haven't put anything online but I do have a bunch of Art Online. I can send you the links. It's Yeah, we'll have it linked in the description for broken heart dot com is one of them and flow forum dot com, and that's more of the painting stuff, but took it. But yeah, art doesn't really pay the bills, unfortunately. And I had a professor at UBC who reached out though, you know, he was doing kind of computational photography, and you know, and computer vision. This is all sort of pre deep learning. I'm pretty old at this stage. Outside of things so, you know, when I learned machine learning and like the computer version side of things, it was all SIFT features and you know, you built the features kind of yourself and then the representations downstream and it was a PhD in the intersection between human computer interaction and Computer Vision. And then I did some teaching after that in the Interactive Arts, and in just a variety of undergrad classes just to pay the bills, and eventually got hired at plenty of fish, which is an online dating site, one of the biggest in the world and work, they're heading up the research team. And we were developing matching algorithms using neural nets, doing some pretty cool stuff, actually, their bank distributed neural net systems across, you know, a few hundred thousand dollars worth of Nvidia hardware, looking at using slurm. And it we basically sort of wrote our own version of TensorFlow and have this whole system going to distribute training. And yeah, it was it was pretty, pretty crazy, pretty complex and a lot of fun.

Sanyam Bhutani  6:47  
And this was before, like we had these convenient algorithms that you could easily distribute across. Oh, yeah,

Even Oldridge  6:53  
I wasn't, I mean, slurm and there's some other techniques just for distributing and kind of managing clusters managing jobs and scheduling. But I think they're still in use today. But okay, we leverage those. And then yeah, it was it was from there, I kind of things shifted when we were bought by match dot com, and you know, and it wasn't as like, punchy and startup be and the work I was doing there was, you know, was less and less satisfying as things went on. And so I took some time off. And that was the moment when I discovered fast AI. And Okay, we got into the the deep learning journey that I'm on now. So

Sanyam Bhutani  7:32  
maybe talk more about fast.ai, but I'm curious, you had your start of the career in the Applied Research domain, if I may have what made you pick that as a role? Did you have any interest towards that?

Even Oldridge  7:47  
Yeah, I mean, I'm not sure I would call it I was doing plenty of fish applied research. It was more sort of traditional machine learning, like we were building matching algorithms and fraud detection systems and you know, really trying to I understand social user bases. And it was research because there wasn't a lot of people doing things at that level. And a lot of the work there is sort of on user bases and databases and stores of data that just aren't accessible. It's one of the interesting challenges of recommendation. You know, I'm very interested in that side of things, and currently kind of actively studying it as well. But data sets for recommendation are are rare and hard to come by. And they don't really mimic the sort of real world recommendation problems that you know, that people are facing when they're building recommender systems for So, yeah, it's a very interesting dichotomy.

Sanyam Bhutani  8:39  
Now coming too fast.ai: you started your fast.ai path if I may in, I believe, 2016. [Yes]. And could you tell us how the journey has been for you because I think you've taken all of the fast.ai offerings since then. And you've taken it both in person and online. So also if I made a comparison of both

Even Oldridge  8:58  
Yeah, so all Mine have been as international fellows, so it's all getting sort of online but live, which I find makes actually a big difference. I took the very first part one I took on my own, okay, and I did that, you know, I discovered fast.ai after it'd been posted and Jeremy shared it, and I sort of blasted through that course, it was the perfect time, like I just left plenty of fish, I was taking the time to really kind of figure out what I wanted to do. And I actually, you know, I was at this dichotimist point where I was trying to figure out if I wanted to get into ML Engineering and sort of get deeper into that side of things, or if I wanted to kind of, you know, double down on the data science, but I wasn't loving the data science I was doing at the time was more on the analytic side, and less on the machine learning and I was much more interested in machine learning and then Okay, yeah, fast AI came on along it just the perfect time and I, I sort of I worked really hard in about a month to finish part one in order to qualify for part two, and I will get you know, I applied for part two and and two Look at that as an international fellow that year and I've taken every fast AI offerings since then and look forward to the coming years program I find it's it's just such a good refresher. And you know, there's always something new and something interesting, something different and Jeremy and also Sylvain or, like they work so hard to update the libraries and change things up. But it's, you know, you kind of have to enroll in the course and be a part of it to stay up to date in my mind. So,

Sanyam Bhutani  10:28  
yeah, quick plug, if you want if you're curious how they developed the library. I've already interviewed Jeremy on the Show, so do check that interview out from the description.

Even Oldridge  10:37  
Nice.

Sanyam Bhutani  10:40  
So, you've been one of the faces of the family, as I call it, because as I said, like these are these little icons that become hyperactive during the international fellowship. There's a lot of discussion that happens and I was going through your profile you spent about five days worth of time on the forums in throughout the three years. worth of visits! Could you tell us how your approach to learning the material has changed? Because even your comments and this is an active discussion still to this day about how do you actually learn fast.ai? Because the material is completely different from anything!

Even Oldridge  11:14  
Yeah, I know for me. So I'll give you the kind of the key to learning for me that that really helped. And this was, you know, I posted on the fast day forum and shared it, but I'll share it here again. There's so much depth to what's being offered, right? Like Jeremy in a two hour lecture is covering so many topics and so many ideas and there's so much follow up that you could do to it, that it's just, you know, this this massive bundle of content that you're kind of getting, I find, you know, like when I watch it, I'm lucky if I get a third of it, right. So what I find is watching it through once to get the visuals and to build up the understanding is really helpful. And I try and do that but I also try and download it as an mp3 or A like an audio file and then I stick it on my phone and whenever I'm, you know, I'm traveling or I used to have an hour's commute to work I'm now at home full time. [Okay] doing the work from home thing, which I really love. But in that hour there and back in my biking commutes I would listen to Jeremy and it was funny, it would be like, you know, you get these ideas in your head as you're listening and it got to the point where hearing his voice was enough to start simulating ideas for me. Listen, listen, here's something I would just zone out. And then suddenly I would snap back to reality and I'm there in this moment biking along and I would literally there was a half dozen times in my in my commute back and forth, I would pull off to the side, get out my notebook out of my bike side frantically scribbling notes because I just had this you know, this idea that popped into my head from what he's talking about. So that really helped. I was a lot more involved in the forums back then I wish I had the time to be more involved now. You know in video takes a lot of time. I do plan to get get more involved. I'm I've got two small boys. And that's kind of, you know, family eats up a lot of time. And it's, you know, it's, it's where I derive a lot of my pleasure in life too. So it's, you know, it's it's a wonderful thing, but it means I can't be as involved as I'd like to be, you know, especially with the, the age that they're out of that, you know, one's for and the others just turning one and so it's okay. It's a lot of hands on involvement.

Sanyam Bhutani  13:23  
Maybe in the future, all things will be back on the forums in a while.

Even Oldridge  13:26  
Yeah, I hope so. For sure.

Sanyam Bhutani  13:28  
How is your approach about learning the material specifically change in terms of how much time do you spend coding versus So Jeremy advises us to replicate the notebooks hood approaches, we do take the reader try Kaggle competitions. Any other?

Even Oldridge  13:42  
Yeah, so honestly, the thing that really made it stick for me was picking a project that was outside the balance of fast.ai. I like I've done the notebooks and you know, it's so hard not to "Shift-Enter" through them. And when you're kind of rewriting them, it's like, you've got this cheat sheet and I'm bad when I've got That the I'll just kind of go over. So the thing that really kind of clicked it for me was actually was a mix of Kaggle. So like, I've been interested in tabular, deep learning for a long time through Jeremy's stimulation there, but if you look on the kaggle forums, there's all sorts of interesting solutions related to tabular that I've posted on some of my previous blog posts. And one of them was a denoising autoencoder, which I thought was just this really interesting idea that I wanted to try out. So figuring out how to do that from, you know, from start to finish in fast AI, you know, with all the different layers and all the different complexities and I think the library back then was a bit more complex. I think I wrote it during the 0.7 version. I'm looking forward to rewriting it for 2.0 when it comes out, and I think it's going to be a lot easier to write. Again, I'm hoping so. Because Yeah, back in 0.7, there was many layers of abstraction and so you had to make sure that all those interlocking pieces kind of work together. If you wanted to do something custom, another Inside the bounds of library, boom, what that meant was that I really had to dig in and go deep to learn the whole library through and through. And so I've spent a lot of time poking around the fast AI code base and man the stack there, and you know, figuring it out. And Jeremy and Sylvain have a very particular style of programming. But you know, it's, it's, it's a bit challenging. I think, for a lot of people, when they first get introduced to it, at least I found it personally very challenging to build up that mental representation. And it's sort of like this, this wall that you have to get over the edge here inside, it's this beautiful city of possibilities, right. So it's,

Sanyam Bhutani  15:37  
I think, once you get used to it, it's a mind opening experience, then you really can be able to navigate through the code. [Yes.]

Even Oldridge  15:44  
What I like to see the evolution of the code over the last two iterations has really helped me understand a lot and I'm like, what I want to spend most of my time on over the next little while, you know, in terms of fast.ai really coming to understand that Like that the programming methodologies and like part two last year, some of the deep learning fundamentals is Yeah, it's so critical to know those things and to understand them. Yeah, I've got, like, like I said earlier before the call, I'm not the best math person. And so, it really like for me to understand it looking at code kind of helps a lot. And, you know, and understanding that side of things, reading papers, I'm lucky enough to come from an academic background, so I can read the papers and kind of, you know, get a basic understanding of them. But many times, you know, read the paper and all, you know, have a very high level understanding and I won't really understand the depth of it, and it's, it takes again, you know, dedication and multiple readings, and there's so many papers coming out all the time. I've got a stack of papers on my desk right now all related to, you know, the newest stuff that came out from the lottery ticket. Yes, right? Like there's been. We're new, like, Facebook put out three new papers and Google put out a new paper that they're promoting. And yeah, it's like, just trying to understand all the implications of that. And specification of deep learning is a pretty interesting concept. There's been some really awesome work on embeddings recently that I'm trying to understand and get into Luckily, I was able to find the source code for that. So that's like, that's one of those things where, like, if I can look at the code, he talks about this, too. Yeah. You know, if you've got the paper and you got the code, then suddenly you can see that like, what takes up three pages in the paper and involves 100 years is actually for its describing it in a weird way. Again, yeah, I encountered that there's a there's a really good papers that aren't accounted for that too, that are, you know, they're trying for simplicity in the papers that I've written. And I've tried to write them more along that style on the blog post. So I'm hoping to get back to more academic and more paper writing and that side of

Sanyam Bhutani  17:58  
[things for sure.] Okay. Put the data into perspective for the papers for the audience. This podcast was recorded in the last week of November. So if you curious about when the papers came out, but I am I want to ask you, like many people have this question on the community, how do they find this idea first? And secondly, how do they like, because for many people first is the first coding plus machine learning goes. So it's really difficult to connect the dots to the external projects, after even finding them. So any best advice is you have for them?

Even Oldridge  18:30  
Yeah, I think I mean,

Even Oldridge  18:34  
finding something you're really interested in and passionate about. And it could be something as simple as like, you know, something you like to take pictures of that you build a classifier for, or, I mean, there's a bunch of really good examples. I think passion is a really key aspect to it. You know, like Jason's work on DeOldify

Sanyam Bhutani  18:53  
Helena's, work on GAN's

Even Oldridge  18:57  
[It's] Amazing. I'm such a huge huge fan of me she really got me deeply into the interactive art scene and I'm a fan of a whole bunch of people's there with it. Yeah, but, and there's a few more like those. Oh, her name's eluding me. The she's she's done a bunch of beautiful work on audio. I think she's a Christine. Yes. Yes.

Sanyam Bhutani  19:20  
Christine's been on the podcast also. So a shameless plug again, do check episode out.

Even Oldridge  19:25  
Yeah, she's, she's fantastic. And her work is is stunning. And it's Yeah, it's like these projects that derive from your passions, I think really help you know,

Sanyam Bhutani  19:36  
Even, Andrew. So another example, who figured out how to create a Chainsmokers song remix using deep learning

Even Oldridge  19:42  
Yeah, that was super cool. I really love that piece as well. Yeah, I reached out to him to see if I could get him onto my team at Nvidia but Jeremy already snatched them up. Yeah.

Sanyam Bhutani  19:57  
So how once we find these people projects? How should we go about learning the things that we need to connect to them specially for Kaggle competitions? Because the fast.ai library cannot be just picked up and apply to them, you have to sort of figure out these helper functions every now and then.

Even Oldridge  20:13  
Yeah, I mean, Kaggle a good example to work. So Kaggle is tricky, like, if you want to apply this to kaggle, there's, you know, there's so many aspects about kaggle that art, like, especially for tabular, which is my area of expertise, you know, the feature engineering is a huge component of winning a tabular kaggle competition. And, you know, and then beyond that, there's, there's a huge, like, there's a bunch of different techniques that you need to know like stacking and boosting and on sampling and like how to combine models in an intelligent way that, you know, without those, you're not gonna play silver. So it's, I don't, I don't necessarily recommend kaggle as a like, it depends on the degree to which you're competing. Live. Yeah. And and if you want to just, you know, participate and try it out then then that's great.

Sanyam Bhutani  21:06  
And how many nights of sleep Are you ready to lose? Oh yeah,

Even Oldridge  21:08  
it's like, you know, the one Kaggle competition that I've seriously worked on the champs scalar coupling was a lot of late nights and sleepless work and a lot of hard work and you know, and I wasn't even the primary person working on it it was we had a an amazing intern Robbie, who was you know, she was doing most of the heavy lifting there and really just taking a lot of that kind of the ideas and the iteration that I was working on with her and trying to figure out so yeah, it's it's Kaggle can get so all consuming and so full time. Yeah, I think you know, for a first project, I would recommend something, you know, simpler, something more, you know, more to your interest. And that just comes down to like at the end of the day, the the fundamental thing you need to do deep learning project at this stage is data. Right? Like, fast.ai, will give you everything else that you need. So you just really need to kind of figure out well, what is the kind of data you want to work on? Is it you know, is it images. That's pretty straightforward. A lot of examples, there is a video that's getting a little more complex, a little more fun. And, you know, and then is it audio? There's some interesting stuff there. Yeah, you know, and do you want to do you want to do GANs you want to do like, right, my first fast.AI project that I really got into was in part two, I was sort of derived from my art was the other style transfer. I got super into that and sort of playing around with different methods for style transfer and, you know, really got deep into it for for the sort of the bulk of the course. Okay, yeah, find something you're passionate about. That's, that's my main advice.

Sanyam Bhutani  22:50  
I definitely agree because the passion will definitely reduce your frustration or make you feel lesser of it because machine learning or deep learning right now is very frustrating, and you need To be able to get through all of that process, and before you get that output,

Even Oldridge  23:03  
I can't remember who was describing it to me this way, or maybe was even on your podcast, but it sort of deep learning. It doesn't work and it doesn't work and it doesn't work, it doesn't work. And then it works, you know? Like, it's not like this steady progress. I mean, once it works, you can kind of get a steady progress mode, but there's a lot of it doesn't works before it really starts to like, yeah, often and beautiful or perform well

Sanyam Bhutani  23:31  
or whatever else. Definitely. Do you think you would, if- I may-your "traditional" machine learning and engineering background was helpful for fast day or any advice for someone coming from this background? Because this is a bottom up approach that we're used to and fast.ai takes a different approach. Any advice for such people? [I mean, for me, it was, it was really refreshing. Like I love this style of approaching this style of teaching. I've always been a big fan of you know, this is the really powerful quote. You can do with this and let's learn, you know, learn that and, and take that direction.]

Even Oldridge  24:06  
You know, like I'm coming from an ml period prior to deep learning. So it's a bit different and that, like that stuff I studied in school was, you know, was all replaced, like, you could do a one line function now. Oh, yeah, you could do, you know, the half of my thesis was machine learning based, you could, you could literally write in, in a handful of lines with the right data set, you know, and, and probably, I'm sure you've got much better accuracy than I, you know, at the end of the day. Yeah, it was a Yeah. And so, you know, I'm not sure that that advice is particularly relevant, but from a programming perspective, having a programming background, I think, you know, definitely helped, I think, if you really want to get more complex than what the library already currently does, which is sort of, in my mind where things get really interesting and thought right is is when you can kind of tweak The library to do things it's not meant to do. That's that you kind of need to spend some time learning programming. And I think Jeremy's done a bunch of really awesome podcasts or video casts on that recently recently walking through the library and doing some live coding and

Sanyam Bhutani  25:15  
those are more dense than the Part 2. So

Even Oldridge  25:18  
[please take that with a grain of salt] Yeah, it's really it's it's definitely high level like you need to be it's it don't take that to learn like a language. From a beginner's perspective. This is more for the people with a, like a programming and machine learning background. Yes. To understand his his way of programming, and that style. Yeah.

Sanyam Bhutani  25:35  
That's great advice. So before we talk about your current role, I think there was a Connect for your current job through the forums as well? Maybe you could speak more to that. But I remember you had posted about rapids[ai] on the forums. And later on, I saw Joshua commenting on this and was there a Connect for Nvidia through the forums or is there a story behind this?

Even Oldridge  25:55  
So I didn't actually get connected to a rapids through Josh or through the forums I was into super interested in rapids and what it could do and you know I'm chatting with him I think I might have even been the one to originally posted it to the forums because I thought it was cool. And yeah, I got really into rapids. But I honestly when I first applied for the role, I didn't even realize that it was for rapid specifically. Okay, and it was one of those strange things where I was on Twitter and it just like my current boss posted it as a Tweet: we're looking for people to study tabular deep learning and recommender systems and I just like a light bulb went off in my head like I can do this dogs gonna come on, it's amazing, right? Like so many to continue my journey a little bit because we just talked about the fast AI side of things. I ended up with that sort of knowledge and fast AI and the recommender systems and the deep learning passion. I got really into our I got I got a role at realtor.com and there you know, was very interested in doing deep learning based recommenders and that side of So we did some pretty interesting systems there, and tried to scale that up and I got, you know, a lot of the work was kind of in my 10% time of like, you know, I'm, I'm super interested in doing this, but, you know, for us to deploy this to production isn't reasonable, you know, isn't feasible right now. So there's like, we got to do the stuff to get things into production. But you know, and and I was trying to, like lead the team and I had a bunch of amazing interns who then you know, we hired on and got to work with them on this and one of them. Angela Tao we worked together on on a paper that we never ended up publishing, we made up like, basically analyzing the fast AI model on the denoising, autoencoder, and a bunch of other tabular models to see what level of performance we could hit relative just like to capitalism to  XGboost and LGB models and it was pretty interesting to see the level of performance we could achieve with with proper hyper parameter tuning and some of these techniques like the de noising aspect of the denoising autoencoder It's basically doing a like a shuffle of the data within the column. And so you're dealing with real world data and the models having to learn these distributions within that context. And it's, you know, you can get very similar levels of performance across the different different types. And it's, it's one of those interesting challenges we like, we basically got to the point where we, we looked at submitting into KDD. But by the time we got to the point for submission to we were comparing, we're getting getting into the technical weeds. That's okay. But we were comparing the Catboost and Catboost is, you know, it's a really interesting, great library focused on some really powerful things for categorical variables. And the metric that they were using for comparison on these was the rmse or might be log. Oh, very messy. But you know, the For a lot of these data sets, there was, you know, such a level of imbalance. And such a, again, when you compare that value to accuracy, like what we were seeing in the deep learning models, you know, in terms of like, you know, the rmse curves and the the precision, recall ROC, and and those were, you know, you could find, like, basically, we were, we were hitting points where there was this sort of the steady improvement in performance of accuracy relative to this, this RMSE, and, and then it would hit this point where you hit this threshold and then after that, you know, you're basically past the point of performance improvement that you're just hitting this noisy saturation, exact well, beyond saturation, it just sort of you like it was almost like overfitting. And I could like we were doing the experiments within the deep learning side of things. So we couldn't tell like I could get models that sort of matched or you know, came very close to the catboost errors, but when We looked at kind of the the accuracy of those models, the previous precision recall curves, they weren't as good as the best ones we could get by dialing it back and looking for that sort of lesser target. It's one of the interesting challenges within the field, I think is, you know, like, optimizing for a metric that isn't aligned with, you know, your end goal. And I mean, you can't get you can't optimize for accuracy. That's not like that's not something you can target you can't optimize for in precision or recall, because it's just great. There's not a there's no loss landscape there. So you have to deal with these, these sort of different loss functions. But that's been something that I've explored, spent a lot of time looking into as well.

Sanyam Bhutani  30:37  
Again, connecting the dots for the audience. This is through a library that Even found on the forums, and a project that he found on the Kaggle community and this connecting those dots. This became a full time role. I'm sure it took a lot of efforts and talent. But again, if you stay true to your passion, it it it can make things happen. I think

Even Oldridge  31:00  
No, I absolutely think it was one of those things where I was just like, I was doing it my 10% time and it was a passion and I think my boss saw that and yeah, it was, it was it was pretty interesting though. Like, you know, I was when I interviewed I remember seeing the first person who is going to interview me and I don't think he's been on but he's a Kaggle GrandMaster Jiwei Liu get a just brilliant and really lovely guy and, but I didn't know that at the time. This interview scheduled with the kaggle GrandMaster, you know, as the gatekeeper for this role, that was my dream role. And I thought, oh my god, like, I literally literally sent my we sent my son out to his grand-to my parents and I spent the weekend studying everything reviewing everything going over everything really diving in, but at the end of the day, he was super sweet and really lovely to talk to. It was it was not as it is not the scary interviewers. Okay, yeah, now I'm at NVIDIA and I'm loving it. It's been amazing.

Sanyam Bhutani  32:02  
Can you tell us more about what a day in your life at Nvidia applied research looks like? Do you get to test ray tracing on games? And what problems are you currently working on?

Even Oldridge  32:13  
Yeah, so I'm pretty focused. I'm working on the rapids team, sort of as an extension to the rapids. And I think we'll get into rapids in a little bit. But my day to day is a mix of kind of coordinating with my team and organizing, you know, what projects we're going to work on. We've got a new series of projects and a new team kind of booting up. So we're, you know, trying to figure out the roadmap, figure out prioritization. Some of it involves communicating with other groups. So you know, I'm very interested in recommenders. I met with Facebook last week to talk about the DLM project, and you know, to see both submissions there. I spent the bulk of yesterday working on they call it an RFC, its request for contributions for TensorFlow can be For this project called DL pack, and it's sort of this, I think

Sanyam Bhutani  33:06  
You opened up a PR too. So people, I'll also have,

Even Oldridge  33:12  
essentially, and maybe it's a good time to get into it. So

Even Oldridge  33:16  
rapids is, like, the, this, this amazing kind of, it's, it's the Python ecosystem on GPU. Now. And there's, there's a huge team, there's 75 engineers full time, you know, working on this developing out equivalents to scikit-learn and equivalents to, you know, to a bunch of graph libraries, equivalents to signal processing libraries. And the one that I leverage and make use of the most is, is a library called cudf, which is equivalent to pandas. Okay. And when I say equivalent, it's, you know, there's we don't have full coverage yet. But this is a matching API. It's really meant to Be at the end of the day plug and play, like you'd be able to take your code in pandas and with very minimal changes, kind of, you know, in many cases, you know, import cudf as pd will work. Yeah, you know, in a lot of code. And it's this data frame library accelerated on the GPU. You know, and videos think that what we're trying to do is sell GPUs and so, you know, if we can get people to do data framework, and I mean, part of it is, it's really just well suited to it, you know, in most ETL use cases or most data transform use cases like kaggle. We've got a couple of examples there, or the Recsys example that I've, I guess we'll get a chance to link hopefully, we generally see about a 10 x speed up in the pre processing the data so that's huge. You know, when you're talking about taking a data set that takes a day to process and you can I said, you know, in a couple hours, or, you know, something that would take you a few hours, you can get that down to a few minutes. I mean, that's the sweet spot we're aiming for where, you know, instead of, instead of having to, like, set up a run to generate the data you need for your model. And you know, and then walk away and go get some coffee and read a paper and talk to your coworker and then come back and see and realize that Oh, man, I, you know, I put the wrong line of code in here. And now it's grabbing this variable on Yeah, you know, there's something wrong with the data and you got to go back and you restart it. You do that three times, and the days over and you go home, and you come back and you try that, instead, we want to get it down to the like, you type it in and you immediately get that back that you're wrong. And then you type it in again. And you know, like, if we can make that cycle so much faster than yet you can be, you know, not even like three or five or you can be 10 or 100 times more efficient. At the end of the day. If If you can iterate more quickly. Try more

Sanyam Bhutani  36:00  
things. So you're not limited by your GPU power, per se, but essentially the library sort of holding you back right now.

Even Oldridge  36:08  
Yeah, I mean, the The amazing thing is, you know, you, you, so you, you take this code that was sort of designed to do the pre processing that you want to do in pandas data, data cleaned and ready for the model, and you know, a lot about categorical optimizations and things like that. And, you know, this is stuff that you have to do to get your data ready anyway. And most data scientists, they're probably spending more time doing that than they are doing model building at the end of the day, right? Because it's just like, it takes a lot of time to clean data. And if we can make that more efficient, we already have the GPU, you're using it for training your deep learning model anyway. Yeah. Why aren't you using it for this? You know, there's a lot of brilliant people and a lot of hard work has gone into building it up to the point where it's at. I'm able to step in at this stage and leverage and it's like, even in the The Nine months or 10 months, I've been at Nvidia, it's been a transformation in terms of what it can do. And it's pretty amazing to see now, you know, what you can do with with the library, to the point where so the role that I came on to do and part of the, you know, like my, my team initially was the rapids plus deep learning team. Okay. And so our role was to basically look at interoperability between the different deep learning frameworks. And so we started with PyTorch passing is built in pytorch. It's, you know, pytorch is one of my favorite frameworks, not to play favorites, but, but I love it. I enjoy working. And so we looked there. And there, you know, there was a bunch of interoperability frameworks like this DLPack, you know, and so we looked at how to basically pass data back and forth between the rapids library and these others using DLPack. So we're working on that for TensorFlow now, trying to get them to adopted official There's actually a package that will do it already kind of outside the bounds of one of the co authors of my RFC development. But, you know, in order to be able to do this, internally, it would be, you know, it'd be a lot easier and more elegant, integrated TensorFlow. And the reason why we want to do this is you can imagine, like, once you have this data in the GPU, and you're doing data frame processing on it, you're kind of manipulating the data and getting it ready. It's currently sitting on the GPU, right?

Sanyam Bhutani  38:34  
Yeah. And then you don't want to send it back to the CPU and back to the GPU again,

Even Oldridge  38:38  
Exactly! It doesn't make sense! And that's like, there's a big, there's a lot of work that's kind of been built around that, you know, cuml and a bunch of other libraries kind of work along the same lines is, like, you got you spent a lot of time getting that into the GPU. Now, like now that it's there, why don't you I keep it on the GPU as much as possible. Yeah. Make sense? It allows you to do some pretty cool things nowadays, like you can literally you can take data bitstream from pytorch, pull it out, do some data frame operations on your batch, push it back in and, and continue on and the translator, their training is almost instantaneous. Yeah, you can, you can do it anywhere. It's okay. It's amazing. [Interesting.] And, and so let you do some some things that you can't quite do a pytorch. I mean, generally, you can do them anyway. So we don't tend to use it that often in that context. But, but yeah, interoperability side of it is, you know, is a big part of what I do, or what I what I did in the past. Yeah,

Sanyam Bhutani  39:39  
what NumPy is to PyTorch will be what pandas will be to rapidsai.

Even Oldridge  39:47  
Yeah and there's, you know, there's there's a bunch of different kind of frameworks that are interoperable with and fast.ai is one of the frameworks actually worked with Sylvain to kind of expand You know, get the, basically make sure that that the interoperability and all the functionality was there. And he was, you know, he's aware of it, he did all the heavy lifting really to make this thing work in the context of fast.ai. So if you check up the fast 2.0, there's actually a rapid version for the tabular or pre processing. Okay. And part of what I did this is like, this is kind of a tangent, but I think a really interesting one in my mind. You know, being at Nvidia, let me step back and look at projects that I wouldn't be able to look at in other contexts. And what like, this is where the research part kind of begins to come in, in my mind is, you know, when I was working at realtor plenty of fish or human in any role, where you're working for a company, there's like this, this sort of bottom line where you're trying to like you're, you're chasing a problem, and you solve that problem, and you're done. You move on to the next one. And if there's a blocker like you don't, you don't, you're not thinking about the big picture, solving it for every company in the world. And, you know, in the context of rapids and in context with deep learning, that's really what I was, you know, tasks to do. And so part of what I looked at when I started doing that was the PyTorch, Data Loader, if you look at it was built, like, way back when, and people were doing images that was like that was what the data loaders be used for. So the mechanism of the Data Loader that traditional labelers to grab data, but you know, do some pre processing on it, load into a batch, grab data, do some pre processing on it loaded into batch. But if that batch is just a single line of tabular data, it's not like doesn't make sufficient to do that. It's not doesn't make any sense, right. And so, you know, we've we've rewritten the pytorch Data Loader we're working on, you know, packaging that up and making it nice and clean. We do pre processing of the data to get it ready for, for you deep learning models like calculating the mean and the standard deviation and doing all your normalization of your continuous variables. calculating your categorical and code And doing all of that, and just just all the standard pre processing for deep learning, we've kind of packaged all that up. And we're building a pre processing framework for tabular data. Another really cool thing that you can do with rapids is like they've so they've accelerated, like the the loading of things like parquet file, so you can take a parquet file, and do like a byte, like it's a parquet is is compressed by encoded data. And so you can load that compressed byte encoded data directly onto the GPU, and just press it on the GPU. And when it comes to tabular and comes to these types of problems, like the, the transfer of data is actually a significant portion of the problem, right, like when you're talking about an image is more compute than ETL are then then data transfer. But when you're talking about tabular data, generally speaking, the models are pretty small. It's like simple feed forward models and embedding lookups. So you have to go massively parallel with that. sizes which we, you know, we've starting to play around with, but you also have to have a fast way of getting the data there and actually compress parquet is making a big difference. So it's not the standard, we're also storing the data of old. It's not now like most most data sets, you find her in CSV format, I really think that's gonna change because it's like, the difference between loading CSV and loading parquet is anywhere between 5 and 20 times.

Sanyam Bhutani  43:25  
Well, okay. So zooming out a bit on the rapids project. Could you tell us more about the rapids project? Because there are many libraries inside of the organization on GitHub, so to speak, that are being developed?

Even Oldridge  43:42  
Yeah. And it's, I mean, it's it's really growing thing. And so there's, you know, the new ones that I know about the big projects we're working on right now, there's this really interesting one called cu signal, which is, you know, signal processing on GPU or cugraph, which is this graph library. There's a The cuml is like cuml is a whole series of, of, of, you know, algorithms and optimizations as well. And then there's cudf is kind of the DataFrame, the bulk, you know, the core library in my mind. Because generally, the data formats that are feeding these other models are kind of coming in that format. And so it's this interoperability that we talked about that you know, that you're allowed, you could do the data manipulation needed to do and then sort of take the data into those formats. There's work going on right now. And I'm not the expert in person talk to about this, but they're working on new CX, which is like our communication protocol for fast communication between devices. So the idea here is like some of the coolest things I've seen that that rapids has done, and these are these are kind of older achievements that they were, you know, back when I started, they could do this but like the GPU version of XGBoost is something that the team spent a lot of time I'm working on and there's this Fannie Mae data set that several terabytes of data, like it's, it's, it's massive, it's this huge, huge, huge data set. And they basically they develop the system where, you know, you you do XGBoost in parallel where you split the data. So it's data level parallelism, you split the data across, and each sort of worker generates trees that could come back. And and so you know, your, your parallelizing the whole thing, each tree is getting kind of different data. So it's, it's this the GPU hist version, but it scales pretty much linearly to the point where you can do this, like multi terabyte data set on, you know, on a large cluster of GPUs in I think it was 45 seconds or I think It's under a minute. And it's like to run it on a single CPU takes, like weeks/months. Yeah, it's crazy how fast you can make this thing you know, when you have the hardware behind it. And so You know, there's some amazing work being done there. And the team is really like iterating fast and developing new things every day to the point, right, you know, I have trouble keeping up with

Sanyam Bhutani  46:12  
literally cool. What I'm able to understand is these mini, so to speak, modules can function independently are it does make sense to keep them independent, but you can also pretty conveniently connect them depending on the problem. [Exactly, exactly. And it's, you know, especially when you when it makes sense to pass the data, you've got this kind of, you know, sequence of models. cudf is, like I said, it's kind of the glue that that things pass back and forth between generally speaking, but you can certainly pass data between between the different models very efficiently and easily] got it. What's your take on deep learning in Tabular? Because that's not the go to situation for now. And there's a question from LinkedIn as well. There's a heavy feature engineering that goes into it. Do you see that changing over time, your thoughts on that?

Even Oldridge  47:01  
So I mean, I'm gonna say, Jean-Francois from from a couple weeks ago on the podcast. And, like really many of the things that you want to do with feature engineering, you just can't do automatically with models. That's hard. Yeah. Although I think you can probably automate a lot of those feature engineering methods. And I think that, you know, setting those up as pre precursor methods is interesting. I know like right now, right now, it's a bit of a wash. Like I think, you know, deep learning can almost compete that some of these other methods I don't think it's it's fully competitive, you're talking like at the highest levels of kaggle. Together, there's like generally, having a deep learning to math model to your ensemble is going to boost your performance because it's learning something slightly different, but it's not going to be your best performing single model on a tabular competition.

Even Oldridge  47:58  
I I think part of the is like Jean Francois said yes - Is it JFPuget?

Sanyam Bhutani  48:05  
yes. Okay. The Kaggle GM, CPMP be yes. Many people would know him CPMP (Kaggle GM) 

Even Oldridge  48:10  
Yeah. Okay. Thank you. Um, so

Even Oldridge  48:15  
the scale of the data matters, right? And so one of the things we're working on right now, for tabular, and for recommender systems in general, which is, you know, it's my background and one of my passions is a larger than CPU memory Data Loader. So basically building a Data Loader that kind of scale the data out to these terabyte level data sets, to efficiently easily load them, because it's always an issue when you're trying to like get the data onto the GPU. If if you're expand beyond CPU memory, like, you can't, you can't just load the data frame in and then transfer over the batches. You've got a you've got to think about this intelligently. So we're building this larger than CPU memory Data Loader. And we'll be releasing that sometime in the next little while to help scale the scale. To the level of data that, you know, that maybe would help for tabular. The other thing that I think like I would say, honestly, I don't feel like tabular has seen nearly the level of effort the other modalities has seen, like if you if you look at the amount of work that's gone into, you know, images or NLP, I don't think we're, you know, we're even at one 100th of that, right. So, I'm excited over the next few years with my team to start really digging into the research side of us and looking at this, there is some interesting work that's gone on like, auto end and TabNet. And there's a few recent papers that kind of they've, they've leveraged attention models and you know, like autoent , is an interesting one where they combine like first order, second order, third order, combination of the input variables in order to generate through attention to generate, you know, these more sophisticated input features and those types of architectures really appeal to me. And I think things like that and things like, you know, automating techniques for feature engineering like, you know, Chris Deotte Kaggle. Yeah. The his recent solution to the the IEEE?

Sanyam Bhutani  50:19  
I think the yeah I do believe would be the Tabular one

Even Oldridge  50:23  
yeah, so his recent solution there where you know where he's doing this like this grouping of multiple categorical variables to find the, the Find the card holders and then kind of generating continuous features off the categoricals that are related with that. Something like that, you know, we can do that automatically through pre processing. But I'm also interested in maybe looking at like, what do architectures look like that kind of thinking that way that that allow the model to discover data in that way. It's a little bit hard when you think about it in that context, because really Like, we used to do this a realtor when we're looking at housing, you know, we talked about features in the context of like, houses the context of where they fit in the distribution, right? Like, if you talk about, you know, all of the different relative features of the house, and then, you know, for any given feature, where does that sort of where did the other continuous features like in particular price and square footage and things like that, where, like, where does it sit within that distribution of all the available? You know, generally speaking, within a market in that case, yes. But, you know, that kind of representation is an interesting on something that, you know, you should be able to automatically generate, and those types of features and the differences between you know, where that individual entry sits relative, it's like, it's not something that's easy for the neural net to necessarily learn autoencoders help with that, like, you know, if you've got the denoising autoencoder, you're changing your data, it's randomizing it has to learn the relationship between so it learns like, you know, two bedroom versus three bedroom and what that means for the other features and it can learn to recognize like, wait a minute, this is a really funny two bedroom. Because it's, you know, got these other extra features. Yeah, the general generally speaking, it's going to ignore those funny extra features, connections and not by taking advantage of them. So, yeah, it's it's interesting. I'm I don't think it's there yet but like I said, I don't think the research has been done. And I'm really excited to see you know, and to talk to people who are doing that kind of research. So please, by all means, if you are doing research into tabular and you've got, you know, interesting models or explorations, you can reach out. I think this you know, we talked earlier about the the last functions when I talked about that project, and I think there's some question about, you know, which data sets in which last functions and I think recommender system shares this issue as well, where it's, you know, it's hard to evaluate one model to another because they're comparing across different data sets are using different loss functions. Recsys has better loss functions like in DCG mean average precision and that kind of thing. But-Yeah, I think there's a lot of there's a lot of potential and a lot of future. I don't think we're I don't think we're there yet. But I agree it's not. It's certainly not at the level of LGBM quite yet. It's pretty close if you turn it properly.

Sanyam Bhutani  53:16  
How do you see the dots connecting, for example, once we are able to figure out how to turn everything on the GPU, we need to do the influence on the CPUs. So there might be a disconnect there, or how do you see that happening? So

Even Oldridge  53:29  
I'm happy you brought that up, because I think there's right now there is a lot of influence happening on the CPU. I think most inference is probably, especially for recommender systems. And NVIDIA has been working the last little while to really develop inference based architectures. And so we're starting

Sanyam Bhutani  53:47  
DALI would be another example that I could think of.

Even Oldridge  53:50  
So DALI is like DALI is a is a mechanism for doing like JPEG compression on the GPU. And that helps like Fast data loading for images. There are like there's literally hardware architectures like T4 is the sort of the inference serving card. Okay? I hope that's the external name for it. That's the one that I know the name, I don't buy it. But, you know, we're, we're looking at it, like we're seeing customers start to shift over to GPU and very, like, very few customers are. But that's a big part of what we're pushing towards, and really trying to kind of show and demonstrate, and the ones that are seeing, you know, improvements in their costs, like their costs are going down significantly, because the GPUs were able to leverage that, you know, the parallelism, like one of the things we can do on these T4s is through the tensor RT inference server, which we built. It's kind of like the equivalent of TensorFlow serving. So it's this, you know, this fully functioning RPC based inference server, and it takes the date in it solves it returns you can use defined pre processing functions, you can find post processing options and all the other stuff is really powerful, really beautiful, elegant solution. And what I'm working on right now, or what we're working on in the next little while is going to be to integrate rapids into that. So we can do some of the rapid stuff in testing in between for pre processing on GPU, right. But they're like, the hardware solutions are, you know, are faster, and they're cheaper for the customers that are going there. So I think, right now, the prevailing and I talked to people all the time, we're like, you know, influence happens on CPU. That's like, that's just where it happens. I think that's going to change over the next, you know, two to five years. And I'm hoping to be a big part of that. Like, I think part of it is the tooling and I want to make it you know, easy to do tabular and easy to do recommender systems on GPU. That's a big part of part of what my team's working on right now.

Sanyam Bhutani  55:55  
I think it again speaks to being on the cutting edge of research. Where you talking about something that Many of the listeners would think, Oh, this doesn't make sense. And eventually the trend might catch up. So a common question, again, is how does a research pipeline look like? So you explaining all of these amazing problems that now make sense in this conversation? But how do you come up with these ideas to maybe explore an idea that may or may not work? Or how do you come up with the go to steps for these things?

Even Oldridge  56:24  
Yeah, so some of it has been, like, as an example, the, the rewriting of the pytorch's data loader is one method where essentially what I ran into was this wall of a challenge. And sometimes this is an interesting thing that I like to sometimes do with it that you sometimes encounter naturally, like there's this constraint that just forces you down a particular path. So the pytorch Data Loader, you know, what I was trying to do initially was build this Data Loader that was efficient using like, memory that's on the GPU. So we've got this thing we got a lot of memory on GPU, we may as well you know, if we can leverage that put the data on the GPU, you know, but what I found was, if the data is on the GPU already, and we're doing these rapids calls, like those rapids calls are happening using CUDA and CUDA and forking to do multiprocessing, don't play nice because like when you when you call CUDA, you're sort of generating this sort of this, like this space. Or this instance, I'm describing it terribly, but you're like, you're basically creating this, this sort of reference that lets the card know that you own this sort of this particular part of memory and not to reference it, you know, know, your, your permission, right? You can't fork that out and give that sort of that key to everybody because then you get these collisions on the data spaces. And so I was running into this with rapids were like, the multiprocessing CPU version was faster because Then having the memory directly on the GPU already, because I couldn't use multi process. And that made me step back and go, Okay, well, if I can't do multi processing, I really need to figure out a different solution. And that led me to like to dig into the Data Loader. And that's not something that I would have done probably before fast AI or before, you know, but really like to step back and say, What does the Data Loader do? What like, what exactly is the code doing? I looked in and this, like those a lot of multiprocessing code, and it was very complex. But then I got down to the heart of the like, if you're not multi processing, this thing's an iterator. It's like, I can build an iterator, I can build an iterator that grabs all the data at once, because that's more efficient. And let's see how fast that is. And I built it and it was, you know, 10 to 15% faster than the base Data Loader. And in the context of fast AI, where they're doing a bunch of other things. It was 100 times faster, or like maybe 200 times faster. Okay, you know, and so we took like in the recsys 15 x model where we were or 15 x project. We, we took the Data Loader from 56% of the time to 2% of the time, well, by by like restructuring it by doing the tensor transformations all up front by loading the data front by doing the shuffle kind of once per epoch, and by grabbing contiguous blocks. And so it's just like if I hadn't hit that wall and then yeah, I don't think I necessarily would have thought to dig in there. Right? I think it

Sanyam Bhutani  59:31  
Always goes back to that question of Jeremy weird, like smart researchers, would I ask why is this happening and not just okay, this is how it is.

Even Oldridge  59:39  
Yeah. Yeah. And sometimes you have to hit that wall to ask well, why but, but yeah, no, that's, that's exactly it. And then the other is the other side of things that sometimes comes up is just you hit this critical mass where, like with that Data Loader, and and sort of a bunch of research that's gone into large batch training and a few kind of tweaks and solutions here in the Suddenly we're, you know, 10 or 15 x faster on the GPU. And that makes us go back and look and revisit that question like, Oh, is it faster to train on CPU? Does it like does it make sense? Yeah, no, no, it actually it doesn't. We were just doing it wrong originally. And, you know, and dealing with all these limitations, and once we fixed those limitations, and kind of, you know, push past that hump, and I'm hoping that's what's going to happen with the tabular deep learning, and we're going to be able to come up with some interesting solutions there. But, yeah, it's like, I think, you know, severe restrictions sometimes allow, allow you to see past or like to force you to push past things and then, you know, seeing a bunch of things kind of fall into place, it's really kind of happen quite naturally. You know, once once you're past like once you made a few improvements, just stepping back and looking at the system and seeing like, okay, these things suddenly changed. And sometimes that's coming from external sources. Like it's, it's interesting to step back and see like, okay, now that we've got, you know, like RAdam, and Mish and cyclical learning rates and all these other things, kind of wonder what that means in the field of, you know, XYZ and yeah, that can sometimes like I have these interconnections sometimes, you know, I try and keep a notebook and write them down. And I don't always get back to the ones that don't want to work on but, you know, I've got these like, hey, wouldn't it be cool if we took this idea from this paper? And, you know, applied it in this context or this setting that should you know, yeah, so. Okay.

Sanyam Bhutani  1:01:42  
Again, coming to your recent same research, titled recommender system training 15 x with rapidsai- we will have it linked in the description was I was going through the paper and was done by a distributed team. So could you maybe tell us how does a task look like that? How do you distribute a work track your ideas and synch with the team?

Even Oldridge  1:02:02  
Yeah. So I mean rapids the whole team is distributed. I've got, you know, team members all over the world, the particular core, the rapids DL Team. Some of them were born kind of from the rapids core team. And, you know, I've got people as far as way as Copenhagen, who actually rexis happened to be there this year, so I got to go visit and have a dinner with him and that was nice to meet in person, but most of my team members have never met. Okay, and so it's it's remote work and I I particularly love it like I'm, I'm able to go down and have coffee and kind of interact with my boys and you know, and help my wife up the door when she's dealing with a four year old who's having a tantrum and, you know, it's like, it helps out right and, and, and I, I like it and sometimes it means working late at night to make up for the time last and that side of things, but it suits my lifestyle, in terms of interacting with a team abroad and kind of keeping track. I think, you know, If you look at the different things that were happening there, like some of them were split up, like the all of the work on the kernel was done by Matt, the guy in Copenhagen, who's brilliant, and he's done a bunch of really interesting work for us. And, and it's just, it comes from, like, the team and the people at Nvidia are just brilliant. And everyone is, like, I, I've never worked with such a talented team. And I'm really, really, I feel so blessed every day to be able to work with them, because I can give a very high level idea or high level concept, you know, and, and, and little more than that, and with a few kind of back and forth iterations, what comes back is better than I ever imagined. Right? And so it's like, when you're working with people of that caliber, it makes it very easy to, you know, to hand off something that's, you know, that's the seed of an idea. And, and then hopefully, when they come back, it's like, they either come back with something better than you imagined or they come back with some really interesting questions that help guide and direct and Moving forward. And that's been sort of how the team's worked is, it's, I'm definitely not the right person to talk to about how to how to work, like a standard software project or a team where, you know, everybody's kind of interweaving their work I'm, I come from a data science background. I'm not a software guy per se, I'm definitely not the best programmer on the team. In fact, I probably put myself in the stores that this just because of how talented people are, and because I'm a little bit rusty, I've been doing a lot more you know, management and visioning and kind of setting up the team and, and the research and kind of keeping up to date on what's going on in the research community. So

Sanyam Bhutani  1:04:40  
Got it what best advice do you have for remote work people who are currently getting started or have been doing that any things that you picked up that have increased your productivity?

Even Oldridge  1:04:52  
So Good question.

Even Oldridge  1:04:56  
Yeah, I I honestly need to do some research on it. Like I For me, it's like I said, there's, there's times when I'm, you know, downstairs helping my wife out the door and I eat lunch with my kids every day. And, you know, that's, it's, that's my priority and it means that some evenings I'm working and not even not watching Netflix or whatever. So that's, that's fine by me though it's not like I, I don't, I don't mind it or regret it in any way I love. I love the work and I love my family. And that's sort of what I do right now. And that's, that's the sacrifice I made to have a family. You know, most parents will tell you it's a sacrifice and it's worth it. Right. It's when the boys get a little older and hoping to be able to, you know, do a little more and be able to participate. I try and, and this is something I've been my wife bugs me about all the time is you know, finding hobbies that kept me away from the computer screen right? Deep Learning is such a passion for me, it's my God-You know, I want to sit down and code something up at the end of the day, potentially but it's It's also it's important to step back and kind of take some time away from your computer and go for a walk or, you know, try and get out there and do some painting or biking or whatever else. Yeah.

Sanyam Bhutani  1:06:13  
That's great advice. Now, it's a week out a bit on Nvidia, so to speak. And NVIDIA Research has been working on many amazing projects, any things that you are excited about outside of the rapid steam, any upcoming projects that you could maybe give us a little hint about?

Even Oldridge  1:06:29  
I can't really tease on anything that I know about.

Even Oldridge  1:06:34  
It's like

Even Oldridge  1:06:36  
in videos 15,000 people, [yeah].

Even Oldridge  1:06:40  
There's a lot of really amazing projects. And honestly, I most of them I find out about at the same time you would write and it was, you know, it's happening at Nvidia, but I'm pretty focused on tabular and that's one of the you know, like one of the pieces of advice In terms of staying on top of the field, for me that that really resonated and that I followed is, you know, trying to stick to a domain that I can kind of keep up with, right and so for me at the domain epic, just like tabular and recommender systems, and you know, the work that's happening and those in the context of those two things I can try and keep up with and drive and move forward. And, you know, and I certainly, like, I'll check out how Linux can work and I'll, you know, be amazed by the new, you know, the new stuff that Nvidia is doing in that domain. And I've always liked the style can stuff that came out was, like, brilliant and blew me away. But, you know, it was my first time seeing it. So, yeah, it's I'm not as aware of everything going on. I'm hoping to get a connection like I've only been at the company for about 10 months. I'm going to Neurips in a week and there's a bunch of people from company going from, you know, more connected on the research side, so I'm hoping to get a chance to connect with them regularly. One of the disadvantages of working from home and not in the head office is I don't bump into these get a chance to talk about my research or learn about theirs. Right. So, yeah.

Sanyam Bhutani  1:08:12  
Now common prevailing sentiment, if I may, is that deep learning or even deep learning research per se involves a lot of computing resources, a lot of Nvidia GPUs what's your take on that any fast ai fellow that wants to experiment on just a GCP instances or for example on a single GPU? Any advice?

Even Oldridge  1:08:31  
yeah, I think I mean, compute resources certainly help and if you want to scale things to you know, to production level, you definitely need you need to think about resources you're running on but in the for the most part, I mean, when I was first learning deep learning, I got a I had a single 1080 ti which was a splurge and you know, and and a really powerful card for me at the time I'm now totally spoiled and then I do most on V100s with 32 gigs of RAM. And it's like, it's a world of difference to have that much memory and to be able to, you know, and to have the NVLink connections between the cards and everything else. Data links, and it's like, fast hardware is a pleasure to work on. And it's really amazing what you can do. And at the end of the day, you know, it's price that you're, if you're able to make use of them, you're you're saving yourself money by using the high end cards, generally speaking, for a hobby, the other cards are totally fine, and there's nothing wrong with a 1070. The only challenge you're going to run into is fitting models in there. I think one of the things that I try and do And part of what I've worked on is so like the the Data Loader that I'm working on the larger than CPU memory Data Loader, right, and it also has an in GPU memory and CPU memory component to it. And part of why I've worked on those three components and not just built the in GPU memory which is the fastest like it's definitely The fastest just upload all your data into GPU memory and operate right out of it right out of GPU memory. But I recognize and you know, like coming from an NVIDIA background, and not everybody has access to a GPU with 32 gigs of RAM. And if that's like, if that's what I target in terms of, you know, who is going to be able to use my work, then I don't think like, many people are gonna be able to use it. But beyond that, I'm a big believer that like if somebody can use it at, you know, in a 11 gig card, and they can really get some value from it, and then they can show that to their boss, and their boss sees the value in it that may lead to the sale of a 32 gig card, because that's how it's going to run efficiently and cost effectively in their business. So I don't think it makes sense to target a particular architecture off the bat. I think we want to be supporting, you know, people right the way through the spectrum. And I'm always like, we literally in some cases, like for our pre processing pipeline that we developed, we had a problem A pipeline that worked when everything fit in GPU memory, we developed that network, right. And for data sets of viton, GPU memory, it was brilliant at it super fast, like, you know, in seconds. And then we step back and said, Okay, what happens if you only have this much memory? What else if you only have this much, and then we've got it down to this point where we're able to iterate over the data in smaller chunks of memory, so that anybody with any size card and we literally were like taking these 32 gig cards and capping them at like, you know, 11 gigs to be able to iterate through the data and make sure that it fits. Because it's important. Everybody can make use of this. It's not going to be useful to the you know, to the community at large. I don't think it's worth working on. Yeah, yeah.

Sanyam Bhutani  1:11:41  
Now, given the exclusive rate at which all of the research comes out, I know you mentioned this on your blog post as well. How do you keep up with the research and any things that you picked up that help you read to the keeping up the deficit?

Even Oldridge  1:11:57  
Twitter's big. I mean, I think, you know, I think we've probably got to know each other more through Twitter than than anything else, because we're constantly posting back and forth. And that's a great way to keep in touch with people and have a bunch of conversations. And, you know, I try and amplify things that I find interesting. And if you follow me, you'll probably find most of this is fast.ai related because that's who I follow. But

Sanyam Bhutani  1:12:18  
that's true to anyone from the fast.ai community that's one thing true to all of the Twitter profiles on fast.ai. Yeah,

Even Oldridge  1:12:25  
yeah, it's a I mean, it's a family, you want to support your family members. It's amazing to see the work they're doing, which is cool. I have a lot of people in the recsys side that I follow, and a lot of just sort of research in general. I read a lot of papers and like

Even Oldridge  1:12:43  
to give you an idea.

Even Oldridge  1:12:46  
I have this kind of

Even Oldridge  1:12:50  
that's, that's on top of this book, that I read last year.

Even Oldridge  1:12:58  
And you know, and there's like

Even Oldridge  1:13:01  
Three sitting in the printer that I print well day and

Sanyam Bhutani  1:13:06  
For the audio listeners, there's a video version where these are visible.

Even Oldridge  1:13:10  
I guess. So I the trick for reading papers I find and I'm actually out of I don't always do this and I'm right now kind of indulging myself and reading a science fiction book, good evenings but I've been in the habit of reading before I go to bed for a long time and trying to you know, cut down screentime before before I go to sleep, it really helps me sleep. So, you know, for a long time I've been in a pattern of reading papers at night. And I find that makes a huge difference if you can get through one paper and I you know, I think I probably read 200 papers this past year, just through that pattern. That's good. Yeah, and it's, you know, if you don't like some of the papers, I get through the abstract of the pages and then I'm like, you know what, this is, you know, too complex for me. This is not My interest, this is not what I thought it was. And I, you know, I set it aside and some of them I dig through and I, I spend, you know, a couple nights on them. And, and often if it's interesting that night, I'll go back in the morning and I'll start really digging in and doing some research and trying to figure out if I can find the source code and take a look through and understand a bit better what's going on. And that's where, like, there's so much going on. And in particular, in recommender systems and tabular, like it's lagging so far behind NLP, where the architectures are like the concepts are kind of similar if you think about them in a particular way. And the architectures are so far ahead in the NLP. keeping up to date there is is pretty interesting. Yeah, so it's, it's, that's, that's the biggest thing that I've done. And then then the mp3 side of things. I have a lot of YouTube channels that I follow that are really brilliant, like the PyData channel, Vector Institute's pretty good now that they're rare. But Yeah, I mean, if you only you just sort of start searching and finding them and I have a sort of a YouTube account that's kind of centered around that. I don't get to it as often as I want, but I try and you know, I spend enough time in front of the computer watching things that it's hard to sit at the end of the day, I really honestly, I wish it was a easy way to export from the, you know, from YouTube to the audio, because

Even Oldridge  1:15:29  
it'd be a lot easier if I could do that and just listen to it as a podcast.

Sanyam Bhutani  1:15:32  
But maybe that's that's something that I'm actually working on for the "Chai Time Data Science" podcast. So some research ideas will come up on the podcast that will be covered, but maybe Now, I know that I have full time job. Yeah.

Even Oldridge  1:15:48  
Congratulations, by the way, so awesome. All different places that people fast.ai learners have gone it's pretty amazing. People pretty much everywhere now.

Sanyam Bhutani  1:15:58  
Thank you. So like, I think it It's one shared sentiment that all of us somehow linked this too fast ai because fast.ai has set up this chain of reaction where we found our passions kaggle blogging, pretty much everything for me.

Even Oldridge  1:16:12  
So I absolutely would not be sitting where I am if I hadn't sort of randomly seen Jeremy's video and gotten really deeply involved in it, yeah.

Sanyam Bhutani  1:16:23  
Now, coming to the job postings, so example someone who's currently doing fast.ai or who's just abbreviate fast day, do you think or traditional machine learning background is absolutely necessary or any other advices on how to go about applying for jobs after you complete, fastai?

Even Oldridge  1:16:40  
Yeah, it's not necessary, but you have to be cognizant that you're going to be competing with people who have that background. You know, so, when somebody sees somebody outside of the fast.ai communities is looking they're they're gonna have a harder time evaluating and I think the key in my mind if you really want a career in him is to find one of those passion projects, you know, like Jason's done or, you know, the deep learning stuff, like, come up with something that's really interesting. Spend some time really developing a solid portfolio and and like Jeremy says pick one project focus on that one project, make that one project awesome. Yet before you switch to another one because like a portfolio of a bunch of like, and Okay, you could write a classifier, you can do regression, you can do a basic NLP thing. It's like, it doesn't show anything other than that you can kind of like, follow a blog post along and do, you know, work your way through that but like taking a project and moving it beyond what the typical person has done with it, and really like diving deeply into it and showing, you know, something that people have never seen before, if you can really do that, you know, and if you codes available on GitHub, and you can show that you know, that you're solid coder that you've got, you know, You really understand these. And then you know, and then I certainly don't require, you know, masters or PhD, I'm looking for that. And that's a strong indicator because it's people who've done a Masters and PhD have shown they can dedicate themselves to a project for a chunk of time. They know, papers that know how to, and like, honestly, that first one is probably the most important thing is like not everybody is suited to like really focusing on one thing for a chunk of time. If you want to do research or machine learning in that context, you kind of need that. Honestly, I think it's this way in any field, like my wife is professional dancer, and for her, like the the tenacity of just continually being there and sticking it out and going through like the vast majority of people who started in her core are not professional dancers. It's a bunch that are but it's not everybody who sticks it out and it's, I think in the arts, it's true, I think in most jobs, etc. You know, tenacity really like pushing yourself to stick with it is the most important thing. And that's where passion really comes in handy.

Sanyam Bhutani  1:19:08  
I think even speaking for you, so you picked up a problem that wasn't really intuitive, right? So like, not everyone was doing deep learning on Tabular, it still doesn't completely make sense. And now you're at Nvidia Thanks-Thanks to finding that autoencoder problem on kaggle. I think that's what set of the chain reaction for you.

Even Oldridge  1:19:28  
Yeah, no, absolutely.

Sanyam Bhutani  1:19:31  
I want to touch up on another topic, your take on ethics, because you're also working on tabular data and deep learning. So any things that you're cautious about, on the ethics side, how do you reduce the bias or any advice for the people who might be looking at these problems?

Even Oldridge  1:19:50  
Yeah, it's I'm definitely not the expert here. It's something that I'm very cognizant of, and and you know, and I think it's a very important topic. I'm excited to see the direction You know that the things go at the University of San Francisco data Institute? Now that Rachel's Yeah, I'm there. I feel like I have a lot to learn in the area. I saw, you know, previous to actually just a few days ago, I would have said, you know, there are techniques out there for ml interpretability. And you know, and you really have to be cognizant when you analyze your models make sure that they're not kind of leveraging things that can be you know, can be bias inducing and that's the solution isn't as simple as just like taking those pieces out of the you know, like you can't just take a data set that has male and female out as an example and you know, just remove that class and expect that the model is going to, you know, going to work a good indicator and actually this was a This came to be for my Kaggle forum as well. Not particularly in this context, but I think Bojan often does these on the Kaggle forums is Basically, you know, if you can, so he doesn't in the context of predicting train and test set. So if you take a data set, if you can predict train and test, like the variables that are good at predicting between your training your tests that are like, basically leekie variables, like they're, they're not going to help, you're going to run into issues with, with leakage. And similarly, you know, if if you want to remove bias and you take like that, that variable that you think is going to be, you know, bias in your set, and you apply it in this context, you know, you if you if you train a model to try and predict that, and you're getting anything better than 50% accuracy than there are other confounding factors in your data that are that are causing this bias, and you need to really think about it, I think it varies on the product. And you really do need to think about it from a product perspective in terms of like, what it is the models trying to do, how the model is going to be used, you know, the context of which is going to be used as everything and I think that's, it comes The application of machine learning and

Even Oldridge  1:22:04  
getting more specific, I think

Sanyam Bhutani  1:22:06  
that I can speak to being aware of that it's a problem that you need to keep in the back of your head also is a huge factor. Many people don't even consider that. But whatever domain specially in machine learning, if you're working on you should have that thought in the back of your head. And it might become relevant now later, but that's the first day folks also really propagate. But it's something important that I think everyone should be aware of.

Even Oldridge  1:22:30  
Yeah, Jeremy and Rachel have a great, I guess, a couple of this one for the machine learning course, then one from the fast.ai-Like the basic deep learning course. on ethics and artificial intelligence. There's lots of examples where it just goes horribly wrong. Yeah. in ways that you can't even imagine and, and it's, it's very sad to see and it's this like, there's a lot of people that are just sort of blind to it, and denied it. And there's people who kind of build models based off of these prejudices. And it's unfortunate that that's the case. But you know, I think as a as a engineer, ethics should be at the forefront of, you know, of our thought when we're building a system or building a

Sanyam Bhutani  1:23:18  
[building a tool, Yeah] definitely. This has been a great conversation. My final question to you would be, what basic ways do you have for someone who's just starting out in machine learning or deep learning?

Even Oldridge  1:23:30  
Oh, you know, the answer to this.

Even Oldridge  1:23:36  
fast.ai is as legendary. I mean, it's been such an amazing course I think, you know, for me, I've gone through all of the all of the coursework that's available. I've done the machine learning course as well. Take your time with it, you know, there's more each time and, you know, don't just shift enter through like Jeremy said, it's, it's important to do the programming so Spend some time learning it, you know, make find a passion project dive in. But yeah, the fast AI videos are going to get you excited about working on this, you know, definitely remotely interested. By the end of it, you're going to be super jazzed. And I think there's a lot of people in the community that you know, that have gone on to do amazing things and that's hopefully inspiring to people who you know, want to take this path as well. And yeah, reach out to them like happy, I'm happy to talk with people who are kind of on this journey on the forums or wherever. You know, it's it's it's that perseverance, there really is the key. I think if you if you dedicate yourself to it, and you spend the time and you really dive in and you focus, literally anybody can do this. And that's I think the beauty of Jeremy and Rachel's teachings is, you know, there's people from all sorts of diverse amazing backgrounds and and the background you come from, bring something to the table and honestly, like coming from a machine learning background, I felt a little bit like I was losing out on having that other background to bring to the table and I've got photography and some other things that I could bring. But, you know, it's it's having having an interest in you know, some passion some something, some jobs and like the radiologists he's working with now or that like the people who don't have traditional deep learning background, you have an expertise in a field or an expertise in some hobby or some understanding like you're going to bring interesting things into the world and that's you know, that's more important than somebody who understands machine learning you know, tweaking your machine learning model by point 01 percent Yeah.

Sanyam Bhutani  1:25:39  
I also want to mention your online handles your Twitter handle is @even_oldridge always any other platform where you're active and the listeners could follow you?

Even Oldridge  1:25:48  
Fast;ai forum, I'm @Even,  I'm not as active there. I think Twitter's the main one these days. Okay, it's kind of like keep up with so

Sanyam Bhutani  1:25:58  
Find Even at @even_Oldridge also linked in the description of the podcast. Thank you so much Even for all of the amazing advices and for joining me on the podcast.

Even Oldridge  1:26:08  
Thank you, Sanyam. This is lovely. I really enjoyed chatting. I hope we get a chance to talk again soon.

Sanyam Bhutani  1:26:12  
Likewise. Yeah,

Even Oldridge  1:26:14  
okay, take care.

Sanyam Bhutani  1:26:22  
Thank you so much for listening to this episode. If you enjoyed the show, please be sure to give it a review or feel free to shoot me a message. You can find all of the social media links in the description. If you like the show, please subscribe and tune in each week to Chai Time Data Science!

