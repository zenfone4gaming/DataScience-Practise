Sanyam Bhutani  0:13  
Hey, this is Sanyam Bhutani and you're listening to "Chai Time Data Science", a podcast for data science enthusiasts, where I interview practitioners, researchers, and Kagglers about their journey, experience, and talk all things about data science.

Sanyam Bhutani  0:46  
Hello, and welcome to another episode of the "Chai Time Data Science" show. In this interview, I am really honored to be talking to the king of Kaggle discussions and twice Grand Master CPMP, Dr. Jean Francois Puget, for the second time so I've already interviewed him on the blog post series do check that interview out in case you're interested. This interview daily picks off where the previous one had ended, so I highly suggest that you check it out. Grand Master was kind enough to talk all about his journey and machine learning and Kaggle in that interview. Dr. Jean Francois is a distinguished engineer at IBM and technical leader for IBM AI toolchain offerings. In this interview we talk specifically about the IEEE-CIS fraud detection Kaggle competition where CPMPs team titled, two uncles and three puppies finished second on the competition. We discuss Grand Master's approach to Kaggle competitions, this one in particular and even more so in general, their team's solution, as well as talk more about his Kaggle experience. So we covered a few points that were missing from the blog interview. I'd like to thank people who submitted the questions via the AMA section and of course to the Grand Master for agreeing to do it. For now, here's my interview with twice Grand Master and king of Kaggle discussions, CPMP, Dr. Jean Francois Puget, Please enjoy the show.

Sanyam Bhutani  2:33  
Hi everyone. Welcome to this episode I am on the call with one of my machine learning heroes who has been kind enough to come on the show for the second time. Dr. Jean Francois Puget, with the king of Kaggle discussions. Thank you so much for joining me again on the interview series.

CPMP  2:50  
Thank you for inviting me.

Sanyam Bhutani  2:54  
I'm honoured, before we talk all about your goal solution and second person symphony's on the IEEE competition, could you help us set the stage by giving us an overview of the competition and what was a quote unquote challenge here

CPMP  3:11  
was a fraud detection, competition, credit card fraud detection. So we are given a bunch of information about credit card transactions corrected by a company called visitas. So it's important to understand that this test is not a card issuer. It's not visa, it's not American Express, etc. So we have to they don't have access to all the information about cows. So they give us information collected from different source, and for many reasons, and the data was awful skewed. So we didn't know what the data was really about. Except Off show exception find Sanskrit come number was transmitted to the first six digits if I remember when so and so we don't have the credit card number anyway, even the first six digits. So working with anonymized data is always a problem. It's, it prevents some feature engineering based on common sense. So that's what we had. And we had to predict if for, for the visitor, we had to predict if a given transaction is a fraud or not. So that was the thing. And very quickly, we all try to identify which transaction belong to the same cow because a way to detect fraud detection, and I've worked on unreduced case, is to compare the contrast section was his story and try to find no money. So that's what we all did. Try To recognize members from the data

Sanyam Bhutani  5:03  
got it in the previous interview had mentioned that you are sort of selective with the competitions are to enter you look for challenging ones where you get something new to learn so what made you pick this competition particularly and why why did it seem interesting to you?

CPMP  5:18  
It's true that for me Kaggle is running so that's why I do kaggle competition and I always tend to pitch competitions where I'm not good at and the stopped I would say this one they said was a bit different in a phrase competition. And I've started competition of fun like to to review and discuss with people who did win. And one person did well in a press competition and we said, oh, maybe we should team next time. Okay, and Sergei brilliants He invited me to join this competition before I entered and even before I completed the previous one, I said okay, give me a few days after I would think about it. And separately I had discussed with Giba: Gilberto Titreticz. We have teamed several time in the past and we we said we would team in the next competition. And I was really tired. And I looked at this IEEE competition. And it looked a bit it took its time series time series are very difficult to me now the most difficult competitions and, you know, predicting future is hard, right. It's a show I was not sure I would enter I was tired. But between discussion with Giba and Sergei Yes, okay, I should join But I waited a long time and I joined like two days before the deadline for entering which was, which is usually unusually late for me. I'm sure it was more because I was invited this time than willing to learn Really? So, it's a bit of an exception. Okay, to my general approach, well, I did learn.

Sanyam Bhutani  7:29  
Did you find the problem challenging enough? And I think this was your 24th meddling competition was it also related to your earlier learning some other competitions?

CPMP  7:41  
Yeah, so I found this competition to be very difficult because it reminded me of this competition malware where I did where but which was a disaster for a lot of people I mean, so because of test data is the Front in many ways from the trend data.

CPMP  8:03  
So

CPMP  8:04  
the key in these competitions is to either remove differences between train and test data or build models that generalize very well. And both are difficult. But so this competition would be very similar to malware, but in fact, not so much. The data is different but it's not defined the same way it was as in malware. So I was expecting a much larger shake up than the one that happened.

CPMP  8:37  
It was challenging because

CPMP  8:40  
I think can't prove it that the cards used in trend are Disjoint from the cards used in test [Okay] so it's easy to over fit in train if you do too much, your mother on non cards. Becase, It's one transfers the lead to test. So that was my approach not to rely too much on specific count numbers. Okay? So I be if some people needs in which explain the shake up that it was a minor shakeup, but that was a shakeup

Sanyam Bhutani  9:20  
I'd also love to know the story behind the team names who your team was titled two uncles and three puppies.

CPMP  9:26  
Yeah, so first. So there was this team of three Russian including Sergey. They were called three or meet, guys, I think. So MIT is a is a high end University in Russia. M stands for Moscow, Moscow. So I don't know the details, but it looks like a top university in Russia. Three guys from that university teamed and they were in a good place. Then when you say China said, Well, I'm coming with jibba. Can Can we join this short course and they renamed the team they didn't ask. So what I can explain is uncle, I'm called anchor on Kagura forums. It started to is I believe a person from China who called me and left or give some advice. Okay. And then currently in China is a mark of respect. So it's like an old wise man, you know, I've read bergland bit so I'm older than most characters. So this it's kind of respect for someone more experienced and older. So now I'm I'm not uncarrier and Sheba was not a man named and currently this but he deserves it. She has be in top character for a long, long period, and it's very near is still near the top. But he also has mentored and helped a lot of people. So I think he did deserve this unfair as well. Then why the burpees? Maybe they felt a bit overwhelmed by us, but they should. And the three, the three army guys are very strong people. So they decided to call themselves but these lucky and I think the other countries

Sanyam Bhutani  11:39  
that's that's an interesting story. Also, you mentioned you joined the competition, I believe 10 days before the original closing deadline. How were your first go to steps for this competition maybe in comparison to

CPMP  11:53  
I always have a similar

CPMP  11:57  
workflow.

CPMP  12:00  
First, I look at data and credit. So kegger is different from real life, because data is already cleaned and curated. But still, so my first step was to look at the difference between train and test. And I filtered out features that work to the front. So I got rid of maybe half of the features right away because they had too many values in test mode repairing and train, lag, the type of device used, you know, mobile from type all new types that time tests are not in trend, so I removed them. The second one is, I always spend a lot of time on setting up a cross validation setting. Because I read I am cross validation, as I do in, in real life mission coming Too many people on Kaggle rely on the public leaderboard. It is ok if the public can privately doubled our various middle public and private tested very similar. But in time series now not. So it's not because you do well in predicting next month that you do well in predicting a six months away. And that's what we were facing. So I tried to replicate the need to predict far away In addition, so I thought a bit a lot about it. And reading the other top teams write ups, the ones who did well had a similar approach, time based cross validation and long and for some faults, very long delay between the train and validation data. So that's the second step. preservation. Then I did a feature selection and parameter tuning. I started with like GBM, which is my, almost always a default I'm starting with you on Wi Fi, at least because it's fast, is faster than extra boost. Not that much anymore as you boost has made progress, but it uses way less memory. So it's a good starting point.

CPMP  14:30  
And then I did some future engineering.

CPMP  14:34  
And then I did and so this happened quite quickly, actually. And that's when I teamed and when I teamed, I was expecting my teammates to have worked on identifying cloud transaction belonging to a SIM card and the deed. So I just borrowed the transaction change than the identify added them to my data set. But again, I didn't want to use the credit card. So the each chain is like your credit card number. I didn't choose them directly as feature because I thought it would not carry over to try to test. I rather boot leg features that represent the past. So at any time, I wanted to have features that summarize the past use of the account. That's how banks detect fraud in practice. So that's what I did. And the This brought me it was a bit of a surprise to me. But this brought me in got rich and very, like in two days. So I benefited from my team features but it was a good good surprise and then I work with them. Trying to Add diversity.

CPMP  16:03  
So not do the same championing that they did.

CPMP  16:07  
And then Giba blended the notebooks together got it

Sanyam Bhutani  16:15  
to linger on to cross validation for a little longer during a previous interview also you had mentioned that this essentially unbound you from the five submissions. So, if you could maybe also give a few general cross validation tips and specifically how did you set up

CPMP  16:35  
your you should try to credit for consolidation sitting to be as close as possible as a trend test of the kegger data. So for instance, in this case, we had two months delay between the trend data and the test data. So in my cross validation, put a one month delay between In each trend data and validation, and the trend trend data was before the validation they touch time. So that will be similar to test where there's a lag, why not two months because then it would remove too much of trending data. So it's always you cannot always replicate exactly the trend test split because you have less data but you try to approach I also try to see if my Cross Validation additions to call evolves as the public data ball. So publicly that Bob could be considered as one folder, it's okay to, to use it as one extra folder. It's not okay to only use the public, you know, Bob in magic. So I try and so to see, as you mentioned, we have a limited number of submission. So I tried to see if I can just ignore Private the publicly doubled score. Each time I submit, I look at the correlation between my cross validation score under publicly double and if I see that both in the same way with our cross validation score results in a betteer LB score up to some noise, then I'm very happy and I just basically ignore on the Lb [got it] so in here it was another case.

Sanyam Bhutani  18:31  
Okay. There's a question from the AmA section by Andres, I think you already answered it on the first but I'll just repeat it for a quick answer. When some Tabular competitions, data is purposely also created to prevent privacy leaks, etc. And the main key in such competitions becomes an obstacle getting the data which no longer seems to be a machine learning problem is that your experience with this competition

CPMP  18:59  
show I have a strong disagreement. Sure. This is a prevailing view that VISTAs have skated called ideas. And I express my view in the forum. I disagree. I don't think best as as account IDs. This test is not the county Sure.

CPMP  19:18  
Look, oh,

CPMP  19:20  
I'm sure this does the does we did they try to chain transactions and blah, blah, blah, but I'm not convinced they have full account ID. So I'm not sure they've escaped and that really just you're gonna put this I made his end of escape and maybe they wanted us to explore and see and they want to see if we can come up with better ways of detecting Carnegie's

CPMP  19:50  
all users.

CPMP  19:52  
So

CPMP  19:53  
I think it is it was a value of value for them to either ofisgate, the account IDs are just they don't have them. But the view keeper side, certainly, you know, it's not to leak the organizer from business said clearly that if credit card has fraudulent transactions, then all following transactions have marked as fraud. Yep, was clear it was said so using that property is not really to leak is when information is costing, same trended or tested. That gives you information about the target. But without the what they give in an intended information about the target, so it's a mistake. Here it was not a mistake. So it's a it's a fundamental property. To how transactions are a bonus for them. So I disagree, but

CPMP  21:08  
I may be the only one.

CPMP  21:12  
Among those who wrote about it in the forum to defend this position, everybody had says it was just a matter of finding it. And I believe it's a measurable mistake in how to approach a machine learning problem. machine learning problem, you want to use properties of the data to predict, to me it's feature engineering, and critten calc transaction chains is just feature engineering. So if you see that feature engineering, it's it's part of machine learning. The person who has a bit about it is a specialist of the planning, where and the Planning Specialist stands to say we don't do features Engineering, we let them wander around. But so the planning is great that indeed learning features. But I don't see how purulent the planning technology can identify those transaction chains. So it's it was a bit of, you know, when the only way to do well is to really do feature engineering and deep learning at disadvantage, so maybe that's why he said it's not really mission running to me its mission.

Sanyam Bhutani  22:40  
Okay. The second question from him is any ideas why neural networks were harder to fit than boosting algorithms. You've already answered that but I want to linger on to that since our previous interview which was almost a year ago. Your opinion on gbmc hasn't changed. You still say they are your go to it Boo Boo mode on that third and what are your thoughts in terms of both industrial use cases as well for your lead sources tbms

CPMP  23:08  
so on.

CPMP  23:11  
So, first GBMs are more recognized than the time in industry, especially actually boost, who was the first modern one? So, lots of companies already use actually boosts and production, like GBM is coming can boost a bit, especially in Russia, but reading so, so I'm deplaning. The planning is critics. But and this is a major haul, but the planning requires a lot of data. Yes, especially in tabular data, you know, if I don't remember how many examples we But let's say you have 10 examples. This is small for deployment. And the problem of the planning is that either you use a very simple model and it does not match. Or you use a more complex model and it over fits very rapidly know it's very brittle. So if you look at the planning success, for instance, on image problems, you mentioned that has 12 million image, then

CPMP  24:31  
making mother overfit on it is harder.

CPMP  24:37  
I think now top Models overfit, but that's a separate discussion. When you have technical samples. It's it's difficult. So you could say oh, I will use transfer learning. I take a pre trained Medan and then I would just have you learned a bit more on my specific samples. It works well on images. Problem is for who date? What would be a pre trained model? It's not clear at this point so transfer learning is not an option. So that's our answer. The the particular answer for this competition is what I just said the transaction chains. I don't see how deplaning Medan could learn them all just writing code to to find them directly is more more effective. I got it.

Sanyam Bhutani  25:32  
I'm a fan of your writeups. I'm sure everyone in the Kaggle community is one of the ones that I particularly really really liked was becoming adults that specific to this competition. And you're Of course passionate about the field and kaggle you active in the discussion, but what what's your advice on keeping a professional connection as you said in the topic with Kaggle

CPMP  25:54  
this post, I was sure it was it would please people from Asia. Your you find us from India, China, and there is a very long history of have between Buddhism and, and other religions to be a philosophy to step back a bit from life agitation and and look at West. That's what's essential, I would say, in western countries in the US in particular people are running all the time. And so you step back. And in this competition I was saying people are getting mad because of a two day extension. If you think of it at landscape, today's What is it, frankly, but I saw people getting mad and so emotional. And it reminded me of, of an episode of where I went the same way was very emotionally invested in Calgary. I was on a competition and term. And within two or three days I was at the top. And at the time I was not a grand master. So I thought maybe I can win solo. I was so excited about that. And then the reset the competition because there was a mistake in the application that I was exploiting. So all my faults were just white.

CPMP  27:30  
I got mad.

CPMP  27:31  
I read

CPMP  27:32  
the rules I read online to see how I could sue Kaggle [Okay] for breaching a contract. It went to this. I was mad. I said, I will never compete again. And what after one or two weeks, I cooled down and I say okay, I won't ever be in the same state. So now when I compete when I invest time and energy, but it's not a matter of death or life anymore. It's not it's not a love story that can become a Hate Story. It's just a professional activity hoby around the expanse, but it's not the most important thing. And so I just shared this, the experience in this blog post, in this form post. And, yeah, now it's, and sometimes even this, it's it's harder to get motivated.

CPMP  28:47  
But I sleep better.

Sanyam Bhutani  28:51  
To talk about how you invest your time, how do you balance your work life kaggle and how do you decide how much time you will Indicate by working on any competition Generally,

CPMP  29:03  
the programming is I always end up using more time than

CPMP  29:10  
So, but first,

CPMP  29:15  
I can do some tackling and my work time because it's, as I said, a learning experience. So it's a way to know about state of the art practice. And as a result, I'm working on machine learning to find them. So what I've learned on kegger is useful directly in my work, so I can do some acting, but I can't do it from time and by nimman. Or even what can work it up whatever resonates on cager on the bridge, which is good already, but they need to work. evenings and weekends on top. I don't think you can consistently get gold medals by working two hours a day on the Kaggle competition. I think we can do solver two hours a day. So maybe that's good enough for a lot of people. But when I enter competition, I really target Gold now for good or bad reason. So I need to that's why I don't enter that many competitions because I really want to do seriously. So first, I don't do more than one at a time. That's the first thing I see people entering for competitions. You may see me and during for competition is just to download the data a bit, but I focus on one. So basically, I tried to do a bit of work before I'm going to work in the morning. Especially usually prepare experiments that will run Throughout the day, I check on them from time to time at work, or relaunch so much possible. If I can, I can access. And then in the evening, depending on the experiments, I would submit, I will prepare another set of experiment to run during the night. The key is to make the machine run 24 hours a day. I cannot spend 24 hours a day but the machine can it's very little and then have a comprehensive life. And no, no, no kids at home. So

CPMP  31:43  
I can spend quite a lot of times

Sanyam Bhutani  31:47  
got it. Now coming back to the competition, I'd really love to know how do you distribute your workflow closer teams who already teamed up with Jeeva in the past, but how do you distribute your work in a team and how to Do you document or track all you experiments?

CPMP  32:03  
So that I believe that it's better to let people do what they want to do?

CPMP  32:10  
Even if I don't think it will work

CPMP  32:15  
someone motivated to do something is way more

CPMP  32:19  
than so someone working on something they don't believe if I tell or less experience teammate or you should you should do this instead of what you want to do. Usually it won't work so I tried to to whether to when I team with Giba, he is more expensive than me so of course its not a problem. But with less experience people, especially those who team because they want to learn from me quote unquote, and let them make their experiments but if if they fail, I taught them to learn from it. Why did it first I said I have doubt because of this if if the experiments fail and it proves I was right and then I want them to redo he was right not because I want to be right but because there was a reason why the experiment phase is very important that machine learning is an experimental science. It is an experimental science,

CPMP  33:26  
like physics.

CPMP  33:26  
So, I really conceived the work making hypotheses for instance oh maybe features around this would improve testing the disease using cross validation and then looking at the research and learning it's very important that you use an experimental point of view depart with preconceived ideas. A should you this whatever the problem sometime they can be lucky but You really have to be open. And if you're convinced something will work, and then data shows it does not, you should not insist. Sometimes I'm guilty of that. So, that's how I spend my time spreading work between teammates, and courage paper to say what they want to work on. So that we avoid duplicates, if possible, and also to share when they do something that did not work. And this is surprisingly difficult in many cases in my previous time, so I share when I try something that fits me, okay, this didn't work. And then someone said, Oh, I tried it or so and it didn't work. Well, if that person had shared info, it would have saved me time. Like, it's very important in coming to share experiments where there are they all successful. Don't People tend to only share what works. Know You should share what does not work because you learn from it. But so, and I say it's time but sometimes people just, I don't know, the afraid of what I could think I don't know. Try. That's why I share my failures I'm given it happens every day.

Sanyam Bhutani  35:31  
Got it. Now finally, I'd love to know more about your solution. It might be a little technical. So if you could just give a very high overview of the second position winning competition and the decision making process behind what made it to the competition versus all your experiments.

CPMP  35:49  
Alright, so I've described the cross validation setting. The key again was to identify what first job would set of features in general so trusted action was important VISTAs provided feature they'd be that were very productive. So if you look

CPMP  36:09  
at the feature engineering and

CPMP  36:14  
tuning you could reach

CPMP  36:17  
at least 0.9 or 94%. ROCAUC, you see, so it's, it's a good score. Moving from 94 to 96 and gold, you needed to work on Card ids [Right]

CPMP  36:37  
my teammates did the work.

CPMP  36:40  
The two just to identify them what they did was to use scatter plot, where you have time on one dimension and the future values on the other dimension and for some features, you worse seeing straight lines growing as time straight. So these were transaction chains. And then their work was to capture these features a lot of teams tried to use to hop code the property that if a transaction in a transaction is a one then the following ones one of so they implemented plus processing procedures directly. It gives good result but it over feeds.

CPMP  37:36  
Because I work

CPMP  37:37  
transaction chains are not perfect. So what we did instead was to create features representing what we believe each transaction chain and then either use this feature directly so my teammates who use can boost which is good that category called features directly prefer to use each to create what we call lag via birds representing the previous transaction in a transaction Shin, like the average of previous transaction, the violence.

CPMP  38:17  
Same for the time intervals.

CPMP  38:19  
The ID, my idea was to capture if you fraudster and you get a credit card number two, then you want to use it as much as possible in the smallest amount of time before the fraud is detected. Not too much because banks are so look for the total balance but the idea was to see if an acceleration in that transaction happen are playing a role same when you're fraudster and you're lazy, you will repeat transaction so The violence should decrease because you, you will always use the same amount. So that's why violence of transaction is predictive in my view. So I added this kind of features. I also used a technique called target encoding. In target encoding, you basically replace value by the average of the target for some collabing this value, but you have to do it in a careful way because it's too good of a future. It leads to overfitting. So and I explained in the competition forum, I use nested for target encoding something that people don't use. Usually, some grandmasters do, and I've seen some of some code like that in their solutions. But in the average character doesn't understand why You'll have to be very cautious about leaking information. That's why we use nested fault. So and with this I went to 96%

CPMP  40:13  
my teammates are specialists of Catboost

CPMP  40:17  
and I had never used it before and can boost is different from XGboost and a KightGBM. It truly handles categoricals quite when and in my case target encoding was helping lead GBM but it was detrimental to CatBoost. So, it means can boost has built in ways of doing target encoding that are bitter in that case, so, I will use CatBoost some more in the in the next competition. So then what many top teams had a similar approach than ours and yet we finish second so What did we do different? I believe we have more diversity between Giba models, my models and the Russian Models. I would say. The second point is we really cared about removing feature that could lead to overfitting.

CPMP  41:21  
So with the with the

CPMP  41:25  
Cross Validation setting, it was easy to see if a model was bad at long term predictions, so these were discarded,

CPMP  41:35  
Secondly

CPMP  41:38  
Sergei and his colleagues on the Russian side, they had this call that was distinct and that was making a distinction between the score on transaction that were on the same chain and on trust data on trend data and transaction that only belong to change. occurring in test this way they were checking that downloaded was not good just because it was overfitting and train.

CPMP  42:11  
So

CPMP  42:13  
overfitting is an ensuring mission and integrity competition. So we had several we took we were afraid of it all along. I believe that's why our mother channelize be better than other teams.

Sanyam Bhutani  42:28  
Got

Sanyam Bhutani  42:32  
to talk about the computing resource that your team use. Could you speak to that? This question comes because there's this prevailing sentiment that you absolutely need a huge amount of setup to.

CPMP  42:46  
I have in this I used machine

CPMP  42:51  
It's now much smaller machine than many and many But still, it's a 64 gigabyte machine 20 core Xeon 2.4 gigahertz so it's by no means top and a current machine on cloud you can get better VMs very easy. Yes, but technical means you scale energy game or XGBoost. But if you have recent GPU certainly 1080Ti or more recent, you can run XGBoost or or CatBoost as fast. So a modern GPU or a vertical machine is interesting. But regarding the computing power, we should not overestimate its good modeling is beats good hardware, and in a previous competition in talking data, I'll finish six solo on the sandwich in 64 gigabytes The top team, they used a VM and Ms one of 1.8 terabytes, though. And they use I don't know how many features I use only 48 features. So I know I mean, we should not have our estimates on tabular data on image processing. If you have a 16 recent GPUs instead of one, you do 16 times more experiments, or you're trending 16 times faster. It makes a big difference as you have say, so what I recommend if you can afford is to reset So, two GPUs with more than 10 gigabytes a 1080Ti or better.

CPMP  44:56  
Perhaps Two

CPMP  44:58  
and a good CPU

CPMP  45:03  
So

CPMP  45:05  
high and gaming PC will do the job and that's what I would recommend to the if you want to own the hardware. The other option is as many is to use Cloud resource called VMs. So there are numbers kegger and notebooks is one um it's not the only one

Sanyam Bhutani  45:28  
GCP but some credit still at the time of recording for for signing up

CPMP  45:33  
yeah and so yes if you want

CPMP  45:38  
if you're competing to for gold hardware will make a big difference. Do you want silver or you don't need to have top end hardware at this point, but you know, I don't know if it'd be sit var would be held without good hardware on him. Oh, I don't think You

Sanyam Bhutani  46:01  
got it.

Sanyam Bhutani  46:03  
Now coming to discussions we all know you're the king of Kaggle discussions I'd really love to know why you do it. Why do you answer questions? Share your ideas. Why not just your ideas to yourself keep winning competitions.

CPMP  46:17  
Yes, I believe in the past. I miss gold in few competitions because I share too much stress jealous now, to be honest, and I observed the socon most popular on forum, Chris Deotte. He is going the same way in in one of his first competition is shared so much that he missed the gold but I would say 80% of the gold medals were just building and his sharing so sharing this as well and us get involved. So it's a balance. So I tend to share now more methodology advice or hints. I don't and I am against people who share ready to use code you just even have to just fork run submit and you'll get a good score. That's had been, maybe you get mid middles but for your Kernel, but what is a help you give you know there is a saying it's better to teach someone to fish. Yep, catch fish then giving them fish. Well, so it's the same. So I'm trying to help people catch fish rather than giving them the fish so I give hints

CPMP  47:52  
and give hints and and give me some advice like,

CPMP  47:56  
work on cross validation. Don't trust probably can be All don't trust Francois' I give an advice about public notebooks never use public notebook output as is just back and look if the public notebook has the cross validation on this trend test split. If they don't then it means and they also adjust to whatever is in the notebook based on the publicly now box call, which is a best recipie to overfit. I know so

CPMP  48:34  
again, I give this advice

CPMP  48:39  
I think it was a good advice because the best I'm not was really bad on the private they don't bomb.

CPMP  48:47  
So and then some people have a specific question.

CPMP  48:50  
I don't know why I feel I have to answer I'd like I consider teaching as a profession at university.

CPMP  49:01  
But I'm not good at at.

CPMP  49:06  
So I'm good at writing educational content. But

CPMP  49:10  
teaching is speaking and not good there.

CPMP  49:16  
I was patience.

CPMP  49:18  
I don't understand. Whereas in writing for us, you can rewrite polish. And if you don't understand you don't know. I mean, unless you complain on the forum,

CPMP  49:30  
so I don't know.

CPMP  49:32  
Look, I like writing a blog.

CPMP  49:36  
I like writing on Kaggle.

Sanyam Bhutani  49:40  
Got it. Another thing that I learned from you is your as you mentioned, you also spend a lot of time after the competition in discussing things. Why not just go on a vacation with the prize money, why spend the time understanding other solutions

CPMP  49:54  
because this is where you can learn your competitors especially The top teams have share with the D. So they write their main ideas. Not always but very often. So you can learn from what they did. You cannot show us late submission to submit things. You do not have time or did not have the submission left.

Sanyam Bhutani  50:24  
Do you also do a late submissions

CPMP  50:28  
on this one?

CPMP  50:30  
So, given a join late and we're we're a five people team. I could get one or maybe two submissions. So I think all in all, have submitted maybe 15 times underneath this competition.

CPMP  50:45  
Oh, there are things I could not try.

CPMP  50:51  
I think less than 15 minutes so after I tried then some things did were indeed very interesting. Some did not So yes, I do the beat of late submission, I read all the right types on, look at and ask questions if I need to apply so many people after competition just run to the next one. And they don't try to learn. I don't know why. And the event I've seen people trying to share that solution before the end.

CPMP  51:27  
People won't read if I shafter.

CPMP  51:30  
So what

CPMP  51:33  
Yeah, got it.

Sanyam Bhutani  51:35  
So my last question to be know, in the previous interview, you gave your best advice for someone starting machine learning what would be your best advice for someone just starting on cankle looking to compete in competitions.

CPMP  51:47  
So first practice Python.

CPMP  51:51  
It's not the only language you can use. You can use our

CPMP  51:56  
but

CPMP  51:58  
it's, it's it's You're off to learn, I would say and you find a lot of machine learning packages with beta. So practice peyten a bit and then enter. Take some courses so I like Andrew Ng machine learning course or the Stanford ML course not his deep learning course, [ok]. So on don't start with deep learning unless all you care about is images or natural language processing. Start with Scikit-learn, read Scikit-learn tutorial, so that you have an overview of the algorithms

CPMP  52:46  
experiment with XGBoost and LightGBM

CPMP  52:50  
and then enter a Kaggle playground or getting started competition and Kaggle practice practice. I say people who go from course to course So they have all the badges and certifications you can imagine.

CPMP  53:04  
But

CPMP  53:06  
this course except and ruling, this way recommended, don't have much methodology content. And you only understand the methodology when you practice my first competition and Kaggle and also comment first the video.

CPMP  53:26  
I thought that will do well, I have a PhD in machine learning

CPMP  53:30  
from a long time ago, but

CPMP  53:32  
I followed some course to refresh. I said I would tell where you would see what you would see. And I lost like 2000 rounds between public and private,

CPMP  53:43  
the web. So

CPMP  53:45  
I said, Oh, I need to learn. And I really try to understand what I did wrong. What I did wrong was no cross validation.

CPMP  53:54  
So

CPMP  53:56  
I learned it's only when you try you fail, you learn You try you fail, you learn and after a while, you've learned enough so that you don't fit anymore. That's how you

Sanyam Bhutani  54:08  
got it. This has been an amazing interview before we end the call. What would be the best platforms to follow you and follow your work?

CPMP  54:17  
Kaggle, Kaggle Forums.

CPMP  54:21  
I have

CPMP  54:22  
a blog on IBM but just learn they are sunsetting the platform so I would relocate somewhere I don't know yet where. But no, okay, good forum. Good competition from writeups from top teams, to me are very, very interesting. Some notebooks are very good. Then the various blogs or first, you're your own

CPMP  54:53  
video blog, it notes interesting. You're on tour.

CPMP  54:57  
I'm not the only good Kaggler, You're entering interviewing and we will be on Kaggling so notice what source it's not because you're from India but I find analytics Vidya Is that how you pronounce it?

Sanyam Bhutani  55:12  
Yeah, right.

CPMP  55:13  
Very interesting. It's a good very good introductory material you don't need to know a lot before reading these to us data science also has interesting blogs to KDNuggets. I'm, I'm very cautious and then if you want to be more technical reading papers and archive so research papers, but for this is it's better to have ways to filter now so many publications so I'm involved slack channels like KaggleNoobs, you find. Find a community where you can share and And learn about new cool stuff. The problem in this space is there's too much information for

CPMP  56:09  
consuming all of it. So you have to be

CPMP  56:14  
to be careful. So I read a bit general purpose, as I said, and then and as we said at the start, and our competition in feeds, I don't know for that I know, between what I will experiment and what keggers would share, I wouldn't know state of the art.

CPMP  56:33  
So

CPMP  56:35  
I'm looking for the next NLP competition to get a refresher. Okay. Got it.

Sanyam Bhutani  56:45  
Thank you so much CPMP, for doing this interview, and for all of your amazing contributions to the community.

CPMP  56:51  
And thank you for your blog and video blog. This is very interesting. Thank you so much. I read all and watch all of them.

Sanyam Bhutani  56:59  
Thank you so much.

Sanyam Bhutani  57:11  
Thank you so much for listening to this episode. If you enjoyed the show, please be sure to give it a review or feel free to shoot me a message. You can find all of the social media links in the description. If you like the show, please subscribe and tune in each week to time data science.

