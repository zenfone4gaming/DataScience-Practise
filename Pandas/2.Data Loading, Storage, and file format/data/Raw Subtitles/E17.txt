Sanyam Bhutani  0:13  
Hey, this is Sanyam Bhutani and you're listening to "Chai Time Data Science", a podcast for data science enthusiasts, where I interview practitioners, researchers, and Kagglers about their journey experience and talk all things about data science.

Sanyam Bhutani  0:46  
Hello and welcome to another episode in the "Chai Time Data Science" show also the second episode in the Transformers slash NLP release series. In the previous interview, I interviewed Andrew Shaw creator of music Autobot, which is an AI powered music generator, you should definitely check out the episode if you haven't. In this episode I interview another one of not just mine, but I believe also machine learning hero to many people, the CTO of hugging face Julien Chaumond. We talk about his journey into the field and his path as a CTO of hugging face. Hugging face has been doing some very amazing work that I'm pretty sure everyone is familiar with. And I try to discuss all of these open source and research aspects being done at hugging face including the life transformer demo in the browser, the demo of the conversational a go reference research and the demo, which also is available for you all to check out the open source library and the into implementations of transformers. Note that this was before the release of the TensorFlow ported version of transformers. So unfortunately, we don't have the discussion about TensorFlow. In this interview we talked of course all about the Pytorch side of things, and things on the iOS world as well. Again, face also has shaped models onto iOS devices which we cover in this interview. I'm really excited to share this interview and I hope you really enjoy the conversation.

Sanyam Bhutani  2:32  
I am really excited to have Julien on the podcast. Thank you so much for joining me today, Juliae.

Julien Chaumond  2:38  
Hi, Sanyam. How are you? 

Sanyam Bhutani  2:40  
I'm great. Thanks. Thanks for doing this. And thanks for joining me.

Julien Chaumond  2:45  
Thanks for having me. Yeah.

Sanyam Bhutani  2:48  
It's It's an honor, I'm a huge fan of things being done at hugging face. I've actually generated an intro of your profile we will have an official record but using your website, so If I may read it out. 

Julien Chaumond  3:02  
Yeah.

Sanyam Bhutani  3:03  
So the queue so to people who don't know, go to transformer.huggingface.com, you can send a text over there using a transformer model. You can give a queue I gave the queue Julian is the CTO of hugging phase. And this are the models indicated he has written many books on the subject of hugs, including hugging your face, and hugging yourself. He likes to read people's blog and other interesting things. You can read to him on Twitter. And then there's a Twitter handle and your email. So what are your thoughts on that?

Julien Chaumond  3:35  
Yeah, that's a pretty good description. Yeah. So now, I've been amazed with gvt two shifts by open AI less than a year ago actually. So it's pretty recent. And I think it's been like really impressive. So we've taken taking the models for the them toPytorch and then I catch them into like a nice demo application webinar. And it's really, really cool. Like people have been using it for a lot of like, super interesting things. [Yeah] And yeah, it's really great to figure it out that it's huggingface.CEO.com, just so that people can find it more.

Sanyam Bhutani  4:25  
I'll have it. I'll have the correct link in the description for people who want to check it out. Before we talk all about the open source projects. Could you tell us more about the work being done at hugging face? And could you tell us what is the product by the company?

Julien Chaumond  4:39  
Yeah, definitely. So we started the company with my co founder, Clement, three years ago today, and we were super interested in NLP. From from the right from the start, and we were thinking that NLP was going to be like a super big subfield of AI basically, we've always been like kind of passionate with text, text processing. Even like before the deep learning techniques were widespread. So we were like super interested in NLP. [Right] We wanted to dive into NLP but also to try and find a consumer use edge that would be that would be different from what the other big companies big tech companies were doing. So we we start we started by focusing on more entertainment oriented vision of conversationally I read So, our first product was like a fun conversation or a basically like an AI that you could talk with and have fun with and kind of became become friends with enough time. So we worked on that product for a couple of years. And we've had like a lot of usage. And like a number, a big number of monthly active users, daily active users are using the app.

Sanyam Bhutani  6:38  
I think the number is up in a few hundred thousands if I look at;

Julien Chaumond  6:42  
Yeah, exactly. Yeah, yeah. Yeah. So yeah, basically, like half a million monthly active users. And it's been it's been growing ever since. What we realized, however, over the last couple of months is that it's pretty hard to do a really good conversationally I, [Yeah] that was like one of the big learnings of these first years of working on hugging face was that the tech and the science simply isn't there yet to do super impressive version of conversational AI, which is something that we have in our mind, like the long term goal for us is to be able to like to build a conversational AI that's going to basically assist you and kind of like replace the usage for you of Google Facebook with some things that will be like way more fluid and way more efficient. So basically, our our vision of a conversational AI is really, really is going to be really really impressive and useful for 

Sanyam Bhutani  7:54  
Almost like a real human.

Julien Chaumond  7:57  
Yeah, almost yeah, yeah. Or even like better acceptance tests that a human so we don't want to look you see, we don't want to replace humans. We like humans. Yeah. But yeah, basically we think that we really believe that AI and positionally I are going to enable us edges that are going to be like so efficient and, and, and transformative for stuff that we're doing. So anyways, we we think that there is still a lot of fix to do on the science and the tech side before being able to ship something like that. So we've kind of taken a step back and decided to focus focus even more on the science and tech side for a few years. Yeah. So because we we've seen and we can probably speak about it later, but we've we've seen that we've been able to to be achieve some breakthroughs on the science side as well. So yeah, for now we want to really dive into the science push the state of the art and do great, great things on NLP.

Sanyam Bhutani  9:13  
Got it, we'll definitely talk a lot about the research side the science side of things that you've been pushing but for context when you started working on this problem now we have Transformers we have these Bert models that look like human genetics but back in the day and what were you even thinking that this talent would be soluble to this extent like it is in 2019, for example.

Julien Chaumond  9:41  
I mean, we were kind of like, already in like 2016. When we started the company, we were seeing that a lot of things were happening in NLP. So basically 2016 was the year of like neuro, neuro beta conversation. So the first systems that were using like deep learning networks to do automated conversations. At that time, it was mostly like RNNs, LSTMs. [Yeah] That were powering conversation systems. It didn't work super well in terms of like quality of conversation, obviously. But it was still pretty impressive that you couldn't get that you could try and apply like end to end, deep learning techniques to do something that was traditionally done, we would have like hand coding and rules, because that's under conversation part and even on like, more globally, like NLP tasks. We were seeing that a lot of things were happening. And so we were kind of thinking that it was going to continue with we've been Surprised, obviously with the with the growth that we feel like a lot of people doing incredible things in the field. And yet basically the past couple of years, we've seen an acceleration of breakthroughs in nnp, which is like super exciting. 

Sanyam Bhutani  11:17  
Yeah. Now to talk a bit more about the founding team, I think I saw an interview with Clement, who's the CEO hugging face, who mentioned that you were earlier working on rival companies for a few years. And then you decided to start a venture together. So how did you decide to work with him and what made you pick him as a co founder? Because usually, like, personally, I would imagine I cannot work or even talk to a person who's building a rival company.

Julien Chaumond  11:44  
Yeah. So yeah, we met close to 10 years ago, in Paris, France. We were working on kind of like, maybe not concurrence that like not competitive, but may must be like startup companies in the same space. And we kind of like, became friends at that point. And then we both did other startups or other projects over a couple of years. But we stayed in touch. And then yeah, basically in 2015. We were both like super interested in machine learning and NLP used like Clement used to work at a machine learning company that was acquired by Google couple of years back, and I was diving into machine learning again. So we were both like super interested in in the same subjects. But most of all, we kind of always knew that we wanted to start a project together because we felt like it would be a good fit like a good doesn't fit so much. See, like destiny, no small feat, and kind of like a desire to try to work together and see what happens. 

Sanyam Bhutani  13:09  
Got it. 

Julien Chaumond  13:10  
And it's been like super. It's been a really good experience.

Sanyam Bhutani  13:15  
I'm sure. So talking about your background, you have a very interesting background you worked and project management, then as a founder, you created prod cast company, you were founded there, then you've also worked as a software engineer. Could you tell us where did deep learning start to come into the picture for you and what made you pick this has an interest of you?

Julien Chaumond  13:37  
Yeah, so back in 2005. Actually, I was in Polytechnique, which is an engineering school in France. [Okay] I was in the, like, I was majoring in computer science. Obviously machine learning was not a huge field in faith, just because like you to It was not working well, like the computation was not there, the new techniques were not there. And it was mostly seen as kind of like a toy toy set of methods to, to kind of brute force was a issue.

Sanyam Bhutani  14:21  
Python libraries would have existed I think, then pandas might produce anything literally.

Julien Chaumond  14:26  
Yeah. But you need to know something about French engineering schools is that, like, the professors are usually driven by theoretical computer science background. So they will read what what they really want to work on is kind of like algorithms, or will you prove that this algorithm is the most efficient way of resolving this issue? And basically, that's kind of like the opposite. The exact opposite of machine learning, which is kind of, okay, let's try to approximate this function and we we don't really know why it works. But at the end, we're going to have something that kind of works. Right? [Yeah] So yeah, like machine learning was definitely not a huge field. But I was still with a friend of mine, we were still kind of trying to apply like really small neural nets to your robotics competitions. So I was kind of doing like really small neural nets, but it was not working that well. So basically, I, I did a lot of like software engineering engineering over the, the next couple of years, doing kind of like mobile apps in that that next step. And in 2015 I was like, as I said, I was super interested in text processing. So I i dive more into machine learning. And I kind of like, took a few nine classes. And because I really wanted to see what machine learning had become in the in the eight years since I last looked at it, and yeah, basically that's why I was in 2008. It was like super excited about like the new things happening internationally. And;

Sanyam Bhutani  16:28  
So I was going through your profiles, and please help me understand you have, I think about 1700 contributions on GitHub, two different repositories. You published a bunch of papers in the recent years. And you're also working on the startup so what all the activities are you handling and what does it day in your life currently look like?

Julien Chaumond  16:49  
So in the city of the company, so in the company, everyone is pretty much like really experience experience, both with machine learning and Software Engineering at the same time, which is like not that usual because you sometimes find people who are like really into science and machine learning, but well not that great at code actually coding things and having systems that were for a long period of time, like even deploying deploying models into production and things like that. So we try to build like a kind of homogeneous team of people were like really good at both machine learning and software engineering, which also implies that I, I basically, work with the team with the technical team, but everyone is pretty much autonomous and and like self driven. So what I what I'm doing is I kind of help the team working on a few different projects, but kind of Like change projects over time, every, like couple of weeks, a couple of months, which is like something that really feeds my way of working on technical subjects.

Sanyam Bhutani  18:10  
Got it. So for all of the projects that are going places currently working on, you have some sort of contribution there.

Julien Chaumond  18:19  
Yeah, I try to Yeah. 

Sanyam Bhutani  18:21  
Got it. To talk more about hugging face you have raised just north of 5 million US dollars. Can you tell us how will that play a role in your future? Will you focus a huge amount of that into research pushing the science? Or what do you what are your future plans like and what rearrangements did you have to do after raising the seed investment?

Julien Chaumond  18:43  
Yeah, so we were super, super lucky because we found investors were a super good match to our vision, which is let's not try to find monetize business application just yet. Let's just focus on the growth, let's just focus on kind of pushing the state of the art in energy. And then if we manage to create something valuable, we find ways to, to monetize at some point in a way that's consistent with our, like, it's equal vision, and like the vision of like NLP as a whole. So yeah, we were super lucky to have them. And we, it gives us some time to really push our subjects and push energy, which is great.

Sanyam Bhutani  19:43  
Because essentially, you're also solving a scientific problem along with a business problem. So I think science is the major factor.

Julien Chaumond  19:50  
Exactly, exactly. The way we describe the company is consumer science. [Okay] Let's try to push the Science in the field of machine learning, and at the same time see how we can unlock some like consumer usages at the same time.

Sanyam Bhutani  20:10  
So yeah, got it. Now I really need some help understanding about the projects and open source ideas. Because there's a lot going on here, maybe we can talk one by one. But to mention all of them at once you have a live demo for transformer open GPT to open AI GPT to model running in the browser. You have a demo website of a conversational a core reference demo, again, running in the browser completely. An open source library implementation of Transformers with more than 10,000 stars on GitHub. Could you help us understand and maybe break down how these projects are being run? Because I was reading up and I think the team is this between 10 and 15 members?

Julien Chaumond  20:52  
Yeah, it is.

Sanyam Bhutani  20:54  
So maybe let's start with the browser transformer model.

Julien Chaumond  21:01  
Yeah, so so we've we've seen over the last year basically that large transformers, language models are really important breakthroughs in an AP basically unlocks a lot of like super interesting things. And I think that we we are just getting started in like discovering new applications and new ways of optimizing large language models in NLP, and seeing how we can and seeing how we can use them to do like incredible things. So the two like I would say, the two most game changing Transformers model last year, yet we're GPT today from open AI and Bert from Google AI both of them were initially implemented by the researchers in TensorFlow. [Yeah] And so like the, the, like, pretty naturally, we wanted to use them in our technical stack, which is more Python space. Interesting. So Thomas Wolf, which is work with our Chief Science Officer, he kind of like took a week off to really sprint sprints and kind of Port BERT and GPT-2 to Pytorch. It was an incredible feat. It was like really, really exciting projects project and we we saw that it was doable to implement them in Pytorch.

Sanyam Bhutani  22:57  
For context, even though they do mention sometimes, but there is no right mapping of putting one model from one framework to another.

Julien Chaumond  23:05  
Yeah, yeah. Yeah. Basically you like the, you see that there is always like, you know, some kinds of like inconsistencies in implementations. It's not really related to titles versus TensorFlow, but it's more like about kind of modernizing information the way different researchers go about. Things like diverse things such as like initializations inference, like global like model architecture, training, resource evaluation, things like that. So it's, it's not like it's usually not the model itself, where it's all the different parts around it. Kind of like dimension and setting that up. Okay, how to how to organize? So basically that was the start of our library pytorch transformers, which kind of became of uptime and pretty pretty pretty quickly, actually, kind of regulatory of large transformer noodles. [Yeah] It is why we, which is why we decided to name into a top transformers. And as you said it's been like really, really popular. It has more than 10,000 stars on GitHub. But more importantly, people use the library to, to, to use pre trained models. Then they download more than I think 20,000 different models a day, which was really pretty massive. Yeah. [Okay] Yeah, that's really, really messy. So it's been a super exciting experience. And we plan on continuing to develop the library. And we have like lots of ideas on how to improve it, and how to make it slightly easier and easier for both researchers and developers to US state of the art energy methods. So that's basically the vision for the for the library.

Sanyam Bhutani  25:25  
Got it. Now, I also want to talk about you mentioned Pytorch. So I'm definitely a Pytorch fan because I come from a fast AI community. So I learned all about Pytorch. But another thing that I read on Twitter is one of your models, it was very well engineered. The complete code was just under 500 lines and the inference training, everything just in three files under 500 lines. So could you talk a more a bit more about what efforts you put in there and what made you pick Pytorch over TensorFlow for I assume even your internal purposes?

Julien Chaumond  25:57  
Yes. So our goal is To make it so that it's really easy for both developers and researchers to understand new models. So one of the goals is that the architecture is pretty easy and self contained. So you don't have to read through like how hundreds of pages of documentation or hundreds of phase of good to understand what's happening, you can basically, yeah, things are pretty self contained in one of our philosophical goals in this project is that it's super easy to kind of onboard to get on board and start actually, both using models and then kind of start tweaking the models co you can change like different training parameters and see if you can get to beat the state of the art. So that's kind of like, really exciting is that our latest library is used by researchers to unlock new methods. And to discover new state of the art. It's not not just a way to like kind of put down words way of using new state of the art method, but it's also a tool to yeah, exactly.

Sanyam Bhutani  27:23  
That gives an amazing me totally is this is a company, a VC funded company that's trying to push public research. So research that everyone is currently using. So I keep getting blown away by that fact.

Julien Chaumond  27:37  
Yeah.

Sanyam Bhutani  27:39  
To now to talk about the second project is a conversational AI demo that runs in the browser. So this is different from the app that you ship. Could you talk more about that?

Julien Chaumond  27:51  
Yeah, so we build that and Thomas Wolf, again, was leading this project We were we decided to participate in a competition last year at NeurIPS the NeurIPS, Machine Learning Conference. 

Sanyam Bhutani  28:11  
Yeah.

Julien Chaumond  28:13  
The competition was called convey AI conversational AI challenge and was organized with the support of Facebook AI. The goal was to build an end to end conversation system that was trained on a data set of conversation called Persona Chat. And so what we tried to do and we actually won the automated evaluation part of the competition, which is right to do is we try to apply the GPT model to conversation Basically, it was a very innovative way of trying to solve conversation by using a pre allowed pre trained language model and use it to generate conversation.

Sanyam Bhutani  29:17  
I'm also curious like, what's going on in the background? So for example, and how is this being put into a website for both the problems for the transformer? What's happening in the background when I'm trying to generate a sentence? And same for the conversational AI?

Julien Chaumond  29:37  
So yeah, so the demos that we shipped, they are web web based demos. But the computation is actually happening on the server. Just because it's not doable right now to to do jiki to scale computation directly in the user's browser. The model is simply to be an you mostly need a server with either a GPU or a really big CPU to be able to answer your request within a reasonable amount of time, which would be compatible with like the UX that we want to provide the user. So basically, what's happening is that we kind of like tweak Pytorch Transformers a little to enable either. And autocompletion in the case of light transformer dot huggingface.co, or conversation the case of our conversation demo.

Sanyam Bhutani  30:48  
Got it. I also really love the beautiful UI that you just mentioned. So could you maybe speak a bit about what efforts did you put in there when connecting the US with the server that it talks to.

Julien Chaumond  31:01  
Yeah, definitely. So to speak with the example of right wrist transformer. And it was actually a demo that we wanted to do for a really long time, basically like for the last couple of years, like for more than two years, we've been wanting, wanting to do a demo with autocompletion.

Sanyam Bhutani  31:28  
That that have been your original vision of the app, maybe.

Julien Chaumond  31:32  
It was not the original vision, but it was something that we've that we've been wanting to do for a super long time. The language models like the generative capacity of the language models before TPT two, were not that great. So you could use like, there was a really nice project called AWD LSTM by Stephen Merity Yeah, it was a really, really nice implementation of a state of the art language model probably two years ago, about two years ago. And we try to use it to the generative interface, and to enable kind of like an autocomplete, but it was not working well enough that the experience was super good. Now, when we saw GPT, two was out, and we saw like, it was really good at like, generating text. That's when we so yeah, this is going to be a really, really nice language model to try and do something great on the UX side. And I should also mention that he it's a work that was inspired by this project called talk to transformer. [Yeah] Which was more like kind of like you one of like, you start typing a paragraph and then the model autocompletes. But we so it was a really, really nice project. We loved it. It's, it's been coded by great developer called Adam King. It's really nice. But what we wanted to experiment with was something a bit more iterative with where you could kind of fake trigger and autocomplete then but then also kind of tweak the text and modify it and completed manually and then kind of like yeah, trigger trigger the NLP model more iteratively in a kind of human in the loop way. 

Sanyam Bhutani  33:51  
Makes sense makes sense.

Julien Chaumond  33:53  
Yeah, that's why we really wanted to push like this kind of UX And we've been like super happy with the with the way people have been using it. And we've had more like more than 100,000 users on the web app. So yeah, it's been super interesting to see how people use it to to either, right? They can write like fiction, like fun relief, and we have stuff, but also use, you can use it in a more cru setting, kind of. We've seen students using the app for essays, stuff like that. Yeah, it's really impressive the way the different ways you can use the technology.

Sanyam Bhutani  34:42  
Got it. Now coming to the open source side of things. So back to the Pytorch dash Transformers GitHub repo. Could you tell us more about what old Transformers live in there? And how did you manage to put all of these as open source projects because Transformers usually are in TensorFlow, because that's because they trained better on TPU port, so they usually pin it on TensorFlow.

Julien Chaumond  35:09  
Yeah. So, um, I would say today GPT-2 and BERT are the most popular Transformers that we have.

Sanyam Bhutani  35:18  
Okay.

Julien Chaumond  35:19  
On the repo. And both of them, like, together, they probably account for like, two thirds of the downloads of the model downloads that we have. 

Sanyam Bhutani  35:32  
Interesting.

Julien Chaumond  35:33  
But yeah, we also have an implementation of Excel net from Google. And from an implementation of that we've seen pretty recently like less than a month ago of Robota from Facebook, Facebook AI. Robota is basically careful pre training optimization of belt so it's really close to built with a few few few differences that we've seen a lot of interest for pobelter or other the last couple couple of weeks. So we think that it's going to be pretty popular in the next couple of months. And yesterday, actually, we released a new model called the steel belt project that's being led by Victor Sanh, from our team, who's a really great research scientist on our team. And what this is about, so this is about is basically shotcut for distilled version of that. And distillation is the process in which you train a smaller neural net to kind of replicate the I would cook for larger neural net. So what we what Victor has done is, is sharnk the BERT model by 50%, while keeping, I think close to 90-95 or 96% of the accuracy of both of the original but on the downstream test;

Sanyam Bhutani  37:19  
That's really amazing to to pause you for a second for the for some context to the audience, could you help us understand what resources there is one need to be properly able to play around with Bert and then with DistilBERT.

Julien Chaumond  37:32  
Yeah, so basically Bert to use it in production. You either need a GPU based server, or which is obviously more pricey than a server. Especially if you get into a product that has to have a certain scale next set of volume of numbers. requests per minute or something. And what this is about is enabling his kind of methods to while keeping most of the same output accuracy, enable people to deploy to way smaller CPUs, or two with the same latency or to shrink the latency. If you keep the same kind of cells, you're able to run like more larger volumes of requests and have them served in a in a way less time. So yeah, basically it's a it's an essence to optimize Bert like models for production. 

Sanyam Bhutani  38:45  
Got it. To talk more about this transformer requirement. What are your thoughts on for example, you hugging faces pushing in this direction, but to push the state of the art and NLP do we really need now  unavoidably a huge cluster of GPU? or how can a person with a small cluster or maybe a single GPU push the state of the art?

Julien Chaumond  39:12  
Yeah, so I think definitely you don't need like really large clusters of GPUs. If you have access to really large clusters of GPUs, and by really large, I mean, like, maybe you can. It's, it's, it's certainly interesting to have access to this kind of compute, but it's probably going to give you two words, research. That's kind of biased by the amount of GPUs that you have access to. So you're probably going to compete with like the big tech companies, or like kind of like a would say computing driven machine learning research. Whereas there is also a lot of interesting research to do in the field of like, more. Yeah, like more resource constraints machine learning. At the same time, like having a pretty good GPU is like, at least like one or two GPUs is doable. There are ways to also get access to like credits from the cloud computing providers to have access to a couple of GPUs. And it's like, you can already do a lot of like interesting things with a couple of GPUs. So I would say that it's not a super it's an interesting to have access to a lot of GPUs, you say the opposite. But you can also work on really interesting things with a small number of GPUs. 

Sanyam Bhutani  40:52  
For sure. So I think what is suggestive is if you have a single or maybe two GPU, use them for prototyping and then for a few less segregated funds, you have the optimal solution. Just try running it with the broader server and see what what direction does that lead to?

Julien Chaumond  41:09  
Yeah, definitely. Yeah. Yeah. That's a good approach.

Sanyam Bhutani  41:12  
Got it, now to talk about the crazy speed at which models are shipped. So there's also small rebel that lives inside of the transformer report, which in itself is amazing is the swift repository, we ship these things on to an iPhone. First, I'd also really like to know so we were just talking about these things need either a GPU or a heavy, heavy CPU. How can these monstrous models run on a small iPhone?

Julien Chaumond  41:39  
Yeah, so I yeah, so the thing is, on iPhones, specifically, do hardware is actually pretty good, right? Like the GPU that's in an iPhone is competitive to a small GPU that you can found on the laptop right? Yeah, that's pretty interesting. And in the most recent versions of iPhone, they actually have a dedicated chipset for machine learning, which they called the neural engine, which used to not be accessible by sabaki developers. It was only used in previous versions of the US and for native Apple apps. But now you get to you get to access the neural engine. And for like, for instance, for computer vision related tasks, you can get a huge huge boost in performance using it. So I would say that the the most recent mobile devices are really good that forms are going to be good platforms in the future for what we call edge machine learning. So not doing machine learning on servers, but doing machine learning directly on the user's device, which can be good for both like performance reasons, budget reasons, and also for to develop kind of like privacy first services, because the less the less data you need on the server, the less risks of kind of data leaks or unethical approaches you're going to have. So basically what we try with so Apple, first of all, they used our Pytorch transforming repository to train a model that doesn't that didn't showcase at WWDC conference here. So we were really excited because when Apple uses your open source, the reboot validation for what you're doing and then we We bought it TPT to as well. DVD to was interesting because you you We started by converting the the smoke model of TPT to do it was already kind of too big for an iPhone, like you need the latest iPhone or an iPad Pro to actually run it generation. It was pretty fast we need managed to work. So the bottleneck was more memory constraints, like it was using it was using a lot of memory because like the network itself, the weights themselves are really large. So the bottleneck was more on the memory side of;

Sanyam Bhutani  44:40  
iPhone, just iPhone and iPad this have three to four gigabytes of memory, I think;

Julien Chaumond  44:45  
Yeah, yeah. And like one particular app isn't any like constrained to small fraction of that memory. So you can just use all the or the system's memory constrained to maybe one gigabyte or two gigabytes most. But what we've managed to do is kind of like slim down the GPT to model a point where he where we were able to run it on the most recent versions of iPhone. [Okay] So yes, it's an interesting, niche, interesting project. And the vision is that, as I was saying, we think that at some point, more and more machine learning is going to be to happen on the edge for a bunch of reasons. So yeah, it's good, even though it's hard today to read productize machine learning models on the edge on devices. We still like investing time in trying to trying to push that subject as well.

Sanyam Bhutani  45:54  
What efforts do you have to go through when like putting these models onto an iPhone, so definitely isn't an easy job. What challenges do you face when working on that?

Julien Chaumond  46:04  
Yeah, so Apple has their own kind of like machine learning intermediate representation system called core ml which is kind of similar to a need if you've heard of an X and an x, which is kind of like a format for production optimized. And your next. So Clement is, is really close conceptually to onyx. And basically, you have to kind of remap your betaworks model to a government model, which is kind of use at time but it's not super hard.

Sanyam Bhutani  46:53  
Got it. I think, would also love to know your points, but this would definitely push it direction of privacy. So people are always concerned when you're talking to models. The recent issues in Europe happened. What are your thoughts in there? So because parking face does shape up the talks to an API, so how do you think of privacy there? And what are your thoughts around that topic?

Julien Chaumond  47:19  
Yeah, so we are pretty serious about first of all, protecting the users data. So we encrypt each user's data on the server. And try to secure that data the most efficient way. each user's data is obviously in a separate silo. You're not going to be able to get someone else's data just by talking to across generally I which shouldn't happen and more like kind of on a more on a higher level. We we, we discussed the subject of like AI over the last couple of years, and we wrote a blog post about what we think is some important a values for AFL companies should be. And so it's a subject that we think that we are thinking about, and that we are that we that we think is, is going to be increasingly important in the future.

Sanyam Bhutani  48:33  
So when we use the app, because I'd love to know what data sets do you use so for conversation, GPT to train is trained on Wikipedia. And a normal chat conversation doesn't look like Wikipedia. So you train on the user's data or what, how has that model been?

Julien Chaumond  48:53  
Yeah, so we trained on a number of different data sets of conversation. So I was mentioning the best and set data set, which was put together a year ago. It's a small data set. So you're not able to find today like really large data sets of conversation. But using transfer learning, you can still leverage smaller data sets of conversation. And we have our own data sets of conversation that we use as well. And using transfer learning is a good way of being efficient, even with smaller data sets.

Sanyam Bhutani  49:37  
Talk more about research. So maybe the still Bert would be a good example. But you have published a lot of great results results in top conferences. What research projects are you excited about and how does your team pick a team that they want to work upon?

Julien Chaumond  49:57  
Yes, right now, everyone working on like older, older results, researchers working on like let knowledge transfer mo or large language models, which we are trying to kind of like seeing what we can do to help them. And it's really interesting subjects. Yeah, like more precisely, I've been like, personally super interested in like, college conditional language generation conditional text generation over the last couple of months. And something that I think it's going to be super interesting in the next year.

Sanyam Bhutani  50:40  
So what's the difference between conditional generation versus normal generation, what happened;

Julien Chaumond  50:46  
Basically, so to take the example of GPT to for instance, you can have like seed the you can have seed the text generation is a small paragraph. [Yeah] Basically the only control that you have under generated text. So what if you could also improve at the same time? Like a structured data points such as like maybe, emotion? Whatever, what what what should the generic text emotion look like? What should the generic texts and that kind of language look like? Should it be like familiar language? Should it be like a release text, things like that, yeah, way to kind of like conditioned the generic text. Another example is basically, code, like you can already kind of use GPT to output code if you start typing like HTML or JavaScript or Python. You get DPD to to actually output Python code. It's not always valid code. So what what if you could add the constraints that the Python code has to be valid? Or the I don't know, like the C++ code has to compile, the late the Latics source code has to actually mean, very Latics code as what things like that.

Sanyam Bhutani  52:26  
So you're aiming to replace a researcher and also programmer by doing that?

Julien Chaumond  52:32  
Yeah.

Sanyam Bhutani  52:35  
That's that previous case, he would also interesting. Could you maybe also talk a bit more about the still word, so the open source version is already out? can be tested out? And do you plan on releasing it on the iPhone as well?

Julien Chaumond  52:50  
Yeah, yeah. So it's something that we've been thinking about. Someone has called the work that we've done on this keyboard nmt equivalent of mobile net, okay, yeah, so mobile net in computer vision was, is a set of techniques that people use to run like image net, train large computer vision models on small mobile devices. So if we manage to, to create something that's kind of like similar to mobile net to be like super interested. The thing is with these two books, yeah, like mobile devices are important, but even like on on cell phones, it's going to be I think, like super popular because a lot of systems, a lot of services. It's going to enable them to use large transformer models in production, because it used to be like too slow or it used to require GPU servers, and now he's defeatable to you're going to be able to do it and like a regular silver Like AWS or Google, it's going to be like super, it's going to be way easier to deploy state of the art energy into predictions, which is something that we are really, really excited about.

Sanyam Bhutani  54:13  
Could you also tell like how much of efforts in terms of maybe mentor person went into this? So I think the only a few collaborators on the project?

Julien Chaumond  54:24  
Yes, Victor Sanh, Thomas Wolf and Lysander, have been working on the on the project for maybe couple of couple of months. Yeah.

Sanyam Bhutani  54:41  
Got it. Would you also be pushing into this direction? So again, this is an interesting direction of building Transformers that use lesser resources again going towards the whole pilot side of things or what current research are you excited about?

Julien Chaumond  54:57  
Yeah, I'm super interested in the in the field. In the in the subject like other ways of making, making it easier for developers and researchers to build on top of like state of the art research and to push state of the art research and to use it in production at scale is a is basically that the thing that I'm most interested about.

Sanyam Bhutani  55:29  
Got it. To talk more about how did you learn these techniques? So I found an older blog posts you were putting out an invite for I think the 224n course. [Yeah] So what are your thoughts about the your current favorite online courses may be and which ones did you personally go through while you were picking up these techniques?

Julien Chaumond  55:50  
Yes. So basically, when we were really diving into NLP three years ago, We found the Stanford class which was told by Richard Socher at the time [Yes] so we call a CS224n it was a really good way of kind of like putting a small study group together and going through the videos going through the exercises, and kind of like keeping ourselves like really motivated and, and doing a lot of things in a short amount of time. So it was a really good, really good way of kind of like diving into the subject. I really recommend it. It's a good class. You can find like newer versions of the videos. In the recent years, it's been told by Professor Manning from Stanford, the content is really good. It takes a lot of time, obviously, because such an advanced class. It's taken by basically Stanford PhDs. So it has to be a good class, right? [Yeah] But it's a good way, I would say to really dive into NLP. And the second thing I definitely recommend is a is a fast AI. We were you were mentioning it's I think it's a really good, good class. And they're doing like the fast AI team is doing a really good job at having new new a new generation of NLP researchers or machine learning practitioners into the field. So it's really good content, as well.

Sanyam Bhutani  57:55  
For the viewers who don't know so fast AI usually is famous for its two deep learning courses there's also NLP course that they recently put out. So I've just completed watching it, do do check it out it has a lot of interesting stuff. To talk more about Transformers, because back when I scroll through 224n I don't think it covered these topics, so how do you personally when something new comes out, go through it or how do you pick up these techniques personally?

Julien Chaumond  58:25  
Yes, so the really good thing about having a such an experience team as we have is that, like everyone in the team is, is proficient and kind of like following what's happening in the field. So it makes being on top of like new things way easier, because we just like basically, we just basically talk about like new papers every day, right? [Yeah] Every day we like Twitter is a really good tool to find out about new techniques, new models. And basically being a team is a is a really good way to, to find in that out. I didn't mention it but we are based in New York. So most of the team is in New York. We have a few people in Europe as well, but most of the team is in Europe. And I'm a really, really strong proponent of being in the same place and kind of like having conversations like impromptu conversations about new papers, new new techniques, makes it like super easy to do yet to learn about new things, which is a really good setup. 

Sanyam Bhutani  59:54  
Got it. Before we continued the conversation, so this was an amazing insight into all things, hugging faces doing thanks to that but what basic advice would you have for people who are just starting their journey and want to work in this intersection of NLP and conversational AI?

Julien Chaumond  1:00:14  
Yeah, so my advice would be just take the leap, right, just passionate about the subject, just start studying, just start. So I, I really like to to learn by doing so I would advise to find something to do like maybe a small open source contribution. Yeah, open source is a great, great way to learn because you're learning kind of like from the bottom that from the implementations, it's super, like super natural to dive more into the theoretical part of machine learning once you've kind of see how things are actually working on the implementation side. So yeah, I would, I would definitely my advice would be take the leap and start doing some open source contributions.

Sanyam Bhutani  1:01:10  
Got it. Do you have any new;

Julien Chaumond  1:01:12  
Pytorch Transformer, our repository is a good way for that. Because we try to be like really welcoming to first time contributors. So I would advise, why not have a look at the Pytorch Transformers? 

Sanyam Bhutani  1:01:28  
I was just asking you about that so what are the low hanging fruits? For example, a newbie who's coming to your repo? Where should they start and or contribute to?

Julien Chaumond  1:01:37  
Yeah, so we tried to give indications to to users in the GitHub issues. And we have a lot of fields lead that would be like not super help to dive in, but that would be super interested. So because we like we have a set of different usage examples. So for instance, you can use our libraries to find you in a different set of NLP tasks such as like squad, glue, something that that. So yeah finding like a script, an example script to fine tune on a new task or to improve the way we find you in an existing test are like pretty, pretty easy ways and efficient ways to dive into the library.

Sanyam Bhutani  1:02:29  
Got it. Thanks for that amazing advice and all the amazing things, before we end the call, could you maybe tell a few platforms where we could follow you and your book? What would be the best platform? 

Julien Chaumond  1:02:40  
Yeah, definitely. So the Twitter hugging face account is a really good way to keep in touch so it's twitter.com/huggingface, or we we try to publish a lot of things on medium as well. Hugging face on medium and of course, our GitHub page and GitHub repos, which is like which are like a good community directions for us.

Sanyam Bhutani  1:03:12  
Got it we'll have all of those links. What about your profile? Where can we follow your route? Your personal profile?

Julien Chaumond  1:03:19  
Yes. So I'm Julian_C on Twitter. You can follow me on Twitter and send me send me a tweet. 

Sanyam Bhutani  1:03:27  
Okay. Got it. Thank you so much, Julien, for joining me today. And thank you for all the amazing efforts in the open source and research direction from hugging face.

Julien Chaumond  1:03:37  
Thank you so much, and let's keep in touch.

Sanyam Bhutani  1:03:50  
Thank you so much for listening to this episode. If you enjoyed the show, please be sure to give it a review or feel free to shoot me a message you can find all of the social media links in the description. If you like the show, please subscribe and tune in each week to "Chai Time Data Science."

