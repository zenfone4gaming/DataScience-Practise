Sanyam Bhutani  0:13  
Hey, this is Sanyam Bhutani and you're listening to "Chai Time Data Science", a podcast for data science enthusiasts, where I interview practitioners, researchers, and calculus about their journey, experience, and talk all things about data science.

Sanyam Bhutani  0:45  
Hello, and welcome to another episode of the show. In this episode, I interview with another one of my gurus and heroes from the fast.ai community Sylvain who's a research scientist at fast.ai. In this interview We'll talk about a lot of exciting, upcoming developments from the fast.ai research lab, the framework, the book and the upcoming course. The upcoming course will be released along with the book and the library as a MOOC later, around the middle of this year. Again, I'm not too sure of the dates, but I think you can follow Jeremy and Sylvain on Twitter to stay up with date on it. We talk all about what can we expect from the library, the course and the book. What sorts of efforts go into these and what topics is the book and the course going to cover is the newer version, I think of fast.ai as a TV series as a movie whose sequel always keeps getting better. So I'm really excited to be talking about all of these with Sylvain. Sylvain also shares with us how does he and Jeremy approach research ideas and how do they collaborate on these ideas. This interview contains many exciting, exciting upcoming insights from the fast.ai lab I'm really excited to be revisiting this interview. Without further ado, here's my interview with Sylvain all about fast.ai, the course, the book and the new framework fast.ai v2, please enjoy the show.

Sanyam Bhutani  2:30  
Hi everyone. It's it's an unbelievable honor for me to have Sylvain, my hero from fast.ai for the second time on my interview series. Sylvain, thank you so much for joining me on the interview series again.

Sylvain Gugger  2:42  
Thanks for having me. 

Sanyam Bhutani  2:44  
It's an honour to have you. Now I want to talk about one thing. In a previous interview you mentioned you discovered fast.ai through a New York time article. Did you expect yourself to be working with Jeremy a few years after you read that article?

Sylvain Gugger  2:58  
No, not really!

Sanyam Bhutani  3:03  
Talking about could you tell us what how did the dots connect for you? How did you end up joining the team? What was Jeremy's hiring process like? 

Sylvain Gugger  3:13  
Oh I mean I still don't really know to be honest because like the course went on and I was kind of doing things but not really I didn't have anything in mind except running as much as he could so was doing all those kind of projects and when Jeremy was leaving all sorts of challenges on the forums so and I kept responding on answering and sharing good then yeah, and then the course ended and week after he asked me oh, would you like to work at fast.ai and I was like oh, sure. Who wouldn't want to work at fast.ai and that, that's kind of how it went. 

Sanyam Bhutani  3:50  
Okay. 

Sylvain Gugger  3:51  
But there was no formal, no formal process.

Sanyam Bhutani  3:59  
How has the journey been on the other side for you. I think it's almost been two years since you joined the team. Can you tell us how has the journey been any highlights from those years?

Sylvain Gugger  4:09  
Yeah. So beginning July, so when you're-right. How's it been a bit tiring. Very-I learned. I learned a lot, obviously with Jeremy. And I don't know. Yes, you're seeing the results like you see in the course you see, everything we do is it's a library. So I'm guessing it's more like, do you think it turns out? Because I think being a team of two instead of just me working as a course also every as the road as to just something a little bigger and makes it projects a little bit more interesting. 

Sanyam Bhutani  4:54  
Yeah. 

Sylvain Gugger  4:55  
And it's yeah, it's been an honor and it's been very rewarding to be a part of that.

Sanyam Bhutani  5:01  
Okay, I don't think you mentioned this also, when I had the chance to meet you. I don't think both of you ever sleep is that true?

Sylvain Gugger  5:10  
We do sleep sometimes. Sometimes. I wake up in the middle of the night and I see Jeremy is pushing things. I'm like, why are you still awake? You're awake.

Sanyam Bhutani  5:25  
And then you end up doing more stuff.

Sylvain Gugger  5:28  
Uh. Yeah.

Sanyam Bhutani  5:30  
Can you tell us about a current day in your life? What tasks are you taking on right now? I believe the book is also this coming to the final draft and any other task that occupy your time right now?

Sylvain Gugger  5:44  
So the focus has been on the book until because we we developed a draft last Monday. So up until two days ago into working on the book at few hours a day and no we have like kind of relaxing for a week because it was a bit intense and so right this week it's more like focusing on every bug but I postponed because we have to finish the book and then we'll walk on the on the documentation of like fast core,fast.ai the fast.ai v2. So that's it so I'm starting to get ready for the for the new course. 

Sanyam Bhutani  6:23  
Okay. Is it is it yours zero I think you call it the zero inbox zero bug OCD kicking in where you into that the repo has zero;

Sylvain Gugger  6:33  
Yeah. Yeah, I'm always getting triggered the fire is like when 1 issue or 1 PR is standing so I try to address everything because I seeing 0 number. And since release, we keep address bug and issues more often. So the features are more discussed on the forum or like any kind of discussion happens on the forums, it's more like on GitHub, is pushing issues and a drive to solve that. As quickly as I can.

Sanyam Bhutani  7:02  
Okay, now could you tell us more about the book? What can we expect from it and will it be a substitute for the course or is it meant to complement the course in some way?

Sylvain Gugger  7:12  
It's more like a compliment for the courses like people like different things. I know I like videos on YouTube but some people prefer reading it a more of a nerd-y process I like like someone teaching is something like a real course. But I look no people, lots of people lots of people like to learn with with books so it's going to be like the material is not that different from the course varies a bit of new stuff that is mostly about deep learning from the foundation with the movies are usually done approach. We did a bit more of an ethics that are usually covered lectures for enough on our own one of the lessons so we let fleshed out all chapter we've see we've branches out, and I think it's a real good one. Above and that it's yeah it's likes of course but in a book.

Sanyam Bhutani  8:06  
Okay, I can you can you give us an insight how do you cover top down approach inside of a book because books are meant to be bottoms up falling the bottom of teaching methodology?

Sylvain Gugger  8:17  
No, you're not necessarily like his work is orchestrated with in 4 section I mean three, three major sections. The first section is more like what can DL do and for managers for instance, would like to know a little bit about what it can do without actually doing some deep learning just like be aware of what's the possibility is on what's the general and then the first part is Top-Down approach with like, we start with examples on how to train models. And we gradually while going through all the applications like scrap the surface a little bit and then so the first section is like, getting from the bottom up to the top again, with a great model. pytorch and how to write a training room to, to use as a middle of an API in fastai2 to get your data ready for a model and, and all that stuff.

Sanyam Bhutani  9:12  
Okay, so it sort of covers both the part one and part two of the course. I know the course is divided into two parts.

Sylvain Gugger  9:19  
Yeah, it's kind of twice with all of that. So we couldn't have everything that seems of the course, but we try to be as wide as possible because like we were already we're already reaching 500 pages and the publisher is starting to get angry at us because it was supposed to be a little bit shorter. And yeah, at some point, we're just adding stuff that he tries to yeah, you do a bit of both parts of the course. And it's is going to be it's going to be a great book, very, very complete like for for someone who really wants to learn about AI. It's going to be very comprehensive.

Sanyam Bhutani  10:00  
I've already pre ordered my copy, so I can't wait for it. But now coming back to the course, can you tell us what sort of efforts on your side going to the course for the six months that we don't get to see you being very active on the forums.

Sylvain Gugger  10:16  
Usually, it's mostly software development and research. Like to try to keep up with everything that happens in terms of research and also keep like improving the libraries to it's easier and easier to use a better teach nutrition was a course like it, we can target even beginners. I'd like people that know even less and less about coding and can tell- be able to train models and deploy them in production. So with fast.ai v2, it's going to be different, you're gonna be a little bit easier for lots of stuff and I'm very excited to see how it's going to go with the course. And yes, since um, and it's a book. Of course, since we're also looking at books, it's going to be we have tons of materials to show during the course, like we're going to base it on the similar books, we are going to remove all the pros, because I mean most of them pro since the goal is not to teach a book, but we're going to use kind of the same code and the same material.

Sanyam Bhutani  11:29  
Okay, can you can you give us a teaser of what can we expect from the course that's coming up? Any new things that you excited about? I know, two things that Jeremy mentioned, putting models into production, he said that might be covered. And many, many improvements to the libraries. What what other things might you be excited to?

Sylvain Gugger  11:51  
So yeah, putting a model into production and I think he's going to show how to do that only in Jupyter notebook. 

Sanyam Bhutani  11:57  
Okay.

Sylvain Gugger  11:57  
That's gonna be super fun. So like all writing all the app that's been deployed, and all the code is interpreted and book was, was exported the middle. So that's exciting, I think is we was probably going to talk a little bit about Nvidia, which has been a very exciting project as well. And that also given birth to fast templates like blogging while writing a everything in jupyter notebooks. So those two things are probably going to be covered a little bit. And woven, but yeah, all the new functionality from the library like are it's easier to write the optimizer it says you're to tweak the training loop or, or it's easier to get your data into into a model. And yeah, we try to also add some new debugging tools that people will like, and I'm going to make everyone's life a little bit easier, so.

Sanyam Bhutani  12:53  
Okay, awesome. Talking about research, what kind of research do you like? What questions do you ask while working on problems? And if you could give us an insight about your workflow of working on ideas with Jeremy?

Sylvain Gugger  13:09  
When you mean, do you mean when we went to a look for in an article? 

Sanyam Bhutani  13:14  
Yeah.

Sylvain Gugger  13:15  
Okay. So we usually she goes straight to wait for the results because like lots and lots of articles and increasingly felt published that I mean evaluated that being published at big conferences. We focus a lot on the math and then when you discover that there is nothing behind like, there is no actual improvement or results, and the greats a great example of that with a variant of Adam that was that one's the best paper award at NeurIPS not last year but the year before and like everyone was super excited was a paper it was immediately incorporated in every major deplaning library and like year after I don't see anyone using it, because when we use it with Jeremy, we notice that there was no improvement in training, and it's like, a lot of high point about, yeah, they had some math and they had like, found some kind of error in the proof of convergement and fixed it. But in practice, it didn't help at all. And yeah, well are often focusing on the math and they forget that we don't know. I mean, we don't have mathematical proof or knowledge of wise was neural networks work in general, so well. So while math can sometimes give a good impression, sometimes is that really useful. So we focus mostly on the table of results first, and then try to see if we can produce them and if there is an actual improvement in training by using that. So like when when paper that we were excited about last year was like, look ahead next to this new variant of optimizers. Where you you do five steps, and then you take one step back and it's it works super great with any optimizer we try to wrap inside. So that's kind of paper. We're really excited about like this. viscerally traumatic, magical results. But something that actually works in practice and can make life easier for people is interesting.

Sanyam Bhutani  15:09  
Okay, how do you distribute your workflow with Jeremy, for example, once you excited about two people and you're trying to incorporate into the framework, how does that pipeline look like?

Sylvain Gugger  15:19  
You mean when we add something or when you know, okay, usually we work pretty in parallel on different things. And we just we chatted on Skype and to just make sure that we're not working on the same thing at the same time because it's bound to create conflicts. So mostly we're working on different things. And then we're going to see what did each one of us wrote and like, try to operate in that and make it better. But mostly, it's it's been working well with just us, and like when one of us is excited about some things he writes them and then we share results.

Sanyam Bhutani  16:08  
What kind of effort go into making sure that all of the training can fit onto a regular consumer GPU what what sort of things? Do you have efforts that go into making sure of that?

Sylvain Gugger  16:22  
So the first one is that we try not to use gigantic models and focus on the one that is actually fit on colab. And use I mean, like anyone's GPU even with 8 GB of RAM. And that's kind of all target and make sure that sometimes maybe it's going to be at a very low batch size, but at least we can train the model on that. I mean, for most of the applications, you can have great results. I mean, as we saw with words, of course, we can have great results without worrying about using a huge model a lot of huge data set with billions of billions and GBs of data. So, I mean for tex, for instance, like even with the huge broad models, for tech classification ULMFit is still like pretty much state of the art. I mean, there was one article read get better results, but the difference is not that significant. And those are models that you can even pre-train on one consumer GPU if you don't, if you're not on English language, and then fine tune on your data set. So now let's have a quick statement is going on to research where I like, all the big companies are showing a bigger, they can make more moles. And it's true. It's true. There's better performance after what that is, you know, that's not what we're interested in. When our research I'm really interested in is everything links to sparsity and especially like printing models on in even kind of valve a new article about training proven model directly. So that wouldn't make like was German football could fit and GPU that way. And the problem is that it's a lot of you probably need a lot of custom code accounts because it's like a little special operations and they don't exist in in pytorch in major deep learning frameworks right now, so I'm excited about that when I have time to actually dig into it and maybe write some custom kernels. I hope we can bring taht to fast.ai.

Sanyam Bhutani  18:32  
Okay.

Sylvain Gugger  18:33  
That that's yeah, that's a trend and it's also going to be very useful to people's in predictions and like, we would be able to train off engineer middle on the even a mobile device, if it's, like, sufficiently prudent and that very heavy so it could fit on a tiny chip on your mpbile, for instance. 

Sanyam Bhutani  18:54  
Okay.

Sylvain Gugger  18:54  
That's, that's the kind of result I'm excited about.

Sanyam Bhutani  18:58  
Awesome. Now talking about the framework, can you tell us, what can we expect from the completely rewritten fast.ai 2 is the framework's name, fast.ai 2. And if you could tell what was the most challenging task in building the framework, if you had to pick one.

Sylvain Gugger  19:16  
So what's new in fast.ai v2 so for for beginner and for someone who just used like the top level API's of the library, nothing is going to appear very new because the top level is kind of the same as before, we did a little bit of renaming here and there, but the top level is looking quite the same as before. We just wre-wrote the DataBlock API to make it more more user friendly. So yeah, if people were just using that we're not going to find the library that changed, what's really new and what's really exciting is all the effort we put in the middle of API's because fast.ai V1  was written with like low level all relevant functions and when directly the high-level API's and there was nothing in between. So when the user wanted to customize anything, they basically had to go to the bottom. And they were telling us it was kind of hard to understand what was proposed for very different functions we were doing, and like rewrites, I really pleased with them. So we made sure like all the mid level will be the mid level API and make sure it's like, for for an experimental user very usable. And so that's the part I'm really excited about. So middle of API is going to be very useful for the people who have like very weird data sets, for instance, and it doesn't necessarily fit with the deadlock API. And they can directly that into that and wearable data and folding them for we've seen so far, which people were using the previous v2, it's been working very well for them. And all of those users told us yet so much better than v1 because it's way more usable. And the little parts like the generic optimizers is going to be very easy to write optimisers without really having to, to cookie and pass lots lots and lots of codes for from the official implementations, it's also going to be used. It's also going to be easier to write a trick of the training loop is a callback system. So it's the callback system is not that different from the one which has been revoked and just made more user friendly again, soit's going to be  lot better. My favorite part is, as a type dispatch system we implemented in Python, which was also the most challenging part and the all and the all, like all the data augmentations works and also transforming works, you can add a new behaviour for a new specificdata ype. So for instance, if you have like a 3D image or an image where you want, like free rotation to be applied in a custom way of as that exactly the same way, as a regular image, you can create your own type. And you can create your own implementation of the transforms, like and you can still call rotate from fast.ai, but you can write the implementation or voted for that specific type using all types of system. And the same for, like, if you remember, in day one, you have this method like show batch and show results, which show you like the results from your training or what would your data  looked like. And those methods also written with the the type dispatch system, if you have your new types, you can customize the behavior with your new types, very easy. And so we made that but I mean, in in v1, it was really hard for people with to to write custom behavior when we wanted to inject custom material for was it was kind of hard for them. So we made it easier with a dispatch system.

Sanyam Bhutani  22:48  
Okay, awesome. 

Sylvain Gugger  22:48  
Which I hope people will like. I hope.

Sanyam Bhutani  22:52  
 I'm really excited about it. But the other side to it, how can the contributors from fast.ai best help in the development of the framework. Do you think it's it's ready beyond the fact that we can't contribute? Or do you have any aspects that we could possibly contribute to right now?

Sylvain Gugger  23:09  
For v2, I mean, contributions right now is more like, it's not trying to contribute by writing the software and contributions we work on right now is like using it and trying to, you know, bottom could you have we had working on v1 and see if it's ended up being easier and cleaner in v2, and if not telling us and telling us what the issue was finding bugs because, of course, lots of bugs and ways to fix them. So I mean, follows the most valuable feedback we can get. It's not as like a PR to implement that function was not there yet. It's more like, "I used it. This was working great, but this was not working so great." And it's really important for us to know to know that before we can kind of v2 and make that official release. Probably going to be I mean v2 is probably not going to be official. should be released until the MOOC is done. Because we're going to use all the all the beginners that next course as like targets and see what kind of difficulties or kind of bugs behaviour occur. And when we fixed everything and make sure like they find v2 working nicely, we're going to make an official release. It's probably not going to be before June or July, but v2 is officially released and in the fast.ai repo.

Sanyam Bhutani  24:27  
Okay. Now coming to another aspect of fast.ai that I think many people forget is the forums which are also at the heart of the course. You are our official teacher much more than Jeremy, you are very active on forums, you almost answer every single question with the courses life, what questions we really enjoy answering. Is there a favorite category that we can ask you in the, for the future students?

Sylvain Gugger  24:52  
What questions do I really like? First of all, I think Jeremy, is almost as active. I mean, he's probably more active than I am. I mean, I focus mostly on the I've only focused on the course from during the course to be honest I don't follow is of course from that much once of course is over. And then focus more on the fastai user forum like when people are having difficulties using the library and I try to help them with blogs and and so to answer your question, what kind of questions do I like? More like what kind of questions do I not like? I don't like questions that are more, a lot of the users come on the forum and are like yeah, I try to do this very vague thing and it didn't work and we leave it at that and like yeah, if you want us to have maybe you're going to have to tell us a little bit more about what you tried and what didn't work. So the questions I like are people coming with I try to do this and they add these are on where the show all the good, the bad and all these stack error trace message, I look at that, and I can find pretty easily when the bug was if it was in your code or if it was in fast.ai and, like fix it if it was in fast.ai, but I'm proof so it what kind of questions I like are all of the detailed question with all the information about what was tried and it was written and the return which was given by by Jupyter or Python.

Sanyam Bhutani  26:24  
Okay. For for the course material would basically is to have for the students who are aproaching the material, but how should they spend their coding time as Jeremy calls it?

Sylvain Gugger  26:38  
Like he said, like, the most useful thing is always trying to redo what's done in the Jupyter notebooks. And on the same data set, but more excitingly on the new that does that make something that's not necessarily assembled the same way. Maybe you Kaggle, but looks kind of like what's done in the course but it's slightly different. Because it's always when you we struggle. I mean, you learn a lot when you struggle to get your data ready for your model. And I mean, a lot of fast.ai data is about making that as easy as possible. So you will learn a lot about fast.ai when you try to actually put your data into the model when it's not, like nice or imagenet or like but new datasetset where everything is, so yeah, trying to redo it in the notebooks. If you are living in San Francisco, going to the study group is probably very, very valuable. And working with like R students, and if you're not forming a studying group, no, like studying group, right where you have idk slack or Skype or zoom, where you can actually chat during the day and, and during your cutting time so that you can, you know, interact and show Yeah, that kind of difficulty and someone can help you and because it's good to go on the forum, but it's also good to try to help people that have kind of the same level and In terms of coding, and like, try to understand what viral message was. And if you can figure out what was wrong, we work and we also learn that this way. And yet, then use the forum to either show what you've done or if you if you have difficulties. First, like I said, with all the good and all that organization, and what you try to make it poor, but didn't work and and then try to help other people. So from that, the most important thing is actually try to good by yourself, just never just things in his book and press Shift Enter repeat. Because that's, that's the way you will know. I mean, you won't find a new single metric.

Sanyam Bhutani  28:44  
Yeah, for sure. Once we get enough proficiency, Jeremy recommends students to build projects. Do you have any suggestions or advice for that? How should we go about building project or finding those ideas?

Sylvain Gugger  28:57  
And, I mean, the first thing is you should pick them in once repeat something they lack something we're really excited about something they are their own expertise about if it's a person that's not a software engineer, they have a report he has some expertise or some kind of data for instance like medical data said I don't know what realistic data set because like we new exciting things often come when people bring their own expertise into deep learning. So yeah, finding the project you're passionate about and if you have no Id like look at Kaggle competitions, it's good and past and present. It's a good idea to me it's it's good to trigger some ideas and maybe like twice twice a day and one of them and interacts with the community for that, yeah, your your project would come from something but filter out because it's going to be easier to work on your, since it's never going to work on the first try, you're gonna have to try on the second try, right? You will have to persuade maybe a dozen time before it actually works. It's, it's, I mean, you really need something you're really interested in and very passionate about, because you're gonna have to be very, very, I just say that after try and try again. So can be sure it's something you want to try and try again. And, like 1000 times. So a good example is like Jason with the DeOldify, I mean, it took him months before the first model works. And it was just very persistence and try it again and again and again, because it was something he was very passionate about. So yeah, that's really the biggest thing is big, some projects that you're really interested in, otherwise you're going to drop it after. If it fails like five or six times then is going to be, it's not going to work. 

Sanyam Bhutani  30:55  
Yeah, and now now, DeOldify is also launched as a business for him. If you persist enough, it can also turn into business your side. 

Sylvain Gugger  31:04  
Exactly. 

Sanyam Bhutani  31:07  
Now, if if you were to give a final best advice to the future fas.aiy students in person online or via the MOOC, what would that be?

Sylvain Gugger  31:17  
So like I said, like I just said, Be persistent. Because deep learning is hard. And especially like the burning cuisine?, when you try to train a model is very, very hard. So be persistent. Actually, code like, don't just read all photos, videos, but do some coding Jupyter notebooks. And if you're not a big coder, it's going to be hard at the beginning. But, again, be persistent, because that's the that's the only way you're going to learn anything. It's just you're not going to become that deep learning practitioner just by switching the videos, you actually need to get strong and and do things to for that to happen. So spend as much as you as much time as coding and like try pick-pick a project you like or good Kaggle competitions and like try every day to make it a little bit better. Because that's small incremental steps are I mean are key to building something great at the end of the day.

Sanyam Bhutani  32:19  
Okay. I also have a request to the students please be as respectful of Jeremy and Sylvain's time on the forums. Please do your homework before asking the question and like you said, please post the questions in a proper format so that they can really help you.

Sylvain Gugger  32:33  
Yeah, that would be great.

Sanyam Bhutani  32:37  
Before we end the interview, I also have a fan request for you. Would you ever teach one lecture maybe more than one lecture in the future? Next one or the future was another course.

Sylvain Gugger  32:50  
I don't know about that term Jeremy is a way better teacher than me and myself, especially with English me getting into it. So I'll probably leave Jeremy. He's teaching me and better answering questions on the forums, all developing  notebooks, and then I'll writing blog post and I'm not sure I'm going to actually deliver lectures.

Sanyam Bhutani  33:11  
Okay, but please do consider it as my request. And Sylvain, thank you so much for all of your contributions to fastai and the machine learning community. And thank you so much for joining me on the podcast.

Sylvain Gugger  33:23  
 Thanks. Thanks for having me. It was a pleasure chatting with you.

Sanyam Bhutani  33:33  
Thank you so much for listening to this episode. If you enjoyed the show, please be sure to give it a review, or feel free to shoot me a message you can find all of the social media links in the description. If you like the show, please subscribe and tune in each week to "Chai Time Data Science".

