Sanyam Bhutani  0:13  
Hey, this is Sanyam Bhutani and you're listening to "Chai Time Data Science", a podcast for data science enthusiasts, where I interview practitioners, researchers, and Kagglers about their journey, experience, and talk all things about data science.

Sanyam Bhutani  0:45  
Hello, and welcome to another episode of the data and data science show. In this episode, I interviewed Dr. Pablo Samuel Castro, who's currently a staff research software developer at Google and is working with the Google brain team in Montreal with a focus on reinforcement learning and machine learning applied to music and creativity. We of course talk all about these themes in this interview reinforcement learning and using AI in creative domains. Pablo is a musician himself. So we talked about and we definitely discuss his overview on using AI to replace or enhance human creativity in domains outside of technical focus. We also discuss how does Pablo approach research problems his take on research and and his pipeline, or approach to working on new problems? This interview definitely has a lot of interesting pieces of advice around approaching research problems. So without further ado, here's my interview with Pablo Samuel Castro. With a quick reminder to the audience, please subscribe to my newsletter if you'd like to stay updated with the interview releases and if you're a non native English speaker, please remember to go to YouTube and enable the subtitles for a better watching experience. The subtitles have been manually checked in the upload so I hope that improves your watching experience. With that, here's the interview. Please enjoy the show.

Sanyam Bhutani  2:21  
Hi, everyone. I'm really excited to have a person from the Google brain team, Dr. Pablo Castro, thank you so much for joining me on the podcast.

PSC  2:30  
Thank you for having me. It's a pleasure.

Sanyam Bhutani  2:32  
Likewise. Now I want to start by talking about your journey how you got interested in machine learning before we talk all about your music research and what can you tell us at what point did you find your passion for machine learning?

PSC  2:43  
So in undergrad, so during a pre cup who became my PhD supervisor, she joined McGill as faculty when I was doing my undergrad. And I think I took the AI course with her, I think it was the first time she was teaching it at McGill. And I didn't really know what I was before then, but I fell in love with that course. And we had to do like some class. The project was some 3d Tic Tac Toe or something. The project itself, I didn't do very well in the competition. But I just was really interested in this idea of having computer programs learn and adapt themselves to to improve. So this was my last year of undergrad. So for my last semester, I was an honor so I had to do an honors thesis, and I asked her if I could do it with her. And so I ended up doing, building this neural network to try to improvise in jazz to try to build a program to try to learn how to improvise and jazz. I've always been interested in music. And so it worked really poorly. It didn't really worked at all. Mostly because so the way I had set it up is I didn't really want to encode rules for what constitutes good melodic lines or not. So here was being naive, learning about how you train these things. And so I set up the system where I had to listen to melodies that generated and then give it a score of thumbs up or thumbs down. And , so obviously, like, there's only so many of these melodies you can listen to. And I mean, occasionally you get certain melodies. That sounded pretty good, but it was probably mostly random chats. But there was just a really informative experience because it showed me how difficult it is to train these things and like the hyper parameters of hyper parameters, choice of network topology and all that, and it's important, but so I'm originally from Ecuador. And so I came here on a student visa and I think this I don't know if Canada still has it, but I think they do. They had this program where if you got a job in the field that you came to study, then you could you got a temporary work visa for a year. Okay, so I didn't want to have to leave Canada so I left school and it went, I mean, once I finished my undergrad I went to work for is a company that builds flight simulators like real physical flight simulators for pilots. And those are cool job, worked there for four years traveled all over the world. Because we worked on my job was to port code from computers built in the 80s. So these things that are the size of walls into like small Linux boxes, so it's big porting from Fortran to C, or sometimes you're going assembly. And so I did that for four years, but then it started getting an itch to go back to school and learn more. And when they randomly ran into join on the street, and I told her you're thinking of switching jobs or doing something different. She said, oh, come do masters with me. So I did. And that's then I started my masters and continued on to PhD with both during our pre cup and percussion?

Sanyam Bhutani  5:37  
Did you sometimes take the flight simulator for spin during sometimes in the evening. Would you take it on a spin? Maybe do;

PSC  5:51  
Yeah, yeah. Because because I was working on these really old systems. There was no multithreading. So when you had to, you did have to compile databases and things like that. And sometimes it would take three hours. So when you listen, there's nothing you can do on the simulator at that point, you just have to wait. And so we worked at night, because we wanted to use the simulators when nobody was training on them. So when it was compiling, I'd go and fly some other simulators and sort of learn how to fly planes and the most difficult one and also the funnest one was a helicopter that was really hard.

Sanyam Bhutani  6:23  
I can imagine.

PSC  6:25  
And I could sort of take off and land in the heliport in Manhattan. But I couldn't do that anymore. This was many years ago.

Sanyam Bhutani  6:32  
Are you confident that you can do it in the real world?

PSC  6:36  
No, no, that's what I'm saying. Like I remember, I have a rough idea for how you would do it but I would not even try.

Sanyam Bhutani  6:44  
Okay. Now, coming back to your journey. Why did you decide to take a PhD and I believe you specialize in reinforcement learning. Usually people have a salty pick about it. What are your thoughts about it because people usually believe that it's not ready for the real world, would would love to hear your thoughts about it.

PSC  7:01  
Yeah. So I mean, when I started my Masters, I guess it picked my supervisors over my topic. So at that at this time, when I started my Masters, it was 2005. So machine learning still wasn't that popular. How many people really did this stuff? And so I read when I was my job, part of the reason I got interested in it is because I started reading the wrestling norvig AI introduction. 

Sanyam Bhutani  7:28  
Yeah. 

PSC  7:28  
And I actually was really interested in evolutionary algorithms, but I just found super fascinating. And so I went to see percussion, and I had taken undergrad classes with both of them. And percussion is a very math heavy guy. And he was like, oh, no, I'm not doing any evolutionary things. Because you can't prove things with them. It's really hard to prove it. So, but I wanted to work with them. So I said, okay, I won't do that. And so I ended up picking what they were interested in, which was metrics. Stay tuned. metrics and markup decision processes for reinforcement learning. And I got really fascinated by that, because it has a lot of really cool math. And I learned a ton there. And so that's kind of why I ended up studying that stuff. So I didn't necessarily I picked it more because I found the math interesting. At that point, again, this was before the deep revolution. And so;

Sanyam Bhutani  8:27  
Yeah.

PSC  8:28  
People weren't really thinking about how are we going to apply it in the real world. There was still like an academic interest and sort of working towards hopefully one day we can use this and here's all the theory that we've built up so far with which is what's happening today, like a lot of the theory that was built before hand is being used nowadays with the networks and love the old ideas are being are being resuscitated now with with deep networks. But when I graduated from my PhD, I did have a hard time finding a job. I wanted to be next academia and that wasn't able to find anything, especially reinforcement learning, like not many people were interested at the time. And so I left academia for a while for five or six years because I joined Google but as a software engineer applied software engineering in machine learning, but applied and remember, Andrew McCallum was a profit at UMass came to visit my office in Pittsburgh once and I met with him. And I told him I do RL and a quote that he said that still stuck with stuck with me until now is reinforcement learning is a solution in search of a problem. And I think at that time, it was it was still the case. Nowadays, I think it's less true, that statement is less true. It's just the we're seeing a lot more evidence that it can actually solve very, very challenging tasks. The key is being able to pick the right tasks that you can tackle with reinforcement learning. So for instance, something where safety isn't super critical. And I think this is in general for neural nets, because you don't have this level of interpretability yet with with the algorithms that you end up training or the models that you end up training these deep networks. But I do think they're, the time is right to start putting these things into practice. And there are a lot of efforts and industry outside of academia that are working towards this. Some of it is not necessarily like user facing product, but maybe more as an analysis tool for understanding dynamical behaviors and things like that. So for instance, um, and I guess consulting, or helping the Bank of Canada, which is the Federal Bank here, they're working on this project to use reinforcement learning to understand the dynamics in the high value payment system. So these are the payments that happened between large banks. So that's a lot of money and it's great related by the by the Bank of Canada. And so they are interested in seeing if reinforcement learning can potentially be useful for understanding these dynamics and provide more realistic simulations to sort of help regulate things more. So this is something where it's not RL isn't in the wild in the sense that people are interfacing with it directly, but it's still being used in practice. For something that does affect people.

Sanyam Bhutani  11:26  
Do you agree to the viewpoint that I believe, for instance, open AI is falling that you can create our own system and then follow it to generalize to bigger issues such as the open AI, open AI 5, for example.

PSC  11:42  
Like that, that that will eventually generalize to others? 

Sanyam Bhutani  11:45  
Yeah. Yeah. 

PSC  11:48  
I'm very skeptical of that. I think there's, I mean, if you look at that, if you look at alpha star, there's a lot of inductive biases that get put into the architectures that they use and this is choices they make, which is good. I mean, you get you do get a lot of ideas from that, but it's still very specific to, to the tasks that they're trying to solve. So for instance, I'm not as familiar with open AI, because I don't think they really released a whole lot of details on when they get there. But for instance, alpha star, I saw a real give a talk about this, and the architecture they used is highly tuned to the Starcraft problem. It's a very complex problem, and they've gotten a lot of really good science out of it, but the architecture they end up using is very specific to that to that task. So, I mean, these are all advances and but will that alpha star then generalize to other problems? I don't think I guess one step towards that direction is with this new zero, from DeepMind, where they could before like AlphaGo and alpha zero, they all still encoded the rules of the game into the training process. But okay, Zero I think is it has to learn rules from scratch so that I find more of a general purpose model. But again, it's still there's still some implicit biases in there in the sense that you're still assuming you have access to simulator, which is a game that for the most part, it's fully observable, your system, which is not always the case in real world systems. So there's a lot of early special in this in the sense that deep networks are like the other craze nowadays. But deep networks are just a part of our there's all sorts of other things and important design traces and algorithmic choices that surround these deep networks that make or break our algorithms and that some of these are very sensitive to the environment or the tasks that you're running them on.

Sanyam Bhutani  13:47  
Make sense.  Now shifting gears towards your journey at Google, I believe you started applying machine learning as a software engineer, then you transition back to research. Can you tell us more about your journey and the transitions.

PSC  14:03  
So I joined Google as a software engineer in ADS. So I was doing machine learning and ads, specifically building models to predict click through rates. So those text ads that you see, when you do a Google search, and you see some text ads show up. And those are going through an auction. So advertisers place bids, they, they create their, their advertisements, and there's this gigantic auction that happens that uses a whole bunch of signals. One of the most important signals is the prediction of whether the user will click on it or not. And that prediction is coming from a whole bunch of other signals, like what the search the user searching for the geography, that those types of things. So that was a really cool job. I really enjoyed it. It was fully applied because it was really building these models that would can then get deployed as part of this auction. So I really enjoyed that. But this was in Pittsburgh. I was there for about four years doing this, but then we wanted to move back to Canada. My wife is from from here from Canada. So we have family and close friends here. And so I transferred to the Montreal office. And it was funny because the year before we transferred, I came to speak to the site chief here in Montreal, and I said, I'm doing machine learning in Pittsburgh. And I'd like to continue doing machine learning here. And he's like, oh, there's almost no machine learning. There's no machine learning Montreal and Google at that time, there was no machine learning. 

Sanyam Bhutani  15:26  
Okay.

PSC  15:27  
This was just like, four years ago. And so two years after that, we had the brain team, we had deep mines, I was pretty funny the how quickly things changed. So I transferred to Montreal. And I was lucky that Chrome had started this team that was going to start trying to do machine learning for Chrome and I joined that team. 

Sanyam Bhutani  15:47  
Okay.

PSC  15:47  
There was mostly building back end infrastructure, so to be able to serve these models quickly and, and it was for autocompletion and suggestions and things like that. So I worked on that for a year and then Rain opened up. And I was lucky enough to be so I wrote to, Hugo Larochelle, who is the manager at the time, and I was lucky enough to be invited to, to join.

Sanyam Bhutani  16:14  
Awesome. Now, can you tell us what does a day in your life currently look like? What tasks are you working on and what research ideas are currently of interest to you?

PSC  16:25  
So my days are very varied. So, my office is in Montreal, which is where I am right now, but I actually live in Ottawa, which is two hours away. So I work here in the office about three days a week. So that means getting into the office. And so right now that it's winter that that means a cross country ski from my house to the train station. So that's like a 45 minute thing. And then I take the train for two hours and then again to the office. I have some hours here. Then the same in reverse, so two hours back, and then 45 minutes cross country ski. So that is one day, sometimes I work from home, and then it's just like, I wake up early and do work. And then I pick up my kids from school, and then my work is lower. But more specifically, what I work on a day to day, again, highly varied. So I have a lot, I like to have a lot of projects happening at once. And they move at variable rates. So I do a lot of work in reinforcement learning. But I also do a lot of work with creativity, like machine learning and creativity. And so I have projects, like multiple projects in these both both these fields. And typically I prioritize, so for instance, icml deadline just passed. So yeah, we had a paper we were working on two papers that we were working on for that and so that took priority. So I put everything else aside and just worked on that. Now that that's done, these projects that I that I have left, at one side, I'm going to pick them up again. And then also, like if I'm working on one project, sometimes like these experiments take hours or sometimes days to run. And if everything else is ready for that, when you kick off the experiment, then like, I don't want to sit around twiddling my thumbs. So then I switch to another project, like second priority and work on that for hours or days. So what am I specifically working on and reinforcement learning? I'm actually working again on what I did in my PhD. So looking at these states similarity metrics to get better. So I just presented a paper at Tripoli I where I developed this this, this method for approximating these metrics with neural networks. So these metrics to have some nice theoretical guarantees, but they're very expensive to compute. And so they've been I mean, when I was working on my PhD was mostly restricted to grid worlds and small environments like that. But what I was able to show with these networks is that we can approximate them pretty well with, with the neural networks and it allows us to to extend them to very large state spaces like Atari or even continuously spaces. 

Sanyam Bhutani  19:16  
Yeah. 

PSC  19:17  
And so, so I have, that's one of my main projects right now is looking at these at these metrics and how we can extend them and how we can use them in practice and reinforcement learning, because up to now it's they've been mostly used either as like theoretical reference. So because they have these nice theoretical guarantees, if you can relate your algorithm to these metrics, then you can might be able to get some theoretical guarantees, or the algorithms that use these metrics. I have some papers with that from a long time ago. They're mostly restricted to Toy worlds/domains, and they wouldn't really work in the large environments that we we work on nowadays. So that's one thing I'm looking at and it relates to another thing, that sort of larger theme that I'm very interested in right now. And that's representation learning for reinforcement learning or how we can try to learn representations that that are useful for reinforcement learning in a way that goes beyond the single test so generalizable or transferable representations. 

Sanyam Bhutani  20:20  
Okay. Now talking about creative AI, are you trying to automate yourself because you have a passion for music as will put us are you working on in that direction? I know you put out very interesting intersection of I think it's generating networks and music, can you tell us more?

PSC  20:39  
So, definitely not trying to automate. I've been myself playing music for longer than I've been coding and one of the activities I've been doing the longest in my life. And so I do not want to stop doing that. My interest in this space is really to develop to the next generation of tools to enhance or augment musicians or artists. And in particular, I'm really interested in how these tools can be used by, quote unquote expert musicians. So I've been playing piano for, like, over 35 years. And it's, it's I know how to navigate uncomfortable space as well. And piano, I improvise and so I'm fine being thrown in, like difficult chord changes or things like that because I practiced enough. But typically, the way you're thrown into difficult situations are somewhat predictable in the sense like that you'll get a hard time signatures are hard harmonic changes, but you do enough of them that you, you sort of start getting getting used to it. And you can sort of know how to navigate those spaces in a way that that makes it sound well.

Sanyam Bhutani  21:57  
You get some intuition over time.

PSC  21:59  
Yeah. So, one thing I like doing so one of my paper I had last year ACCC is trying to use generative melodic models as part of the improvisation process. So the model of the gist of it is that the model essentially hijacks your notes and replaces them with with its own notes. So you no longer you still control the rhythm, but you no longer have control over the the melody that's being produced, which really drastically changes how you approach improvisation. And so I use that live with my trio and it worked. Okay, I mean, you couldn't really tell that it was it didn't sound terrible, I guess let's put it sounded okay. But it was it was really challenging. And it was really difficult to to improvise with this, but in an in an exciting way, because it made me think about improvisation in a very different way. And so that's what really gets me excited about this stuff is can we make artists that already know how to do do things really well and have their zone there? They know how to nail it in the zone, can we get them into a different zone where they can still do a good job, but now they're thinking of the problem in a very different way in a very different space. And ideally, this produces new art that may not have been possible without this push from from these intelligent agents. And so that's that's where my interest in this comes from more as as an extra tool.

Sanyam Bhutani  23:27  
Do you think like, usually, ML models are thought of as black boxes, even though we have interpretability tricks now coming up, but do you think your real world experience or your intuition is helpful for you to debunk the models or to get an intuition of where things are going out of balance?

PSC  23:45  
Sometimes, I think also, your human biases can can hurt and I think this is a general problem with with a lot of machine learning that there's a lot of anticipation revising that happens. And it sometimes it bugs me quite a lot. When they talk about a model, the model thinks that red and blue or colors or the model found this analogy or something like that. Or the model is confused by his analogy or something like that, you know, it's these are all human sentiments, but that we're projecting onto these machines that are really just doing numerical optimization over over a data set. And so I think that it's just a very different way of approaching problems. So when you have the stream model, the way it approaches a problem is very different than the way we approach a problem. And so, I mean, this happened to me when I was first getting because it during my PhD, I didn't touch that neural networks at all. It's only when I joined brain that I had to catch up on all these neural networks. And so at the beginning, that was one of the things where I was, I'd see things like anecdotal evidence of something the model was doing. And I'd attribute what I would expect a human to do to handle these types of problems. And so I searched in that direction, but then it ended up being something very, very different that that ended up resolving the problem, because these models are just handling these these problems or challenges in a very different way than we would as human. So human intuition is obviously necessary and important, and we can get you very far. But I think attributing too much of your intuition to what to try to understand what the model is doing can lead you astray and can often make you tend to anthropomorphize these models more than more than you should. So like an example of this which which bugged me a bit, opening I had this blog post recently where they they had these multi agents, they had like a capture the flag game going on. And they were seeing that the agents and jump over the thing over the walls. And so they figured out a trick or fine whatever. It's really just they they found a set of actions that policy that that led them to the rewards. They're not thinking in this way that oh, like the fact that they're jumping over a wall is interesting to us because we don't normally jump over walls.

Sanyam Bhutani  26:39  
Yeah.

PSC  26:40  
But from the agents perspective, it's just a policy. It's one of the action choices or a set of actions choices, that ended up leading to the high reward, which is what they're optimizing for. So this anthropomorphizing of the training process or the resulting clinical behaviors is is something that I try to avoid.

Sanyam Bhutani  27:01  
I think it's also because of the marketing aspect to it. Not many people are fortunate enough to understand that it's just math running in the background and they feel okay. This is an AI trying to do things. So I maybe it's for that, but it exists and;

PSC  27:15  
That's for sure there's a PR element to it. And that's where they've gotten a lot of criticism for for some of the releases, and I get like you their startup, they need funding. I get that but I do worry a bit when I see a lot of these, these press releases that it's it's overhyping, these things. So then you have media outlets that yes, they don't understand the that it's all just math behind the scenes, not super interesting relative to what we were already doing 20 years ago. But then they they have these amazing videos, the videos, the video production is incredible of that particular platform. I was really impressed. So they pick up on this and so they report it as a learning how to break into houses? I don't know, I don't think I was actually, you know, that's the type of thing that these click baby headlines that media likes to pick up. And this leads to this, this hype that AI is people think AI is at a state where it's not in, it's not yet at the state where a lot of media portrays it to be. And I worry just because this, I feel inevitably inflates a bubble that I do feel we're living in right now. And when and if this bubble bursts, I i a lot of the smaller companies are going to be the ones that are most affected. So it's it's tricky, because at the one hand, I understand you want to raise funds and you want to get people excited about your research and what you're doing and make the research approachable to people not not familiarized in the field, but also on the other hand, you also don't want to play that card too too much because I think it'd be and be hurtful.

Sanyam Bhutani  27:25  
The truth lies somewhere in between. Now coming back to your research, how is is challenging can can one not just convert music into nodes fatal language model on top of it and and they good what's the difficult challenges?

PSC  29:20  
So there's a paper by some colleagues called music transformer that was published in iclear last year. And it's taking a transformer model which was from the attention is all you need paper that was originally designed for language and applying it to music. They had to change a few things in terms of how how the data is represented, but but at its core, it's a transformer model, and it produces amazing outputs. So recently, the my colleagues released this listen to transformer site where you can go and listen to samples from this and it's really cool. It's really exciting stuff. So that is doable, but there are still limitations. For instance, that the length of these pieces is often limited by by the computational capabilities. So it's not like an LSTM where you can just kind of let it run for as long as you want. The LSTMs, we tend to sort of diverged into nonsensical territory. This is a lot more coherent, but but I think it does become difficult to have something that that's much longer scale. The other thing is influenced time. So they, they're still expensive to to sort of generate these these pieces. And you have to have typically access to special specialized hardware, like GPUs or something like that. And so I'm interested one of my interest in creativity is in the performance setting. So can we use these models in a live performance setting and that's a whole different ballparking. So this inference can become really problematic. And so but then another problem with that is the models like music transformer, yes produces really interesting melodies and outputs. But it's a single model that then everybody uses. So I have to sort of curate to find something that appeals to me, artistically. So having something that's more personalized, is a challenging problem, because not everybody's going to be able to find their own data set and train their own model. And typically, if it's if you have very niche taste, and the data set is going to be very small. So there is some work already in by by a few people, including myself and some other colleagues and in trying to develop robust, robust ways of personalizing these generative models to adapt to specific users tastes without necessarily having to retrain a full generative model. So there's there's still a lot of, even though we can already train general models to produce really compelling music, there's still a lot more work to be done before it can become a core part of artistic creation. 

Sanyam Bhutani  32:12  
Also, because music is very subjective, some music that I might enjoy, you might totally hate and vice versa.

PSC  32:19  
Absolutely, absolutely yet. And this is where the personalization comes in. So the datasets that they train on are typically Western music, Western pop or rock, which I'm more of a jazz person. So some of it appeals to me, so I'm not as much or actually the music transform, I think was trained on on classical mostly so it's mostly classical piano, which I really like but look, some people might not like as much, but also if you like if you're from from Asia, you might have different style of music that you want. You want to explore and not just Western music, or from South America. There's there's different even though I think the overlap is a bit greater there. But they're still like traditional folk music from from South America that probably won't see that reflected in that data set. So there's a bunch of these issues of low data regime regimes that we still don't quite know how to handle properly yet.

Sanyam Bhutani  33:16  
One thing I'd also love to know is about the data set curation. I know for a fact that we cannot just go and scrape the top US top charts because you have licenses involved, what sorts of data sets are available? And talking about frameworks? What frameworks do you use? Are you Pytorch person or a TensorFlow person?

PSC  33:36  
Well, I'm a TensorFlow person, not surprisingly, I mean, that's not I've done a little bit of Pytorch and I've one of them coding in myself. I've been a mentor for summer schools and things like that. And often they're they're written in Pytorch that the training material so I I understand Pytorch, I don't code in it. And I started TensorFlow I mean, here at Google, obviously, we a lot of us use TensorFlow and not everybody uses TensorFlow. But that's where I started when I started looking into neural nets. And so yeah, for me there, there's less friction now to stick with TensorFlow over Pytorch. But I know a lot of people are have very strong feelings about this. For me, it's not as higher producing as for some other people, they have just gotten used to it. I don't know. In terms of data sets, that's always a challenge. I think there's a lot of open data sets out there really depends on what you're trying to do. So Magento released this data set, I think last year as well that I clear, which is this piano, Yamaha Piano Competition, so it's for choice, piano players playing on a on a piano on the table to record the performance. Both digitally and the raw audio. And that's called the maestro data set. And so that's what they use to train the music transformer on. And so that data is it's out there. But yeah, there's there's other things. So for instance, one of my projects is with lyrics and models, I can assist you in writing English lyrics. And so that we've been using just in public data sets that are out there. There are like, for instance, the top 40 hits over the past six decades. It's there is that that's public, it's open. So we've been using that. And fiction books dataset. librispeech, I think it's called, that has a whole bunch of books that are that are not bound by copyright. So third, challenges difficulties with each of these data sets. You have to do some curation and some cleaning of the data and normalizing so that all the data is in the right format for training these types of things. This is sort of a dirty work in machine learning. There's a lot of this that has to happen. Before you can even start trying to train. And even as you train you, you might find issues with your data set that in the lyrics one, for instance, we're still correcting issues in our data set as we as we move forward. So yeah, it's just, I don't know, kind of fine with what's out there. And building your own data set is hard. It's really time consuming. And I really appreciate when people do that, because it's extremely valuable for the rest of the community. But it does take a lot of time. And it's potentially less rewarding than training the model and having it produce things that you want to produce. Yeah.

Sanyam Bhutani  36:45  
Does your research also trickle into the magenta project? Does your team talk to the magenta team as well?

PSC  36:51  
So I'm an unofficial number or official stash on official number of magenta, so I don't I don't report through the magenta reporting chain, but I know all of them. We chat pretty regularly. I sometimes sit in under the research meetings and we have started. We have a few collaborations but it hasn't been as there haven't been as many as a would like, in part because they sit in California and we sit here so the physical distance and the timezone difference does cause some friction, just more difficult to coordinate meetings. And and if you're talking about math, like using a whiteboard is so much easier than doing it over over video. So just things like that. But but we are talking about new projects. That would ideally be some collaboration between some people here in Montreal and some of them in California. But yeah, I mean, I give when I give talks a both representing Google but also magenta because a lot of the stuff I talk about is the word that comes out of magenta.

Sanyam Bhutani  37:57  
Awesome. Now coming back to the search engine, I'd love to know what does the research pipeline for you look like? Many people aren't sure. What is the life in the researchers they look like? And how do you approach new problems? You said you work on multiple problems in parallel, let's say, you find it interesting idea that you want to work upon what are your steps and you getting started on a newer idea.

PSC  38:20  
So I have a lot of ideas. I mean, like most people, some are more difficult to get started with, in the sense that maybe you need a special data set that you don't have right now. So the first step would be build a data set. And that immediately would for me be reducing the priority of that project, because that's something I'm not super interested in doing right now. But keep it in the back of my head because maybe a data set comes up. That's exactly what I needed to go wait. Now I can actually tackle this problem buty eah, some ideas come from reading books that often are not machine learning related books. And sometimes the ideas that end up happening are not necessarily machine learning related. So I had this website I made that was exploring evolutionary algorithms, but not with the sole purpose of surviving but there was a an element of aesthetics embedded into it. So there were these these little things that kind of look like birds, and they had colors. And so the female is preferred really colorful, male birds, but male birds, the more colorful they were, the shorter their lifespan, so it took them longer to find a mate and they might just die. And so this dynamic playing with that was pretty interesting. And it ended up producing this thing that I finally found it was quite artistic in the sense that it was very aesthetically pleasing and it was a dynamic and movement you have these things moving around in different colors. And it was it was really nice to look at. That idea came from reading this book on. It's called the evolution of beauty by Richard from so it has nothing to do with machine learning, but that sparked an idea. And I like those types of ideas because when I think of them, it sometimes they can already imagine how I would write code for it to reproduce the idea in a very simple scenario. And that's the type of problem I like to start with where I can immediately start running some simple experiments to get a sense for what's happening. And the same goes for reinforcement learning so reinforcement learning, it tends to be a ideas tend to come more from as offshoots of problems I'm currently working on. So this was representation learning. This, actually can't remember exactly where it came about, but it was more as a consequence of another problem but another we were thinking of I think it was the yeah, we had a paper in Tripoli I last year and trying to understand distribution and reinforcement learning. And I think from there, I started thinking about representations more. But in those problems, also, I like to start with grid world are two problems before going into the deep setting. Because I find it's one, it's a lot faster to run experiments. The visualizations are a lot cleaner in the sense that you can compute the CD optimal value function exactly. And then you can directly compare against that and you can generate visualizations and animations very easily, that allow you to really understand what the algorithm before you start throwing neural networks into it and doing it once that have that kind of as a proof of concept and I start trying to tackle the neural network with for larger problems, but I like to have this this proof of concept or toy world reference point that I can point to and often like, I like things that have theory to them. So often when I'm starting a project like that try to come up with with some theoretical basis or theoretical motivation or justification for what I'm trying to do, and then build the algorithm on top of that. So the algorithm isn't just built on some creative idea. I mean, there are a lot of cool algorithms that come from that. But I like I really like algorithms that come from, like a theorem or something. So in this scenario, this algorithm is proven to converge and it has sacred sample complexity. In practice, we're going to loosen some of those requirements for the theory to hold but at least it's it has this connection to the theory. And so then, once you get into the deep networks, then you have this this issue of trying to find the right hacker parameters and making sure you're you're comparing against the right baselines and making sure you're optimizing the bass lines and that type of thing, and often writing the code especially for deep networks surfaces some problems that aren't problems in the theoretical situation. And yeah, that's kind of how I approach these things. I try to start from simple to convince myself that there's merit in the idea and then try to scale it up.

Sanyam Bhutani  43:19  
Now, coming to another aspect of future aspiring researchers who take we're taking MOOCs who might not be have been fortunate enough to do proper education. What advice do you have for them, people who are using MOOCs to educate themselves but aspire to be researchers in the future?

PSC  43:36  
I think MOOCs are great. I think it's a real, we live in a really exciting time where there's a lot of resources out there for for people to to learn from and be self taught. One thing I would night it's funny because two nights ago, I was out for dinner with some people including my ex supervisor Doina?. And one thing we were talking about is that because there's so much emphasis right now on deep networks and the power of deep networks. I think a lot of people getting into the field often forget or don't realize that all of this stuff has been around for decades. I mean, neural networks are from the 1950s. There's a lot of work that from the 1980s that people are using now. This is old stuff. It's just now you have more compute. And yes, there's new algorithms obviously, but but it's it has a long trajectory. And the all the people that are the Turing Award they they I mean, Geoff Hinton was working on energy based models, which not many people are looking at anymore. And so there's there's a lot of core machine learning and core math. That is really important, I find for to make a really strong researcher and because there's so much emphasis right now on deep networks, I one thing that we were talking About two nights ago is that we fear that a lot of young people want to sort of bypass all that and jump right into the exciting stuff. That works, which is fine. I mean, maybe that, that, maybe that's just fine. But I would say that there's a lot of value in studying that, that core, the heart of where all this came came from, which is Mega in RL, for instance, like, there's a lot of things you have to learn before you even think about neural networks. Just the bellman equation and temporal difference and things that sarsa you know, all these algorithms that are from way back in the day that are still being used now, but if you understand them well, without deep networks, you're going to have an easier time understanding them. When you when you start trying to to apply deep networks to them. Yeah, so as an example, we had an AI resident last year, Carlos, and he, he actually dropped out of high school. And he was self taught and but he really taught himself that from the start from the basics and he had a really strong, he's very good at math. So he had a really strong mathematical foundation for what RL is. And then applying it to deeper all he had some some really good results, we had a triple A and icml paper. And now he's with open AI. So he, I think part of the reason why he's he is able to produce such great results is because he understands the core of where all these deep results are coming from. 

Sanyam Bhutani  46:38  
Okay, awesome. Another thing that I'd love to know more about and unfortunately, I hadn't heard about this Latinx. I know you're very active in the community. Can you tell us more about it, because as you many people like me are totally unaware of it.

PSC  46:52  
Yeah. So this, so essentially it's an effort to try to increase the representation of Latin Americans in the AI research community. And a lot of it is inspired from the work that the people at Black in the eye have been doing, for the same reasons to try to increase the representation of black people in the Irish community. So we I was at icml, in Stockholm in 2017. And they have these mobile apps where you can create chat rooms and meet with people, etc. And so somebody created a chat room Latinx at icml. 

Sanyam Bhutani  47:31  
Okay.

PSC  47:31  
So I joined in a bunch of us join me said, Let's meet for lunch Sunday. So icml is like 8000 people. And so we met for at least that year was about 8000 people. We met;

Sanyam Bhutani  47:42  
Definitely. 

PSC  47:44  
Yeah, we met for lunch, and there were 20 of us. So 20 Latin Americans out of 8000. So it's like, okay, there's a problem here like we're, we're not really well represented. And all of the people that were there, all of us that were there, we don't live in that America. We live In the US, or in Canada or in Europe, then we work for companies like Google or were professors at universities. We're not in, in, in North America or in Europe, there was I don't think there was anybody that was living in an Latin American and pursuing their career in Latin America. So that we wondering, like, why is that why why are there more Latin Americans here. And so there were a few efforts that that sort of started around the same time, there was this organization Latinx and AI, this this coalition that had already been started. And so I reached out to them and offered to help. And so they had already been accepted to have a workshop at NURBS. So that would be a lot next in a workshop at NURBS. And so I helped with with the organization of that first workshop, and essentially the idea was to try to get people from Latin America, not exclusively Latin America, essentially. But really, one of our main motivations was to try to get people from Latin America, to present their work at this workshop, so they, we went through a review process, and we got sponsorship. So we were able to finance all of the people that had oral presentations, we paid 100% for their trip, their lodging, the registration, and I think 50% for posters. And it was really amazing because that revealed that there there are many problems and many reasons why Latin America wasn't represented, when is it finances, so it's really expensive to get there, two, is just visibility. I mean, they're the, the research that they're trying to do might be more limited to their local, local areas. And maybe they're not submitting or maybe they're they they're not playing the game, right in the sense of trying to get these papers into the main conferences like nerves or icml. Or maybe they don't even want to try it because it's so expensive. Maybe they're liking hardware to be competitive, so you don't have to read GPUs. That's it the thing. And so it was a huge success. We had a lot of people come, people were super grateful for this. And so we've started we've continued doing this. We did it again. Last year. We did it at icml. Last year we're doing it again. This year, we've had a few more informal gatherings AAA, I organized this lunch sponsored by AAA. And it's also gotten a lot of companies interested in it. So Google is, has been really supportive of this. They've both a sponsors and supporting me traveling to these these events and, and helping with their organization. They've been extremely supportive. At AAA, Google hosted an event that was for Latin x and black in AI. And that was extremely useful and people found great value in it. So that that's kind of the gist of those efforts. And there are other efforts besides Latinx and he is so keep who happened last year in ?. We had it was like a summer we had none that afraid to ask you, you, Joe. Chelsea Finn, we have a lot of Jeff Dean came, man, you're sure I gave a talk, I think over video is there were a lot of like the big names, they're giving lectures, I was teaching some particles on transformer and reinforcement learning, you know. So just the community building that happened there was was really exciting. There's also a Ria, which happens in Mexico in August. So I was there last year, and I'm likely going again this year. And it's again, it's part summer school conference. There was a event at MIT a month ago that I was there, that, again, is trying to foster this community of Latin Americans in research and in AI research specifically. So that's kind of a long answer to, to your question.

Sanyam Bhutani  51:50  
So for an outsider and I want to use this advice as a proxy for other underrepresented communities as well. How can an outsider the like, for example, me contribute to such communities?

PSC  52:01  
So there's there's a few different ways. We have one of the easiest is probably, if you're, if you're a researcher, you can help with reviewing. So we have our call for reviewers call for papers. And so people submit papers and the more high quality reviewers that we have, the better not just in terms of the emphasis is not so much in filtering out papers, but in providing good feedback to the people submitting. So maybe that's why they're not getting into the main conferences because they're not getting the right feedback. They're not being guided into how to write a proper research paper. You know, these types of things that sometimes it just takes one sentence from like a very senior researcher, very experienced researcher saying like this is you don't don't do this. And so we do a lot of these things. We've been plannig, for instance, had a thing last year that was really cool that the people that were accepted to present posters, they would pair them up with mentors. So they could send a poster to their mentor and the mentor would give them feedback. So I did this for one person. And they said leader poster and I gave him a bunch of suggestions for how to make the poster more effective and more appealing to try to get people to come talk to them more. There's even organization committees. So each each year, we have a different organizing committee for setting up that's a lot of work to set up these conferences, everything from sorting out visas to sorting out the meal logistics to for sorting out sponsorships, a program committee for sorting out the papers, all these things take work so you can volunteer there. And then just in general, like keeping your ears open even just if something comes up that you feel like perhaps would be a good opportunity for for showcasing some of these efforts like not just Latinx and athletics are black and queer in the eye. And and sort of if you notice people that are involved in the community say, hey, you might want to check this out or or you might want to contact Pablo for this next proposal. You know, there's there's a whole bunch of basically just, there's a whole spectrum of how much you willing to put in. And any type of help is always really welcome.

Sanyam Bhutani  54:21  
This is slightly going off on another tangent, but now that you mentioned it, many people miss out on this fact that the usually the community rewards the best talent for good reason. And like you mentioned, many people don't aim for the best conferences because it's completely out of their reach. Also, even though they might have the skills or the brains but yet they're not able to contribute to it.

PSC  54:47  
Yeah, absolutely. And maybe it's just a question of presenting it like a workshop which is a bit less selective than than the main conference. And there you meet with researchers that maybe they just allit takes like one famous researcher to tell you, this is really cool stuff, you should submit it. And then that's enough encouragement so that there's huge value, I find that one of the main things I like about these conferences is the person to person communication, which is just you, at least for now, you can't get it from video. You can't get it from email. There's there's an immense value in that. And, and I think a lot of the more junior people, or people that are not yet really in the community, the feedback we've gotten is that they get immense value from having more senior people, approach them and talk to them about the research and sort of spend some time with them.

Sanyam Bhutani  55:40  
Awesome. Now, before we conclude the interview, if you were to give one base advice to future aspiring researchers who are maybe trying to automate their music, creativity or otherwise would would that be.

PSC  55:54  
I think follow your passion. Not so I had to leave research for a while because I just couldn't get a job. But I never stopped doing music. I never stopped reading. I mean, it stopped reading academic papers, but it still was, you know, coding my own experiments and and, you know, five years later I was able to rejoin the research community. But not because that's what was popular but because I was always really passionate about that. And so there's a lot of things that are really popular right now that maybe you really excited about. But if you really excited about that cool, but follow that path, because you're really excited about it, not because it's popular, and you think that it will get you published at NURBS or your job. Because I mean, research is hard. It's a really long game. You're there's a lot of failures, you get rejected so many times from conferences or from whatever. There's just a lot of downsides, not downsides, but there's a lot of failure moments that it's not roses out the hallway, there's people you compare against. I mean, competitiveness is still an issue, even though we try to reduce it, but it's inevitable that you compare against you compare yourself against the person next to you or the person who graduated the same year as you that now is doing much better than you are. From the surface, which is not necessarily the case. There's many aspects to to success that go beyond your age index, or how much money you make. I mean, I my original plan was to be in academia when it had to leave. Yes, it was sad, but I still had an amazing job. I still had really challenging problems to work on. Had a great family had a great life. So I was really happy. Yes, I couldn't do research, but that's fine. So yeah, don't be, I guess, follow your passion. And don't be discouraged if it doesn't necessarily work out immediately.

Sanyam Bhutani  57:58  
Before I end the call, what would be the best platforms to follow you and follow your work. 

PSC  58:04  
Twitter. 

Sanyam Bhutani  58:06  
Can you shout out your handle because I know some people are too lazy to scroll to the;

PSC  58:11  
P C A S T R, so pcastr after PCASTR. Yeah, I'm pretty active on Twitter. I occasionally check LinkedIn but I'm hardly ever and Facebook is more for just like personal things. For me, it was really Twitter.

Sanyam Bhutani  58:31  
Thank you so much.

PSC  58:33  
Thank you for inviting me this fun.

Sanyam Bhutani  58:35  
And thank you so much for all of your research and contributions to the Latinx community. 

PSC  58:41  
Thank you for this podcast.

Sanyam Bhutani  58:50  
Thank you so much for listening to this episode. If you enjoyed the show, please be sure to give it a review or feel free to shoot me a message you can find all of the social media links in the description. If you like the show, please subscribe and tune in each week to "Chai Time Data Science".

