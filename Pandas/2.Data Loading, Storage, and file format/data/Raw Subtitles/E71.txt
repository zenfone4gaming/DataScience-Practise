Sanyam Bhutani  0:13  
Hey, this is Sanyam Bhutani and you're listening to "Chai Time Data Science" a podcast for data science enthusiasts, where I interview practitioners and researchers and Kagglers about their journey experience and talk all things about data science.

Sanyam Bhutani  0:45  
Hello and welcome to CTDS.show the show bringing you quarantine content, having interviews with my machine learning Heroes for you to enjoy. Now without more self promotion, I want to talk about this episode with another legend on Kaggle. The first ever Kaggle's, kernel Grand Master, someone's whose storytelling I've personally enjoyed a lot, and I'm sure when I can speak for the community has provided a lot of benefit to the community data scientist at Edison software, Grand Master heads or tails, Martin Henze. In this interview, we talk about his journey into data science, his transition from astronomy, astronomy involves a lot of looking up to the skies and trying to figure out through data what's out there, and I try to draw as you can imagine many parallels between astronomy and data science and his approach to creating the amazing kernels. If you're not familiar with them. Please pause this interview right now. Go to his profile on Kaggle check them out. They're really worth your time. We also discuss his transition into doing data science during the day. There are also I believe, a lot of rich hidden golden advices for people that aren't working towards sharing their kernels on Kaggle. Now without further ado, here's my interview with Kaggle legend. The first ever kernels Grand Master heads or tails, Martin Henze. Please enjoy the show. 

Sanyam Bhutani  2:30  
Hi, everyone. I'm really excited to be talking to another Kaggle legend on the show today. Heads or tails, Grand Master heads or tails, Martin, thank you so much for joining me on the podcast, Martin.

Martin Henze  2:41  
Thank you, Sanyam. It's my pleasure to be here. I'm a big fan of your show. And I'm really happy to finally be on it.

Sanyam Bhutani  2:49  
Really excited to be talking to a viewer supposed to meet in person. Unfortunately COVID didn't let that happen. So I'm really enjoying the chai that I was supposed to give to you. I have that in my cup right now. So I;

Martin Henze  3:04  
For, for the time being I have to do with my own tea, so cheers.

Sanyam Bhutani  3:09  
Cheers. I want to start with a question from Rohan Rao, it's simply heads or tails?

Martin Henze  3:18  
Well, you might think this is a simple question. But as the as all the big philosophical questions like Python or R, TensorFlow or Pytorch, this is something that you can't answer with a sublime. So it depends, you know, it's the yin and yang, light and dark kind of way. It depends on the on the situation. It depends on the question of today on the specific question that you're being asked. So, it's both head and tail, depending on what's happening.

Sanyam Bhutani  3:52  
Okay. And maybe you have to go back and listen to the answer to understand it. But I want to now start off by talking about your background, you have worked as a postdoc researcher in astronomy? So I want to ask you this question, astronomy involves a lot of looking up to the skies and trying to figure out what's out there. This is my understanding of it. So please excuse me if I'm wrong, but trying to understand through data, what's out there? How would you compare that, if I may, to discovering insights with data sets? Do you see any parallels?

Martin Henze  4:28  
Oh, there's a lot of overlap. I mean, when it comes down to it, data is data really? And astronomy is very observational science, astronomy and astrophysics, right? We look at the sky, as you say, and we we try to observe it as much as possible. It's a you know, it's a physical science. We have physical models that go into it. But unlike other physicists, we've concretely set up our own experiments in the lab. I've counselled our own daughters??? in the lab. That's not possible. Maybe you know, that's a good thing, that's one possible might be a bit too dangerous. So we observe and we try to observe as much as we can. And from that, we get our insights, and we get our ideas. So what you get is a lot of data, it's a lot of imaging data, just kind of image the night sky and see what sort of objects can you detect. There's a lot of time series data, in that you, you kind of observe the sky over a longer time period, and see whether there are any changes, which is one of the most exciting things because there are a lot of changes depending on what kind of objects you look at. The sky can be very, very variable. And you you bring it all together, you have a lot of statistics, you have a lot of data to begin with, because there is a lot of sky right there. There's too much sky there for for all at the same time, right. That's why we sometimes get the notes about small bodies coming ready to fix To the ride that we only see a couple days before, I mean, none of those are really a danger or rating problem. But it just illustrates how much sky there is. So we have a lot of data. And astronomers always had a lot of data beginning, about 100 years ago, I'd always tried to use the cutting edge techniques, which at the time, which is, you know, statistics, but also data cleaning data wrangling with a very pen and pencil and paper away. And for that reason, we kind of been used to working with data and extracting insights from data. And that's exactly what we do in calculus well, right. We have our special methods and tools and mythologies, in order to extract as much insight as possible from the data that Kaggle gives us. And in astronomy, we're doing the same just, you know, instead of Kaggle it's the universe that gives us a lot of interesting data and we're trying to understand what going on out there, and the cosmos. So;

Sanyam Bhutani  7:04  
So I, maybe I discovered an easter egg in your Kaggle profile, your PhD was included a study of the Andromeda galaxy. Is that your profile picture on Kaggle and Twitter?

Martin Henze  7:15  
It is not. I thought about it, but I thought it would be too it would be too obvious. No, it's, it's another redshift in nearby galaxy galaxy that we kind of see face on. So we see the spiral structure directly. And it's kind of it's a beaut, big, beautiful spiral galaxy will pull galaxy. It's not Andromeda, but Andromeda looks very beautiful as well, I can recommend.

Sanyam Bhutani  7:42  
Okay, now I want to switch back in time to your astronomy days are these when you were just working with astronomical data? Can you tell us about that journey? Why did you pick that path and now you sort of segregated your astronomical experience into time series immediately to date. Word those terms. Were you familiar with those terms even back in the day? Or were, did you find out that he This is something I already knew about when you actually started Kaggle?

Martin Henze  8:14  
Yeah, so those terms I already knew. So, fun fact is that in astronomy, when we talk about time series, we call them light curves, because that's what they are, they measure kind of delights of the star over time. So they are a curve of the light. And, and all these terms exist essentially. And then astronomy, it was the first time that I really got into contact with data and that I really got into contact with, with projects that involve a lot of data and a lot a lot of data preparation, a lot of data analysis, data cleaning, of course, and different method. So my master thesis essentially was based off I'm analysing a large stack of historical observations. And those observations were made before the age of digital cameras. So they came in these in these large glass plate photographic plates, that were really physically large, and that were taken off the Andromeda Galaxy between 1960 and 1995, more or less with one of the largest telescopes in Germany. So they looked at the galaxy over and over again hundreds of times during the years to observe the galaxy itself, but they also observed a lot of stars within the galaxy, variable stars exploding stars. And that's what my thesis was based on trying to find those exploding stars and study their likers study the time series evolution. And that's not an easy task because those big plays they contain hundreds of thousands stars like 200,000-500,000 stars that you could detect just in the image. And I was looking for the one or two stars in that image, that would change the brightness very strongly over time. So it was a bit like looking for a needle in the proverbial haystack. And that's what made it interesting. And that's what made it challenging. And that required a lot of data analysis techniques, data cleaning techniques, in order to extract those couple of dozen stars that ended up finding from this massive set of millions of stars. Overall, though, it's photographic light. So that was my gateway into database project. That was my first database project. And I was really hooked. I was hooked on data, and I was hooked on astronomy, okay, that's why it's been the following 10 years working in astronomy Now I still think it's a very fascinating field. And now I'm kind of transitioned more into the data field.

Sanyam Bhutani  11:09  
I want to linger on to that point. I read, you'd studied about x rays, x rays that were coming from these directorial activities in the Andromeda galaxy. Could you speak about the techniques that you were exposed to pun intended at that time while while reading about this, and they'll also allow me to transition into how did you pick up different techniques when you joined Kaggle?

Martin Henze  11:35  
Yeah, so you're right, I started with, with optical observations, these big glass plates. And then for my PhD thesis, I started to work with X ray observations. So the thing about so x rays are just another type of light right of electromagnetic radiation, but different from the light that we see with our own eyes. x rays are very energetic. Which means that x rays are produced in processes that are very energetic as well. So x rays come from stars that are not as, as nice and calm and peaceful as our sun, but from stars that have very high temperatures, very high gravity, the very violent processes are happening. Right? And this this energetic universe is, it's much more variable, it's much more it's much more violent than the universe that you can see with your eyes when you just look at the night sky and everything looks unchanging from night to night really. It isn't, looks like that the extra universe they are, you know, very, very strong changes from time to time and X ray detectors. They also work differently from optical telescopes because these energetic these energetic photons you have to treat differently than the normal visual ones. So that was there was a kind of a change in methodology, in the sense that from these optical observations, I moved to extra observations and I had to deal with, with a world where now you would end up counting photo. So I had literal observations where I counted 10 photos, right? And I said, yeah, that's the detection. That's a new object. That was enough signal because the noise was so low because the telescope was so good, that we could say, okay, we can do something with that. And the running joke in x rays is really that with two photons, you have a detection with three photons, you have a light curve. So the time series and with four protons, you have a spectrum, so kind of an energy spectrum and you can try to, to analyse, and it's mostly a joke, but there's a little bit of truth to it as well. Yeah, you're right. Yeah. And and you're right. It's a different kind of it's a different world. In a sense, you're looking at the same objects, but they look very, very different in these high energetic rays.

Sanyam Bhutani  14:14  
Makes sense. Now, I want to transition into your KagglK journey. And my questions are based off on two of your Kaggle interviews, the blog one and the recent profile. What made you take the jump onto Kaggle? Why, why did you decide to sign up on Kaggle and start creating the amazing kernels that now I'm sure it is, all of my audience should know of if you don't please go to his profile and check them out even before continuing with the interview.

Martin Henze  14:40  
I'll wait for you to check them out. Yeah, so my history with Kaggle or my knowledge of Kaggle actually goes back further than when I joined. I had been aware of Kaggle for quite some time. I think it was first in in 2012 when I stumbled upon Kaggle, I didn't join at the time, but I was aware of it, because I think they were running some astronomy related competition. And I just stumbled on it. And I thought, hmm, that's cool. You actually, you know, you don't have to do the research yourself, you don't have to do the analysis yourself, you can just, you know, ask a lot of smart people around the world to, to work on that project that you and I started looking a little bit around on the platform, and I found very interesting and I almost got hooked five years before, before I finally did, but at the time, I was in the process of moving to a different country to stop one of my postdocs there. And so, so it didn't make it high enough, up the list of priorities. For me to actually take the step into Kaggle would have been very interesting if I did it at the time. But I was always aware of it was always at the back of my mind that something that oh, that's interesting to explore. And I finally ended up joining in 2017 at the time where I was in between postdocs. So I had just finished one of my postdocs in Spain. And I had another one lined up in California, where now, essentially, just a little bit more south. And I remember Kaggle. And I thought, oh, I have a couple of months in between jobs, potentially, this is a good time to get into it. This is a good time to get started. But if you know kind of in our academic world, the fact that you don't have a job at the moment doesn't mean that you're not working on these problems, right? Just because you're not paid doesn't mean that you know, you don't have any observations or anything to analyse. You're still writing your papers. You're still getting your data. So that was something that I would do during the day.

Sanyam Bhutani  16:57  
We call that thinker's curiosity.

Martin Henze  17:02  
Yeah, there might also be a little bit of the old publish or perish situation. But no, no, just just because there's a kind of a few months of funding gap doesn't mean that the data is incoming anymore or the, you know, the papers don't have to be worked on. So that's what I would do during the day, essentially, ironically, saying that it's astronomy. And I will, I will kind of work four or five hours during the day. And then I kind of would put that project down. And I would in the evening, and during the night, I would start looking at Kaggle. And my idea was at first that this is a great place to learn more about the state of the art techniques. I wanted to follow the time to use for this likers that I was looking at. I wanted to learn more about how to analyse them, how to classify them in in a big data kind of way, right? They don't have to do too many manual or too many custom analysis. But I could try to, you know, find the needle in the haystack in a more clever and more automated way. And that was one of my, one of my goals, also to learn more about machine learning, because at the time, you know, machine learning was getting more and more popular, and you kept kind of hearing it here and there and, you know, different different papers, different crazy results. And I wanted to know more about that. I know, I knew nothing about machine learning, essentially, at the time, right. And I wanted to know more about that. And then as it happened so often, the more I got into Kaggle, the more I got interested and exposed to all the different fascinating problems and fascinating projects that were happening there. And I got, you know, really, really into it and really interested in and contributing and finding out more. And the community really, really helped, which is something that I'm going to say a couple of times during this interview. Because it's so important that the fact that we have such a great community, made out of people who are not just smart, but are very friendly and very warm and welcoming and accessible and helpful. If it makes a world of a difference, when you're new when you're trying to get into a new subject and try to learn something, and it's so, so important, and it's so valuable.

Sanyam Bhutani  19:38  
Certainly, it's truly the home of data science as as they title it. We'll just talk about that in a bit. But what are your thoughts on when is a beginner ready to share their work on Kaggle? Someone who's just joining they're just joining the platform trying to say they work under physical isolation when when should they feel ready to be able to share their work?

Martin Henze  20:00  
I think they will. I mean, everybody works at their own pace, right? Everybody does projects at their own pace, and at their own comfort level. But I would recommend to share a little bit earlier than you're comfortable with. Because there is a danger. That perfectionism, right? You you can always take a little bit with with certain kernel with a certain analysis, you can try to make it a little bit better but those are only very small steps. So you want to share a little bit earlier than when you're happy with a kernel just to see from other people's feedback and reactions, whether you're going in the right direction, whether the kind of the grand scheme of what you're doing makes sense. And whether the tool that you're using, whether you're using them correctly, and I don't mean put out something that that's half baked or you know, something that you just wrote in an afternoon with a little bit of fault in it, but don't overthink it. I so there's there's a balance between putting some thought and, and, and trying to, to polish something to perfection, which might just, you know, not go in the right direction in the first place. So as a beginner, I would say, pick a project, it doesn't really matter which one but of course there are this great starter projects that we all did like Titanic, or house prices, pick one of them. Make up your own mind. You know, read some kernel if you need inspiration, but make up your own mind how to approach the problem, how to solve the problem, and write up and allow this doesn't have to be shiny. It doesn't have to be pretty. It just should answer some of the questions that you have about a problem that you have about the data set and put yourself out there and look for feedback. Look what other people are doing, and start interacting with the community. And that gives you will give you many, many more ideas, and many more pieces of inspiration to work on.

Sanyam Bhutani  22:14  
I couldn't agree more, in the startup world we call it 80% perfect is just good enough. Just go ahead and publish don't wait for it to be 100% good. What are your thoughts and you had hinted this in this interview also in in Kaggle's blog interview, you had mentioned that community was the Kaggle community was very helpful with the feedback. So someone who's just starting out, they really need feedback. They want to get their work out there and get experts opinions, their ideas, what's what's, in your opinion, the best way to engage with the community. How should someone ask for feedback or put when they put their work out there? For example a kernel, if I may.

Martin Henze  22:55  
Right. I realise that as the community is growing, things are becoming a little bit more easy. But things also become a little bit more difficult because we have many, many more kernels now than we had a couple of years ago. And many more people, but also means that many more people who read other people's kernel, and who can give feedback. So the important thing to remember is really that it's a two way street, right? You're not just on Kaggle, to put out your work and look for other people's feedback. You're also there to give feedback to other people and to to communicate with them about their work. So what I would recommend is if you're at that stage as a beginner with your first project, but your first project out there, but that's something that you're reasonably happy with, but you think that might, you know, need some some input, and then just start interacting with the community. Look at other kernels, give some feedback there. Participate in the forums and fences. Certain question that you want answered. And more often than not, if you put something out there, that's original, which is important. And that that's yours, and people will look at it and will give you feedback. So, so this is what I think of the more natural way to do it. And to kind of underline more, that natural aspect, what I'm not recommending is to go around and to put comments or, you know, discussions everywhere saying, hey, I made this kernel look at it. You know, this, this kind of advertisement, which has increased a little bit too much in Kaggle in the most recent times, it's not a good thing. I you want to interact with the community in a way that you give valuable feedback, and then valuable feedback will come back to you. It's a natural process, you don't want to you don't want to turn it into an advertising project. Because now people on Kaggle we know about data, we know how these things work we realise when somebody is and I have to say spamming and, and people who are doing these kind of spam things, they are not highly regarded. So you don't want to do that be be genuine, just, you know, focus on the on the issues, focus on the projects, give you feedback, and feedback will come back to you, I guarantee it. And even more so. So that already works in the in the starter competitions like Titanic>> or house prices, because you know, people are engaged. But that works even better once you decide to join a live competition, because if you join a live competition, everybody essentially starts from the same place and some people of course, have have have knowledge about tools that's more extensive than yours. Some people might have some domain knowledge, that's more expensive expenses than yours, but that will always happen. But in terms of the problem, we all start, you know, with looking at a problem for the first time and thinking, oh, how could I possibly solve that. And if you have an interesting idea, if you have an interesting kernel, or an idea that you could put out in the discussion, then that will be useful for everyone. So if you're engaged, if you're, if you really contribute to the community there, then the community will appreciate it. And we'll get back to you.

Sanyam Bhutani  26:38  
It's, I like to think of engineering as an art of sorts data science is the art of data if if I didn't say so. And if I were to draw a parallel, it's literally like an artist carrying their painting around and showing it to other artists say what do you think of mine instead of just discussing art with them and that's literally annoying in some sense. That's that's where I think and I agree with you that it's also a spammy behaviour. And I think people fall into that trap because they really, some, some of the people might want to have some attention on the economy because they've put out some book. But like you said, if if it's really genuine, it might take a little while, but it does get picked and people like you have also started a thread where they they're trying to especially you are trying to find underrated kernels. And the community is great at that if if the person creating the kernel is very genuine.

Martin Henze  27:33  
Yes, and there are a lot of these hidden gems, as I like to call them as well. So, so yeah, I just started this, this new series in which I tried to find three kernels every week that I think are underrated. I think they are they're really good. There's really quality there. But if you look at the votes or the comments or the engagement that those kernels have received, is that underrated in the sense that there could be more people looking at that more people engaging with those kernels, and also more people learning from them, really. So I'm kind of trying to do my own small thing. And to bring a little bit more balance to the community in that sense that we tried to focus a little bit more on the on the quality instead of, you know, call notes that are being spammed, or, or things that that are very basic, but for some unknown reason, have a very large number of votes.

Sanyam Bhutani  28:33  
I'll definitely have the thread linked in my show notes. So audience please check it out. If you want to take a look at it. I now want to talk about your process of creating kernels. How do you start with a blank notebook? What are your first steps? When you just starting with a blank Jupyter notebook you're trying and you have the data set loaded? How do you approach any data set or any problem in general

Martin Henze  29:01  
Let me let me be snarky here for a moment. I don't start with a Jupyter notebook. It's not my, my favourite kind of environment. You know, I'm an R guy, I make this, you know, sometimes acoustically clear. And I like Python, I like Jupyter. But I think there are you know, better IDEs out there. So please don't hate me for. it. But just a little side remark. I think that when it comes to new projects, as I said, we all started more or less from the same point of view, we're starting with a new data set that we haven't seen before, and that we want to analyse. And every data set is different. We know that very well as well. But there are some similarities from data set to data set that we can make use of. And those are the steps that I pretty much follow in every kernel and I think the the only project that I started really from scratch without knowing what to do was Titanic, was my first you know, Kaggle project that many people but tag approaching. And ironically, I just remember that I still did start that in Jupyter notebook. So, you know, apologise for for snarkiness. Now, I assume. So there are some fundamental aspects about data sets that you can always start with, right, you start with a very high level big picture overview. Let's talk about tabular datasets because that's what I mostly have worked with. And they're the parallels are really easier to find. But the steps themselves they also translate to NLP they definitely translate to time series, and in the sense they also translate to imaging properly so you first want to know about the general structure of the data, right? And that can be very easy. How many rows? How many columns? How many missing values? What are the types of the features that you have numerical features, categorical features, how many of each is kind of thing, it's what you want to look first at. And then the goal is really to get a bird's eye overview of the data. And for that, I use visualisation extensively. That might just be my preference, because I'm a very visual person that really helps me to see the data spread out and the data dissected on several axes, and trying to see those relations see those distributions. But other people might be useful as well, right? So you could start out with just plotting the distribution of the target value, right, that's already tells you something that just by you know, just the feeding the data into, into your model you might miss. So you might see that, okay, that the target is, is normally distributed yay, everything is is fine, and it will never happen. Or it might be, you know, lock normal, or it might have some weird distribution, or it might be imbalance, right? If you have a classification problem that might be very, very imbalanced. And if you didn't know that, you might, you know, you might feed it into your model, and you get 98% accuracy and very, very happy. And it's absolutely useless because it's unbalanced. So so this kind of fundamental thing is something that you you're looking at first, and you want to try out and find that at the beginning, because you don't know that you don't know anything about the data yet. And you want to see what are the fundamental property and you can do a similar thing for the different predictive features. You can just plot it as tribution see what they look like, see whether there's anything interesting there and once you know that since then you can start look at interactions between the features. See whether there is there is an easy impact or an obvious impact that your target has on one of the predictors, something that you can then start to exploit or modelling. Look at correlations, look at multi dimensional plots. Take the different features apart, correlated with each other, put them in relation to each other, and try to understand what's really happening in your data. Try to have a deep look into your data. So it's a kind of it's a multi step process in which with each step, you go deeper into the data. And with each step, I can guarantee you, you will get more ideas and you will find more features of the data and more oddities that you can then decide to explore further if you want to do see something that Oh it does correlation looks odd, I can try to, to split it by that other feature and see what's the impact there maybe I can find a clean separation or something like this, those are the kind of the typical discoveries that you will make. And then from data set to data set, they are more or less important. And you will, you will follow the more important one and you will get to a more kind of complete understanding of what your data set looks like.

Sanyam Bhutani  34:32  
I can also see a parallel like you mentioned earlier, you will use to visualising the X ray data, the the I think you call it the images or the stars on the glass lab, and I can see the parallel between data visualisation is do you think that's also an originating point of your love for visualisations?

Martin Henze  34:55  
It might be I mean, the astronomical data essentially arrives in images. So whenever you you get a data set from telescope, you get a set of images. And for for normal optical data, those are religious images as you imagined them to be so that you the ones that you can find when you Google for Hubble images, and you see these these kind of beautiful large fields of galaxies and stars, and x ray data, another little fun fact, it's actually a data cube that you get, because an extra telescope doesn't just accumulate all the photons that you get, like like an optical detector test. But for each photon for each single photon, it measures the XY coordinates plus the time of arrival, the energy of the photon. So it's a four dimensional data set that you get and you can take those four dimensional data set you can kind of squish it down to two dimensions in of space and just look at an image, or you can kind of take it apart. And essentially, you could look at the, the time series of all objects in the image simultaneously. Depending on how many objects you have, that's not realistic. So there's some kind of slicing and dicing of data;

Sanyam Bhutani  36:24  
Recent techniques.

Martin Henze  36:25  
Data transformation, exactly that data extraction. And then but that's the kind of it's a two stage process because what we would typically do, or what I would typically do is to take these images and then turn these images into tables, right squish them into tables, where have the coordinates and the energies and the time see is for all the objects and then do some statistical analysis on those and then turn them into images again. I had where where I would have a plot a visualisation that would show for example, the amount of variables for certain object, together with its its average spectral fingerprint, let's say to to keep it a little bit non technical I, and then we see, okay, so there are some objects that that are very similar and they show similar variability. And then there's another set of objects in the parameter space that are similar to each other, but different from those. And you can try to do some of the mineralogical physics with those and try to figure out what could be the differences. And in that sense, that those are those are visual methods that are always applied, that are always used to try to figure out about what's going on in my data.

Sanyam Bhutani  37:43  
Okay, now coming back to the original question, and I really linger on to this point a lot, but I see many people struggling with it. Beginners, go online and they find this checklist of courses of things you absolutely need to know here. I need to know Python or R then I need to know about math, all all of these details. And then maybe you're ready to jump onto kernel and I think your approach, as you described, described, it also follows but I call it top down learning approach. What are your thoughts there? How how should one go about learning? When they're also trying to, let's say, put out kernels or when they are on Kaggle?

Martin Henze  38:25  
Yes, so if you're starting out in that way, um, it's a similar thing to what I said before, you want to put yourself out there a little bit earlier than what you feel absolutely comfortable with. And you do need a certain set of background skills or certain set of fundamental skills, you do need some fundamental skill to, you need to know a little bit about programming. And it doesn't really matter which language I mean, I like Python, Python is great. I likeR, R is great. Whatever works best for you. Whatever flows more naturally for you. And, you know, realistically Python and R, those those are the best options when it comes to data analysis projects and when it comes to Kaggle projects, specifically, because those are kind of most common languages that we see in the kernel, specifically. So pick one of those, learn about the basics you want to know about the basics of programming about you know, structures, like like loops and conditionals. And maybe, you know, some language specific things like list comprehensions. In Python, you want to know these, these fundamental because they're relatively fast to pick up and they want to make the latest stages of the learning much more efficient. And then for Kaggle, specifically, you also want to learn about what drives the different algorithms that we use if you don't need to go too much into that too much into detail. You want to have an idea of what's the decision tree to start with. What's the decision tree? Okay? No, yes, no, yes, no, yes, no, you know, heads or tails, heads or tails, tails. And, and and if you understand what's the decision tree then you can understand what's the random forest. And you can understand you know, what's what's boosted tree. Okay boosted tree, you need to know a little bit about gradient descent. But again, don't worry about the mathematics too much at first gradient descent just means kind of going down the hill, you know, in in relatively small steps, and going more or less in the right direction with every step. So those are the kind of fundamental the fundamental ideas that you need to know when you get started. But then, I recommend and I encourage you to get started with a specific project as soon as possible. And I think that's what what many people will tell you as well. Don't waste too much time on trying to learn some some abstract ideas and some abstract notions. But get started with a project get started with Titanic, Titanic is very nice or with house prices, and take all these these ideas and all these tools and learn their application for a specific project, because that will tell you and teach you much more about how these methods work when they work when they don't work on a very specific data set. And for that, it will come in very, very handy. If you spend a little bit of time understanding the data set. Right. That's where the whole exploratory analysis and visualisation will, will be very useful. We want to understand the data set first. Make up your own mind about it. And then you can start using these different methods and these different tools and trying to solve the problem and on what a data set looks like, you will you will see very quickly which which tools work out of the box and which tools might need some some data preparation or in order for them to work much better.

Sanyam Bhutani  42:18  
You were one of the Kaggle legends, the first kernels Grand Master, and if I did relate to perfection, your colonels would be the example that are all almost perfect out. If I dare say perfect. You keep iterating once you put out a kernel, even after you put out I think you go through a lot of, as you mentioned, discovering questions and finding their answers. When is a kernel for you finished? After you put out a version you keep iterating on it, when you decide to end iterating upon it?

Martin Henze  42:53  
Yeah, that's a good question and and a kind of dangerous one. Because the the kind of the, the gut answer would be that a kernel is never finished. Right? Once you get into a problem, you can always think of, you know, other ways that you could analyse the data. Another way you could try to model the data, it's never really finished. So the important part is that you have to decide when to finish it. Like you said earlier in our chat that at some point, you have to decide that, that at this point is good enough. At this point, it's sufficient for for what you want to do. Right. And my goal with Mike Arnold is to give people a comprehensive overview of the data. Right? And I usually stop them at a point where I think that okay, now now, this is enough information and from here, people can go and look deeper into certain aspects that they found to be interesting. When they were reading my kernel and there are lots of ideas, so, I always kind of keep a list of of ideas at in, in just a text document for every competition that I work on, and there are always more ideas that I have for the kernels like I could I could look into this feature in more detail, I could look into this interaction in more detail and they never stopped to naturally never stop you will read you will realise that as well. When you start working on a project at some point, you have to decide this is good enough. And, and this is sufficient for the purpose of the kernel. And now I want to do something else or I want to, you know, start working on the problem from a different angle. So some of my kernels include models, right, some basic starter model, I can Xj boost with, you know, with the basic parameters, no hyper parameter tuning or anything but not all of them do. So sometimes they do that model on the side and I don't publish it because you know, you want to preserve it these a little bit of a competitive edge even if you're sharing and, but for the for the kernel try to, to kind of try to get to a point where you feel like, okay, this this is a comprehensive answer to the question that I want to answer. And they're more more strong. They're more kind of ideas that I could follow from here. But I want to leave that to the readers of the comments. I want to leave that to other people to get their inspiration and to explore these other steps. That's my approach. 

Sanyam Bhutani  45:43  
Okay. And this also brings me to another question to someone who's just starting out in the community and they are maybe interested in looking into data, creating insights by themselves. They look at the other amazing work your work. I'd also like to mention some other people, let's say SRK, Shivam Bansal, Andrew Lukyanenko. All of us definitely get inspired by your content, your style. There's also a dangerous area where you start copying it and bring it into your work. How can how what's the right approach because if you keep looking for example, if I keep going through your kernels, I might just recreate them in a certain way. And that's, that's, that's a dangerous territory.

Martin Henze  46:28  
Yeah, and I don't see that as a big problem though, at the beginning, because when you're starting out as a beginner, it can help you to have a certain structure and and a certain philosophy that you that you can emulate and that you can start from and, but I can guarantee as you as you go further as you start to tackle further problems, you will develop your own style of analysis depending on you know, your own philosophy and your own. approach to things. So for me, I, I learned a lot from other people's kernel. When I when I first started out I learned a lot from from SRKs kernel, of course because they were great. I learned a lot from people like [?]. There was a guy at the time called called Phillip S., who was putting out great kernels, great visualisations, he hasn't been around for a while, unfortunately. And, and I looked at at all of those, and I think I, in a way model my early kernels on on all of those different, you know, inspirations and ideas that I was seeing. And then, you know, over time, develop my own style of analysis and develop my own approach. And something similar will happen. It's really inevitable. If you if you start out and if you you know, look at enough problems looking enough analysis, you'll come up with your own style. So I wouldn't worry about that too much. If you at the beginning leaning, lean a little bit more on what other people have done on the structure that other people are using in their code. That's, that's perfectly fine. The important thing is that you put your own original ideas out there and you put your own work out there. And from there, you can only grow.

Sanyam Bhutani  48:27  
This, and this is sort of an extreme version of it. But many people might fall into the trap of plagiarising so I think they're two different things. And you might agree, one would be taking someone's work and building on top of it, you take their work, you really add some value and share it. And the other is sort of just using it and adding very minor changes to it. And this this issue is sometimes visible on kaggle especially in beginners who fall into this trap of getting early woods since since it's gamified category. What are your thoughts on? How should we avoid that? And it's I think it's also misconception that people really want to get a medal, for example, or just want to get to that tier.

Martin Henze  49:17  
Yes. Wow. There are there are a lot of layers to this question. So let me let me try to unwrap it, I think so the first aspect, building on the work of others. This is this is something right, this is something important. And this is something Kaggle really excels in, through the kernel, specifically, but also through the way that the community interacts. So the kernels give us the opportunity to kind of stand on each other's shoulders. If somebody puts something out there, I have an idea of what to do with that analysis or what to do with that model I fork the kernel, and I put my own spin on it and I put it out there and people can learn from that as well. So, so that is a great thing and that is a very, very valuable aspect of the calcula environment. Now, the important thing to keep in mind in these kind of processes is that you always want to give credit, you always want to make clear for people, what is your own original contribution, and which contribution you take from other people. Right. And, and fucking kernels, you can do that you can, you can say, okay, I took this setup from, from SRK. And I, you know, I tuned to the hyper parameters, I did some additional feature engineering, and here's what my model does. But it out, they clearly say I take this data from those sources, and those ideas from those sources, and I have my own ideas, and that's what I find. And that's great. That's the way it should be. Coming to the the plagiarism and the gamification. So that's something that we, we keep talking about in the community every once in a while, and it is a problem. Because the the kind of the chromatic cores of it are the way that you look at science, the way that you look at data analysis and the way that you look at your own work. So where I come from in academia, plagiarism is a big, big, no, no, it can get you into a lot of trouble and for for good reason, right? Because they're the papers that you write. They are your intellectual outfits. They are your contribution to the community and to find that if you take somebody else's ideas and put them name on it and then publish it. This is a grave offence, this is a very unethical thing to do and, and you know, you get punished for it in the community for good reason. And this is something that, that people who the very few people, the small number of people who try to game the system in Kaggle, they have to realise that what they're doing in that way is unethical. You cannot go onto the internet and find a medium article and just copy paste it into a kernel, and then put your name on it, and then publish it and and ask people for uploads. This is wrong. This is unethical. It's not your ideas. It's not your original contribution. You're taking somebody else's work, and you're trying to pass it off as your own. This is something that we don't want to see a guy go that's something that I don't want to see and the other thing is that for the people who are doing this, it's not going to help them in the long run. [Yeah] Right. Because they're not actually. They're not actually interacting with the data. They're not actually having. They're not doing that actually growing. Yeah, they're not doing data science. They're not growing, they're not learning. They're just just copy pasting stuff. Because they think that the upvotes and the medals and, and even the rank that they get for these kinds of things, that they they mean something. But they don't write I I'm a I'm a fan of the gamification system. I'm a fan of the ranking and the medal system in Kaggle. I think it helps people it helps to motivate them, it helps to what they mean. But all these things, they only mean something in relation to the original work to the contribution that you put out there and in relation to your reputation in the community that you've been build through your contributions through your interaction through your questions through your answers through your work, then they mean something, but without that they don't. So it's it's a it's a hollow victory if somebody, for some reason manages to gain their way to a monster rang or something like this with plagiarised content, it's not gonna last and it's not gonna help these people in the long run, but people have to be aware of it people have to be aware of where the boundaries are. Because they might look a little bit more fluent for a beginner, right? Many of us when we do coding, and we run into a problem, what's the first thing we do? We check Stack Overflow, right? You have a solution there, okay. It has 400 votes. Great. That's that's the thing, and I put it into my code. And, and that's okay, when it comes to these kind of technical things, right? You wouldn't necessarily give a citation to to some random guy on Stack Overflow for kind of solving this problem. This is something that we do we kind of we all answer questions and we all benefit from questions that somebody else's answer. But when it comes to science and data science is science that's when I believe then taking somebody's original ideas or a work that that somebody has put substantial amount of time and effort in and passing them off as as yours. And that's just unethical. That's wrong. Please don't do that.

Sanyam Bhutani  55:38  
Yep. If I dare add to that part of this also originates from and if I can speak to that the fascination for becoming a Kaggle Master, Kaggle Grand Master. I've been fascinated enough to start this podcast for two reasons. One is, I want to discover if there is a secret Fight Club, where the Grand Masters hangout which I haven't been able to do, no one wants to break the first rule. The second reason is we all look up to the Grand Masters as the respected figure since Kaggle is really the home of data science. And people look at this, from a perspective that GraMd Masters are really good data scientists. And if I can just get that number of medals, I can become a master, or I can become an expert. And as you mentioned, you you won't be really doing data science if you're just just aiming for those medals. And ideally, those would come along as you gain visibility if you're genuine.

Martin Henze  56:31  
Yes, exactly. So they're kind of two to this aspect I'd like to mention and in the sense that the medal and the ranks, they will be, they will come automatically. If you if you progress if you put your own work and if you if you're learning if you're growing there will be a side product of your learning journey, they will not be the main thing because and to put it a machine learning terms. You don't want to overfit to the medal, right? You can do that you can over fit to the medal, you can over fit to the ranks. But it's not going to help you in your own personal development and in your own learning journey. Because those are the metrics that you want to evaluate your progress on ride, not just the medals, not just the ranks, you don't want to overfit to the metal. And just trying to find a good soundbite don't want to overfit to the metal. [Yeah] The other thing that needs to be mentioned though, is that the cases in which this happens, a really rare I and I think we are very fortunate and maybe lucky in the sense that the community that we have is so exceptional in that almost all people are playing by the rules almost all people are are open and friends And are our genuine people are really genuinely there to learn and to learn from others and to to give back to others. And if you think about it, the most respected Kagglers and Grand Masters that we know, right, people like CPMP, like Bojan , like, like SRK. They give back a lot. I they they give us all a lot of inspiration, a lot of ideas. And and that's why we respect them, not necessarily because they're Grand Masters this is just a side product of their the journey of growth and their journey of learning.

Sanyam Bhutani  58:42  
Certainly, I also want to talk about I think you've recently ade the transition into doing data science during the day, which I think was last year. Can you talk about that transition? Because you did that after becoming the best on kaggle kernels. The legend on Kaggle kernels. How's that transition like for you, and how helpful was Kaggle for you in that transition?

Martin Henze  59:08  
Oh, that's very helpful. KaggleKwas was instrumental for the whole transition. If it weren't for kaggle I wouldn't be at this point in my life and at this point in my career, I'm at the moment and well, it essentially began when I when I joined Kaggle, right? So I, as I said, I got myself into all these interesting projects that were out there, and all these interesting tools and methods that you could use to approach this project. But I learned a lot about all the different things that I wouldn't have even considered as a project before I taxi rides in New York and Wikipedia hits over time. All the the interesting NLP projects that that I kind of got into and most of them I started really from zero. I had so the taxi competition. I hadn't done anything with geospatial data before. And at a time when the competition was was launched, it was, I looked at it and I thought, hmm, this might be an interesting thing to get into, will I be able to really contribute something, because I don't know anything about geospatial analysis about maps or about coordinates. And then I thought about it a little bit more. And I looked into the data and I thought, well, if you're already at this point, you might as well you know, get invested into it, that tried to quickly learn as much as I could and to, you know, to, to give back in some in the shape of accountable and just show, you know, the community and the others who are working on this project, what my views were and what my analysis was. And then this way, I'm became more more interested in data science that was happening in the real world, right? Helping people in the real world. So I really like these data science for good competition, that that are happening on Kaggle every once in a while, I think those are very valuable for people to really make an impact. And that was the main reason for me really to decide that it was time for a change in Korea. Right, it was, it was a it was a natural thing that I still think that the astronomical project that I was working on, that are valuable and that are very worthwhile. Right and I'm still in contact with my with my friends and colleagues and you know, I see what's, what's happening in the universe. But I also think that the the data science projects in the real world are very valuable to and they're a good way for people to And to grow and to, to learn more and to grow into a different direction. And for me, it was it was simply time to, to go into this different direction and to do something different. 

Sanyam Bhutani  1:02:15  
Awesome. Can you share with us what do you do in your day time when you're doing data science a Edison software? Can you tell us what all tasks do you work on?

Martin Henze  1:02:25  
I can tell you a little bit about it. So as we are a startup that works with email, right, I have a great email app that you can download. And we just launched an email service that you can use. And what we're doing in the data science team, is we're doing market research essentially, on on aggregated and anonymous commercial data that we see from our customers. From our users, and we're trying to understand, primarily for the United States, but also for other countries. We're trying to understand how consumer behaviour is changing over time, and how consumer behaviour is changing from place to place for different industries. 

Sanyam Bhutani  1:03:22  
Okay, got it. Maybe I'll try to bring you on the show, again to maybe discuss in detail about that. But for now, I'll stop my question where I want to come back to Kaggle, you are already a legend in the Colonel's category. What's next for you on Kaggle? Luckily, you also continue your sharing of kernels with us for the community. Are you looking at other categories as well? Do you have any other plans?

Martin Henze  1:03:49  
Yeah, I'm definitely looking at other categories. I mean, I still think of myself as reasonably early in my Kaggle journey, because there is there's just so much to learn. There's so many other things that you can do, I want to get more involved in competition. So during my first year, I was focusing a little bit too much on on the kernel in the sense of that I was a little bit distracted by every new competition that came up. And I didn't really follow the ones that I was I was working on. I want to do more of that. And I want to become a better data scientist and a better machine learner. Really, because when it comes down to it, I don't think that at this stage, I'm a very good modeller. I am probably not hardcore in that sense. And although I know a lot of people who are and I admire them very much for their skill to jump into a new competition and to be up there on the leaderboard. [Yeah] So I'm reasonably happy with my kind of exploratory skills. And I'm going to continue to work on them. And I'm going to continue to put out new kernels because there's something that I really enjoy. And I want to be yeah, I want to be a little bit higher on the leaderboards at the end of the competition, because this is something this was a kind of a challenge for myself. And another thing that I'm thinking about is that it won't be fun also to team up in competition, which is something that I haven't done before, partly because, you know, I was in that sort of distracted mood where I was jumping from competition to competition, based on you know, eda and kernels when something new came out. So teaming up with somebody, I think is would be a very fun experience will be something that I definitely want to do in the near future.

Sanyam Bhutani  1:06:03  
Awesome. I think that also exposes you to the culture of data science for people that aren't already data scientists. That's also one way to understand how do you work in data science ?? on real data science problems, if I may say so.

Martin Henze  1:06:18  
Yeah, and that's a great part of our community, right, that everybody has a different background. Everybody has a different approach. And then if you if you ensemble those all together, you get something that's kind of bigger than the individual parts.

Sanyam Bhutani  1:06:34  
Certainly. Now, this has been an amazing interview. My final question for you is, what best advice would you have for someone who's trying to utilise data in physical isolation and just starting their data science or Kaggle journey? If you had to give them one advice.

Martin Henze  1:06:53  
Well, I mean, right now in this in this crazy world of COVID that we're living in say one fun thing to remember is not to put yourself under too much pressure. I so so this is an unusual situation. And there there is a lot of kind of, let's say emotional pressure on all of us to deal with this situation in a healthy way. So don't don't try to do too many things at once. If you want to start, get started with Kaggle. That's great. Welcome. We're very happy to have you. And let's get started small, get started, pick pick one project. Pick a starter project or if you see a live competition that's interesting to you where you might have some existing domain knowledge then get started with that. But don't try to do too many things at once. If you're starting out as a very beginner if you have no background in programming, or in machine learning or in anything, just start with one language. With one project, and then bit by bit, start to build from there.

Sanyam Bhutani  1:08:08  
Got it. Before we end the call, I'll have your profile linked in the description, your Twitter profile, and your amazing blog, which is another place where you sometimes hear insightful stories, any other platforms that you'd like to mention where we can find you and connect with you?

Martin Henze  1:08:25  
I'm really most active on Kaggle itself, I would say in the forums and everywhere. Second, second to that I'm probably most active on Twitter. I don't tweet a lot, but you know, occasionally, but at some, some ideas, some some thoughts and some feedback that else I have for other people. You can connect with me on LinkedIn, if you're interested. And that's it, essentially.

Sanyam Bhutani  1:08:53  
Awesome. Thank you so much, Martin, for all of your contributions to the Kaggle community. And thank you so much for joining me on the podcast.

Martin Henze  1:09:01  
Thank you for having me. It was a great pleasure.

Sanyam Bhutani  1:09:10  
Thank you so much for listening to this episode. If you enjoyed the show, please be sure to give it a review or feel free to shoot me a message. You can find all of the social media links in the description. If you like the show, please subscribe and tune in each week to "Chai Time Data Science."

