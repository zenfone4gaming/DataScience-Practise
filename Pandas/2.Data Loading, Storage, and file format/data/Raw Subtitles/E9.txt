Sanyam Bhutani  0:13  
Hey, this is Sanyam Bhutani and you're listening to "Chai Time Data Science", a podcast for data science enthusiasts, where I interview practitioners, researchers, and Kagglers about their journey, experience, and talk all things about data science.

Sanyam Bhutani  0:46  
Welcome to another episode of the "Chai Time Data Science" showl. I'm really excited to be interviewing one of my machine learning heroes for the second time on this episode Kaggle Grand Master and senior computer vision engineer at the self driving car, division of lyft, Dr. Vladimir Iglovikov, Grandmaster has already been kind enough to share all of his secrets and his journey into machine learning slash data science in the amazing blog interview to which you can find a link in the description of the video or the podcast. In this interview, we talk about albumentations framework, which is a framework by Alexander Buslaev, Eugene Khvedchenya and Dr. Vladimir Iglovikov. Albumentations is a fast image augmentation library that also serves as a wrapper around other libraries. In this episode, we talk all about image augmentations. What makes the library great and the efforts the research that went into the library. You also talk about open source and Grand Master shares many great advice about working with open source. Make sure you check out the blog interview in the description and enjoy the conversation about albumentations. 

Sanyam Bhutani  2:17  
Hi everyone. Today I'm honored to be joined by one of my machine learning heroes for the second time. I am joined by Kaggle Grand Master and senior computer vision engineer at the self driving car division of lyft. Dr. Vladimir Iglovikov, we'll be talking about one of his open source projects. The albumentations library. Thank you so much for joining me today Grand Master.

Dr. Vlamidir Iglovikov  2:42  
Thank you for inviting me and thank you for giving this opportunity about talking about like open source and in library that we're doing in our free time. So it's really good opportunity like I mean to promote it maybe explain more to the like audience and you know, get more feedback and we'll see how it goes.

Sanyam Bhutani  2:59  
So we all know you are one of the Grand Masters for computer vision competitions. But before we start talking about the framework and all things open source, could you maybe help us set the stage for why image augmentations are important for Kaggle or maybe even computer vision tasks.

Dr. Vlamidir Iglovikov  3:18  
So the main reason why image augmentations name for and for like it can be division tasks, because it helps me and if it didn't help, there was no one but like in evidence shows that basically, when you use image mutations in the correct way, it was a model performance, generalization and etc. So the reason and do they really want me think about this reason number one, so what does it mean should mutation, you basically take your original data and modify it in such a linear or maybe nonlinear way? And it means what you can think about is that you think synthetical extending your data, how much if you take all your images and edit flip horizontally version of this increase the data say what What about if you just like dig this image in edit or images that likely horizontally and vertically and so and both of these combinations. Four times. What about if you had like a bit of rotations here and they are and this like, you know, it's bigger, bigger, bigger and bigger. And of course, like different transformations that can be likened to the spatial that I just described. But one can go more creative, you know, play with pixel values make it more blur and something notifications of this. And so what does it mean? It means that you can extend your data set, it becomes bigger, and as we know, especially in deep learning bigger data set, typically in most 99% of the cases, just increasing the size of your train data set increases the accuracy of the model, let's train on this. Think about this is emotion meditation is regularization. We know that there is like a lot l two and some other types of regularization that help you more linear regression or deep learning The more generalizable and improved performance, all of them has one if you cannot interpret them, so it will do there is some value with your constraints, your weights. And so what is a good bad like point one, I don't know, like crystallization will tell you, you should. On the other side, they allow you to have some insights, let's say, you know, you train some model and then you look at the train data set and I plan to have blurry images, you just okay. Any big randomly try to blow images that you have any big just insurer may help to perform better on the images? There are no maybe you see some images, hit some other properties. And you basically can adjust your image recommendations based on the visual inspection. And so basically, you can use your brain, your background, everything that you learn so far;

Sanyam Bhutani  5:51  
Like you said to create artificial data, essentially.

Dr. Vlamidir Iglovikov  5:55  
Yeah, but it's not just like enough just artificial data because if I blindly try to improve my data set it's huge I can improve in such a way that it will be taking corner cases in my test data set and help with dealing go to some. [Gotcha] Yeah.

Sanyam Bhutani  6:12  
Okay. And I assume like many people aren't aware that even data collection is is quite resource intensive. So that's that's one of the areas where this is definitely pretty important.

Dr. Vlamidir Iglovikov  6:24  
I can tell you this example last year, I believe I read two papers and probably within one week, so the people were talking about emotional classification. And they were comparing so, like in one paper, what do they do they take I believe it was out of meant they train them one on one for image land and they like tuned in chosen some internal data augmentations in intelligent way and they go like the one accuracy believe like 84 yeah, there was another paper that also took similar network like accuracy was 81 and then the to the the second major labels from, don't remember where but the size of the data set was 3.5 thousand bigger.

Sanyam Bhutani  7:05  
Okay.

Dr. Vlamidir Iglovikov  7:05  
It was like 1 million like and it was became, you know 3.5 billion or something like this. Now the network on this bigger data set and of course it took more time. And of course, they needed to collect this to store to use much of your power and the pencil, the same 84% so this is kind of one way think about this example. That kind of works, I want to emphasize but that like second typically is probably preferred in industry even in takes more time and more collection. Just one you need to choose this data mutation intelligently. And this is the course you will like it maybe like not an art, art graph science. So you need some like person who's skilled in this to work on this. Second, on the other hand, it just blindly collect, train good enough so and in both approaches are important. But if you're talking about specific mechanical configurations, or basically talking about situations where data set unlimited in your car, I collect like the wind thousand times more. I mean, albumentations especially good one is basically a way to go to be to, you know, basically in condition or maybe to be baseline. And I'm pretty sure that like if one and I hope one of the listeners may try to do it, you take this thing imagenet classification of some other standard benchmark data sets and like add some data limitations from our library ones that are not like us there. You may be state of the art and you can publish paper, and get things from this. Yeah.

Sanyam Bhutani  8:32  
I'm sure many people would be interested, we will have all of these links that we'll be talking about linked off in the description, so please be sure to check those out. Now, during our blog interview, which is also linked in the description, you say that you started Kaggle by and sort of experimenting with all all of the experiments and you settled on computer vision, with the results we all are fans of. And I think during this period, the library would have started to emerge. So could you tell us like, why did you feel the need to create start like working on this library?

Dr. Vlamidir Iglovikov  9:05  
I mean, first of all, I want to say that like, I'm only one of the authors of the library like I would like to say big things to Alexander Buslaev, Alex Parinov, Eugene Khvedchenya, and they I would like to say main contributors to the library. So in this sense, it would be very unwise and unfair to say that I am an author, like everything in the library and one of the oldest we have slightly different skills probably I'm slightly stronger than that, like people like Kaggle Masters, Grand Masters, we all have like good top solutions, but I mean, we have slightly different component and we all use this library and I will excite projects at work, but I mean, still yes. So in all of us work in this and our joint worked, helped us and led to this library. So I believe like why, in general, we started like, called this library. Built in the days when I started working Kaggle I implemented implementations like men of basically I mean, it's really not rocket science to write like a function that leaves an MP dot fleep good enough but and you do this issue appears when you work on image segmentation and an kago for the clear long time they're like a lot of image segmentation you know, competition so you don't need just to clip an image you need to flip your mask at the same time and you do this with some probability and I mean I got plenty of bugs there because sometimes you just forget to do it in sync and just issues there and similar story with crop similar story with rotations and you get a lot of like cold with can leave some some mistakes can it still work like and;

Sanyam Bhutani  10:39  
Also seem like back in the day like I think fast AI now has all of these fancy options and even Pytorch but back in the day I don't think these libraries supported.

Dr. Vlamidir Iglovikov  10:48  
So I'm talking about like station when it was probably like my first gold medal ultrarunner song segmentation like myself, like touch of the internet. And it was like years ago, like really long time. Maybe or maybe DSTL competition the hair but like two years ago and spring so, but it wasn't a singular that I'm fast AI I didn't develop there and like many other libraries. So like we were writing the score because we needed to. [Okay] Then I mean we looked around because the situation didn't look good. And there was like image oak library which is great and great and was like a lot of a lot of things we learned and goes from this library. And I believe again, like two years ago, there was a competition Amazon from for us something about crucification which a label crucification or instead like, just about Amazon was hosted by a Kaggle by Planet Labs, and go there it was crucification. So there are less issues about different, you know, seams between masks and images. But problem that we had because data set was relatively small groups, and we had a lot of GPU power. It was interesting competition was like MNIST type that competition. None of us wanted to do it. That's why we stuck 480 networks was like the biggest leg that we can like inside oh, you know, to use not just in production but still in doing because it was easy to do just to play some big balls on the issue we faced of that competition is that LGB utilization was not even close to be 100% I was CPU to believe I was residuals of maximum [okay] but you utilisation was significantly lower the reason is because two years ago in the show was extremely slow. It provided some nice functionality like nice pipelines but it was so slow and as we know i mean if you want to write for to be unconditional, like do some kind of good projects that work you need to iterate first first iteration you don't want I mean, do GPU utilization to be a bottleneck so of course like me adjusting that we do you replace HTTP by SSD to like improve input output because we thought like it was an issue like originally, then I mean, we started looking into this lectrons formation some problem Falling and will be like figure out that some of them extremely slow.

Sanyam Bhutani  13:03  
Because when when you at this stage you would want to like ounce every juice from your hardware so I think that's;

Dr. Vlamidir Iglovikov  13:10  
Absolutely old people old Kaggle is they're not just like into machine learning they're probably typically like into decent write code and also they have like home they're like their boxes with multiple GPUs some cooling optimized hardware or look CPUs and etc etc because you do as you just mentioned because it's your free time and it is your compute power you try to make everything as optimized as possible. So, this augmentations and they were slow in the replays them some became better and not and also like as we know in competitions winner share their goals so someone shirt here something shirt there and like somehow different people working on slightly similar like similar notions of some organizations pipeline and then in like then can you competitions and then it was full of 2017 I believe andals karma challenge. And I merged in the same. I like it to provide a challenge where Alexandar Buslave, I and Mr. Thompson?? finish first, slightly misleading, which was like meriter dating. And so they're good like we would expose to each other called about image augmentations, Alexandre and I realized that kind of similar structure.

Sanyam Bhutani  14:28  
And they started to sort of connect for you, when;

Dr. Vlamidir Iglovikov  14:32  
We started thinking about this. You look at it from crypto performance, you're used to your pipelines, you know, optimize it, you know how to make it better. [Yeah] And you can improve on top of this and get every new competitions or emission mutation pipeline for every class became better. So we're actively going one after another Alexander's pipeline was the most developed University and the most ambitious and I will say like so immature of limitations of implementation library that we have right now. Now is mostly based on his vision of how image augmentation should be and prototypes of the cold lake and even like eco structure. So it's like, that's why, you know, if you want to think who's the main like author and configure contributor over there, like a limitations library, it's like somebody whose life whose computer is an engineer who works in men books in Minsk in Belarus right now. So yeah, and then Alexander also participated once all the 3d some space and the challenges and then descendants of Rebecca and Victor, do know they will be the science ball in 2018. And at that point, we probably like discussed and decided it's time basically needed to release it. So yeah, like I mean, Alexander is usually like, you know, refused to release he said, it's not perfect. Like I managed to be like, think about as badly and like, I mean, just doesn't remember at all, it's like, release what you want. Because as I know, after many Kaggle competitions, release the code and it's been redone, it's unclear what to do spaghetti code still is useful for sour and of course on top of this, you can iterate collect feedback and make it a better product. So somewhere about a year ago, CPR of the last year, we released gold, it was very buggy dirty, it worked for us supported only classification and segmentation tasks, but it was still already a lot, especially because it was based on, you know, so many Kaggle competitions, but projects and other things that we're doing.

Sanyam Bhutani  16:24  
Okay. Now, I also want to like, talk about the breadth of transforms that are supported by the library. So it supports quite a few augmentations that I don't think other libraries support. So could you maybe highlight on that?

Dr. Vlamidir Iglovikov  16:40  
Uh this is good. So in some sense, this is one of the issues that we like have this Library, we know how to use it, we know what functionality it has, but our documentation and probably like promotion examples of Jupyter notebooks less developed than it should be. That's why for many people, it's intellectually unclear what feature now to debate and what like differs our library from models? First of all, I mean let's talk about what are the what are the similarities what are the differences, similarities first of all like image of mutation the change and if you're talking about image show that influenced the library a lot, two years ago it was completely different store right now right now it's much faster more functionality and guys that work on this I have great respect to them. They do the job. So but still like what is our like skin maybe selling points if you want why we developed it, as I mentioned, we didn't likes beautiful this instrumentation library. Our library what we do we just look around with you what transformations work foster in what implementation because it can be NumPy it can be OpenCV, where it can be with interesting thing for images that are in like a bit images. It can be open city to do some kind of transformation, but for you know, l maybes, higher Bits images and 11 bit or 16 bit per pixel so you can find in satellite imagery. It's typical standard there. I mean;

Sanyam Bhutani  18:06  
This, you, you would have like gained the knowledge through all of all of this Kaggle experience and all your other experience from the other projects that you've been working on.

Dr. Vlamidir Iglovikov  18:14  
So lab is some something yes, but like probably not. So in terms of comparing benchmarks, I mean, for me in this transformations, we had, like some kind of implementation, let's say, using open CV, and then someone tells the like, mentions that like, not all we get the idea, like, let's ship like, you know, compared to in NumPy, open CV is faster than in the profiling benchmark. And then you see that in this case, you know, this in this case, no. And basically, it's what we implemented in our code, trying to make it as fast as possible, because we want to avoid the situation as it was an Amazon challenge where CPU is a bottleneck, and right now GPUs like, become bigger, faster, CPUs on grow as fast. And that's I would like to show this so first is the spirit for most of the transformations that we like I mean, we are faster of course execution is changing in, gets better keras is still slow to achieve and gets better here and there but still kind of slow. So that's about it. So and in our report if you want to compare what we do haven't been in love with in our repository, we have a code to benchmark on your hardware across different libraries. And also in our underneath have a table that like compares and shows what's the situation with expectable with latest measurements. And latest that we got, like we did it was about a month ago. So it was the split. Second one is about like ipi so when I was writing this image limitations building the days it was some really horrible scoring functions here and there, right now. Urbanization, support this functionality, basically, like you know, get the least of the transformations that you want to use their parameter, something random, something random, didn't work, probably images to use it, and it's waterline transformations, transformations can be breathe advanced can do some spatial like flips, notations transpositions or crops, or it may be something like color transform hcb RGB space working so weird one life compression and we use it for some forensic challenges and listings. So and you can do it one transformation being alive. And of course, we also have this functionality when in this pipeline of transformations you can choose want right now that is some probabilities. It's really convenient, you want 10 transformations. And this is pretty advanced because typically for imagenet, classification or some other task, they have three you know, a crop resize, a need of color transformation, and that allows you to build a really, really diverse and this basically helps a lot and computer vision competitions. Again ever want to mention that in the last eight months to my knowledge all top also a top solutions to Kaggle can be revealed and challenges used our library well, I mean, we should probably add a stigma and extend this, but it is the case. So I mean, one of the reasons because instead of three like small transformations, people do 20-30 and do some kind of really weird stuff. So secondly, it was like API that makes it very convenient. Third one. Third one we added recently, and it's why I'll mention it now. So for reproducible research open you want to know what happened and how so how my current machine learning pipeline work, I have some kind of standard pipeline and everything that can be defined like AdSense and learning rate type of the optimizer momentum for the miser augmentations some parameters of the network something else, it is the outside like config. Before this, like when you use Yama will do some config, it will speak it will define this a limitation there because like they were defined as the Python code, but in the release, we added serialization deserialization sorry. Now to do reproducible research, basically, here's my config. Here's my result. This includes like parameters, networks and accommodations [Got it] is possible and this is more closer probably edit away from the competitions and closes the production environment again, isn't really cool, important feature. I'm not sure that any other libraries do this. Next;

Sanyam Bhutani  22:16  
I think not many people even like talk about it and I assume like base entity not just approximate even like experimentation or research even. I don't think many people even talk about this.

Dr. Vlamidir Iglovikov  22:26  
As reproducible research. I mean, people like don't talk about this as much as about we are beating human performance. But this area is growing and we're seeing cvpr enables like people discuss this and they like some papers related to this reproducible research comes most Learning Academy doesn't care about this. As much as you know, most of the papers you can't reproduce. But this is a bottleneck and I believe Andre Carpathia talked in his talk about Tesla and the audience how they like deal with this deep learning and how they reproduce their works and there was something about this there. So and at the same time you would like to do some kind of transformation augmentations defined in Python code then you want to dump into like Python dictionary or JSON, configure your mail and save it and commit as a code somewhere or maybe so previously, you also have this functionality so and Alex part of edit this and this was really cool. I mean, I use it a lot especially these days when I'm became much more responsible to make reproducibility and call that I'm writing this one or do one. So, we got like, I mean, in terms of computer vision tasks, we support crucification type of the dust when only emerges like affected you support segmentation task when both liking symbolism image and mask affected. We support bounding boxes again for detection tasks when he mentioned bounding box like effect in the same way like crops or transformations, rotations, whatever it is. So again, crucification segmentation detection test me support key points for like media transformations, which allows us key points and like big thing right now, especially because research moved from academia to production environment. [Yeah] So support is. And we also can do it at the same time. For example, if you're for some image you have segmentation mask, bounding boxes, people and whatever it is, and then let's see you want crop some part of this is automatically growth, same area in the image, same in the mask thing in the bounding box, the same and the key points. And so if you're talking about instant segmentation, or some interesting tasks that involve, you know, multiple outputs or different types, it everything is done simultaneously. Because you're making like, you know, doing bugs or doing some mistakes is extremely easy. And our library allowed to do it out of the box. So this type of the task, what else can we do this functionality again, adding seen any other library, multiple input, multiple output? Let's see, I believe right now. There's something can be where you have like two eyes and you need to do some kind of classification or regression at the end and let's see you would like to do similar transformation to this like two images and randomly choose like flips rotation some color transformations but it's really exactly the same to the both images, cordia our library allows it out of the books you can have like you know multiple input images I mean for one to I mean as much run off your computer has and apply this or you know the use case if you have a video and you want to grab some part of the so calorie to do some kind of different transformations out of the box I mean you can basically apply to reduce to sequence of frames were interesting for sure. So this allows this because again like instantly start reading go for these the real the bucks it's one thing multiple inputs. At the same time you can do similar story for multiple out. Let's say you have like few segmentation masks, few bounding boxes up points in a year you would like to lose similarly grow up or update or transpose or some other information for each of them at the same time. So input images, one amino them and also output like masks, multiple targets. It happens out of the books like the all of them simultaneously. And this is again very convenient, especially because people start talking about like me to pull out different targets than the game. So you do it works out of the books, I strongly believe for everyone who's working academia or maybe competitions for some type of the tasks to try our library because you get like huge competitive advantage over people that just don't do it. Because I mean, people who delays to implement this out of the box with a few lines of the code of this is really cool. So am I believe these are the main things that we have right now. And this like last few that I described to be makes a difference. So serialization, multiple targets the problem is; 

Sanyam Bhutani  26:56  
I think even in as per my knowledge, like image standard image augmentation a sort of the standard, but when you start talking about segmentation and like bounding boxes, not many people are even talking about it, let alone like talking about augmenting data there. So, I think that's also an exclusive zone for albumentation.

Dr. Vlamidir Iglovikov  27:16  
Yeah, definitely. One thing I want to add in the latest released, we added lambda transform, it also exists an image choke and then some of the libraries, what is lambda transform? So if you have some function that came into your mind to get to your paper, and it's not implemented limitations, if you have this function, you basically can make it part of your image transformation pipeline machine like interesting way, I mean;

Sanyam Bhutani  27:41  
Just like a lambda function

Dr. Vlamidir Iglovikov  27:43  
Lambda function Python, you can also make it part of the lambda transformers. Yes, so this is also. So there is also some transformation in some other library that you might be part of all pipeline with all this like synchronization, anything like this, just to wrap up with lambda and this documentation so this like broadly long, so yeah, definitely helps.

Sanyam Bhutani  28:05  
I'm sure that that's everyone would love to check it out. And that's definitely super interesting. But I'm also curious like, since it's, I think four of you working on the library, could you share maybe what kind of experimentation and research goes into a continuous in can because you continuously adding features and also developing the library. 

Dr. Vlamidir Iglovikov  28:26  
Okay, so how much time do you spend on this? Not too much, because we do it in the full time because we have like someone who has condition someone whose work someone who has other projects. [Yeah] And because it's in more of a stable phase that we like, kind of like good enough to us. [Okay] Time that we invested but this is another big at the same time. If we look see some investment of foundation, okay, I mean, request is no difference. The biggest driving force for us right now is the community and again, like if you're asking how can you help a library the biggest thing buckton field that you can do your feedback if you found a bug submit an issue with request bug report and our GitHub repo. If you like to have issues using it in terms of documentation, just basically you know, something that can be obvious to us maybe not obvious to you submitted to extend documentation or request will be even better. And again, some any type of the feedback and that's what you want if people send me feedback oh, wow, this is interesting feature let's say we don't use key points but people requested this immediate this civilization we needed this. That's why we are in this. So too much sources, if you want something for our like work the projects, institutions, and it may happen that we do we'll just basically endless but it's like one source but right now it looks like we are more or less in the good shape. Second one feedback from the users their requests like what where how to do it. And again, so this is our next second round before so much time hard to tell. Maybe we can do it more. But because open source project that everyone can like pull And useful purposes and because we have to pay money for this free time.

Sanyam Bhutani  30:08  
So we all like that Kagglers do love sharing but I want to ask you like why not keep the secret sauce to yourself like stay the exclusive Grand Master and not like make this open source why create this as an open source project instead?

Dr. Vlamidir Iglovikov  30:23  
So I don't think the secret sauce business work here so let's see one of the things that I used to like compete well and let's say I work hard some competition let us I'm full of ideas then experiment, iterate, try check them. And at some point I get to the plateau. I'm out of ideas or nothing works and let's say in place you've done it would like to get higher. What do I do? I share my code. I share my ideas in the forum. I am basically share everything with the community. Because why because instantly like this, like hundreds of people jump on them because they want to get to the fifth and 50th place. They use their creativity their ideas and they give me new ideas so and I use their insights of course they get higher than me but I also like to use their knowledge to like refresh my brain and to get high on top of this works for like probably one of the motivations so I don't believe in the secret sauce that much of something because if you look at the Kaggle if you're going to conduct paper people try to there to some career studying some clear experimentation to see that we'd like invented this type of delay or this augmentation. So this lectric optimizer and we got some boost. Kaggle can be different they're significantly less transport. If you look at the like solutions of the winners don't 40 different ideas. That one person listen, but it's really unclear. That's why there's no one secret sauce, there's secrets, bunch of different sources. And just saving this and helps you to work and as and against augmentations gives us but it's not like life and that you can implement these things yourself. Not the additional libraries have similar functionality and make them better, but made it worse. Me, it may be helpful, but yeah, and of course, third one, I mean, if the only thing that I have in Kaggle competitions is using this image augmentations libraries, I kind of suck myself in terms of new ideas, papers, research creativity. And this is one of like 50 things that that are helpful. So [Got it] and last one I like to share with the community. I share my knowledge and a blog post or share my knowledge in paper sessions like podcast interview, and again, thank you for inviting me like me and I feel good when people are going to help people benefit as much as possible. This is also one of the reasons why we need this to help the community to advance the research to help travelers and it's also pretty fun.

Sanyam Bhutani  32:57  
I'm sure all of the people, not just from Kaggle. Not just from academia, even from industry are grateful that you always share your knowledge. So;

Dr. Vlamidir Iglovikov  33:05  
Our library is definitely used in some big companies, I will not tell you which one because they can share this I just know because my friends who work there and they told me about like other teams so their teams using and but definitely had a good sort of not only in Klike somebody projects, some probably use it in their pipelines.

Sanyam Bhutani  33:24  
That that's good to know. I think that's that in itself speaks for the library. So that brings me to the point like could you tell us a few things off the framework that you're really proud of things that you put out over the past year?

Dr. Vlamidir Iglovikov  33:41  
You said like a few things here like that I would like to talk about first one from style wise???? and Maria ????like the serialization deserialization. Because they it's like important step like for industrial adoption and overall for the reproducibility search and removing duplicate work within a community. So this probably like my favorite. At the same time, this multi target thing that like does multi talented image thing that like, again, allows limitations to such a tasks where this imaging thing some of typically used this is a little thing. But this is a technical one, what I also would like to talk about so for me and for other folks like for us this instrumentation library is not just like you release the source code, as you may know, they're all i mean enormous amount of like good code good, like some libraries and the GitHub that like me is like 50 stars so let them know and know that they exist. And so called is eaten and it's good can solve some problems. No one knows about this. I think a lot of like, we don't think about a library owners about just like some library, some source code, some theory from software engineering perspective. Being partially about this library is about a product. This library is good, but again, if no one knows It exists. It's not cool. If people don't give back it's also not cool. We will work and maybe not hard enough, but you work on some kind of user adoption. We would like to like to give big thanks to open data science community oh, there's the guy probably like many of you know this from the Kaggle competition. We have discussions then people give us feedback and many new features and bugs we implemented based on the work of the people from there. So also, like Kaggle community gave some feedback. I like support, you know, like even like some tweets about our like, releases were repeated by kaggle and even like, see all the Kaggle that like help like user adoption promote this library. We and some people we presented some different like conferences and meetups and right now I believe we have like 2600 stars and then you'd have I mean, all my like other project have significant level. So I'm not sure this numbers leak, but I mean, still bigger than many things that I know. So like, the thing that I would like to be proud about work of the our team cared about promotion. And yes, just boosting user adoption and helping other people just like to know to use the adopt. [Got it] Yeah.

Sanyam Bhutani  36:14  
Okay. And if you could maybe like, I know this already, like, plenty of stuff in there, but maybe some future feature that you're excited about and you looking forward to?

Dr. Vlamidir Iglovikov  36:25  
This is good question. Let's think about this. So right now, as I've said, this library city spy zones, it's works really well. So requests like that we got right now on the issue tracker, and they're like, are there some, like small bugs that we are fixing my pretty fast? Questions about the commendations, and this is important for us, we need to extend it and have my examples. But fact that albumentations would edit on the Kaggle kernels probably like me, we assume and we hope that Kaggle community will help us to promote albumentations and show examples for its users in advanced cases, like in the kernels. So documentation was like a big thing for us maybe not as exciting but kind of mandatory. One thing led can be interesting is right now there is like some work about style transfer for image augmentations. When basically people you know, using Gans and people do some kind of you know, the image and then they do the style transfer, it gets same image but at night, and this used to, but still objects are the same, so it can be treated as imagery mendacious, and this'll work about this. We will eat require some neural networks here, and there are some gaps in it's not clear how to make it part of the pipeline. But we believe that it can be pretty late, good step in this direction, and other features. So we found this image augmentation library, we have a lot of transformations. But one of the differences between experienced deep learner and probably like less experienced is the intuition and experience just in terms of choosing what transformations to use and one parameter structures because this is not structured well going so right now you're getting into music he was also cuddle master she created like a website of limitations not ML really where you can upload an image and try different transformations like parameters to visualize how well does it look and based on this;

Sanyam Bhutani  38:18  
Right in the website is it? 

Dr. Vlamidir Iglovikov  38:20  
Oh, we didn't give a link just probably will add this to the you know, comments to;

Sanyam Bhutani  38:25  
Okay.

Dr. Vlamidir Iglovikov  38:26  
I'll give you a link such I mean, so but we need to scale it down there's a lot of transformations and it's it's still very unclear what parameters to choose what transformation because what do I other folks typically do you take few images, you're bloody from his formations, waving this for an hour, like some Jupiter notebook, some experiments on them, what makes sense you add to your pipeline, but this is manual and inefficient and not very scalable. We are going to make some kind of websites motivated to form in the Guinness example in sweat and this like you know, to just be help to just what transformation to do how does it move because some transformations the transform image in a very like strange way that you just don't even expect. So this issue can be solved. And yes, so we expect this, again, like so it's like incremental change. It's something that should open doors to magnificent use and many pipelines. So probably this our New Year's plans, we will, you will look up here and there because as I've said, it's our, you know, free time and got into libraries more or less working. But he like if somebody wants to participate to create like issues, feature request proposals, you can back about documentation all which is again, better to the request and do some work so that I mean, become a contributor. It's really appreciated.

Sanyam Bhutani  39:45  
Okay. I also want to talk about the setup for the framework, and it's extensibility. So could you tell us like, what libraries is it based upon? I'm sure it's multiple numbers, maybe like the majority ones and water frameworks are currently supported by albumentationd. 

Dr. Vlamidir Iglovikov  40:00  
What it is based on? So as I've said transformation, you know, looking what is the fastest way to do so of course, I mean, some transformations are done better with Beulah NumPy. So NumPy is part of it, of course, open cities collectivise. And for many of them, like for most of them, I will say open cities used. We also there are some transformations in image that looked very interesting, and we didn't want to implement it. That's why we had the rappers on top of that. So with this, like, man, maybe something else. And again, it may happen that tomorrow someone great request with some library that's also used inside of this. And here I want to mention that again, if you will, because of this lambda transform many transformations, you can look at the functionality of the libraries like here. So in terms of like, that's what it is based on what is fast is used, such as it's clear from the legacy perspective of what frameworks are supported. Main framework for deep learning for us right now. It's been revitalization neulander scarers and cafe and something else so we developed this organizations library our main focus was Pytorch [Okay] so Pytorch is supported but what about other like libraries? So in this organizations library What do you think about images and musk is just like non byte arrays. So if your image in your like loading pipelining and Data Loader can be can be implemented as an Numpy array good enough we don't care just like use them that's it. [Got it] So automatically means that like if you're using Keras and again, like there's a lot of examples in the internet or hobbies, organization, that chaos I mean, go ahead. Things are slightly trickier. It works sometimes are on some kind of like tip records and some of the things there. We never checked how it can be used there, but if someone will find a way it will begin highly appreciated will extend the invitation is functionality. So I would like to see by Pytorch and Keras is definitely supported everything else, if you will be able to implement your images and masters Numpy arrays, during data loading pipeline and in good enough, like this.

Sanyam Bhutani  42:09  
Got it. So the library, I was going to the database and the library boost top results at Kaggle, topcoder, cvpr miccai. So do you think like if Kagglers start to include albumentations in the pipeline even more, they can start bagging metals, maybe like, move from bronze to silver from silver to gold?

Dr. Vlamidir Iglovikov  42:33  
Well sure if they do it in the correct way, like and it's definitely will give them numbers. Again, I mean, there is no guarantee the fuse organizations, it will help you a lot. But as I mentioned, first of all Venus solutions and the last eight months and Kaggle can be reviewed in competitions, used organizations, maybe not all but most of them, and based on my personally the folks. And secondly, I mean, I also believe, like do this functionality to be developed in this like simple API that allows you to pretty crazy stuff and if you lions am it will speed your iterations up so that we'll be able to focus more on some creative ideas rather than some mundane work on debugging transformations. In a game like we have extensive set of different transformations, which means that you can like experiment and apply them randomly. Basically do a question if you're in the bronze and you switch from are the like implementation library to you and organizations. There are chances are one guarantee you but it may happen that it will just you literally step and using it in a more aggressive way, it will move you higher on deliverable.

Sanyam Bhutani  43:35  
It's also like as we call the deep learning is all you can be so if you like mix all chemicals, it might explode but mixing the right way and you get the right result.

Dr. Vlamidir Iglovikov  43:44  
Yeah, and again, like you know, right wait, and you should do it first again. And I spend a lot of time on debugging my image like transformations pipelines during the days done right. Not that I don't need to spend spend spend my time on this is great. So I hope others can also benefit from our work.

Sanyam Bhutani  44:01  
Okay now I know this might sound intimidating to beginners So, what kind of expertise would you imagine would be required to you know get started with this like I know their example codes can one pick those up right and get started right away?

Dr. Vlamidir Iglovikov  44:18  
We hope so. So we created examples through like you know how transparent only images images and masks multiple transform some segmentation like so. I hope you'll be like people will be able to peek like from these examples. Again, if something is not clear and most likely to make it but I really encourage everyone to beginners non beginners, give us feedback write something in the issues and basically it will add us because I mean, we know how to use it we understand the code. Documentation may be not that good well written and Jupyter notebooks that we wrote may be like misleading, but we can see what are the steps basically a give us feedback and if something's not clear, we will extend Another thing, yeah. But again, encourage you to look through this notebooks before like, you know, asking questions. Maybe we answered some questions there. [Okay] But it should be applying. It should be a few lines of the code for pretty crazy stuff.

Sanyam Bhutani  45:15  
Basically do your homework, but the creators are always there to help.

Dr. Vlamidir Iglovikov  45:20  
Exactly and not like, like, we're eager, like we're looking for like your feedback, how to improve the product and how to make it worse. 

Sanyam Bhutani  45:27  
So I think, like many people would definitely appreciate this because we, I don't want to name any names, but we have these huge fame frameworks that are sort of open source, but creators are not that inclined towards feature requests or even like issues.

Dr. Vlamidir Iglovikov  45:42  
I mean, I don't judge them that much. Open Source framework is freaky. If it's like, oh, I mean, even small and like, you know, we kind of only like stable face is our straight now supporting is not rocket science. I mean, you get some requests here and there. But it's nothing but the same time if your problem maybe another thing that I would like to mention here, I would like to say big thanks to all explorative who invested a lot into like architecture and like, you know, automatic recommendation generating and making everything. This saves a lot of time to us, if it was like much less than the quarter of the quarter was much worse, you'd had like more bug support to do painful and we would like more stressed out because of code and like we need like big support is the support is not that hard. At the same time the framework is bigger baby like slightly will return and there's a lot of feedback from the users on the everyday basis about bugs feature requests, supporting little bit painful in because you can live because you have your like main job because you have some other activities, you may be less responsive to the requests.

Sanyam Bhutani  46:52  
Okay. I am like everyone very thankful to you and all the creators for this framework. So I also wanted to ask like, how could we how can we support the framework? If we want to, for those who are interested?

Dr. Vlamidir Iglovikov  47:07  
I believe I said this already a few times, but just the most important feedback eventually is that what do you want? What doesn't work for you like issues? Like you know, if you join ODS.ai, there is a like special like channels like channel there we can leave like feature requests and back and some other questions. It's one in if you would be like happy to participate and maybe like you know, and we mentioned an episode where he leads to be like mentioned people that participated and contributed and again, maybe you'll be able to sell it for like you'd like land and resume contributor told limitations. I mean, we wouldn't be here like everyone who's like, you know, doing small request like everyone is at it. So, if you would like to improve functionality, I mean, a new transformation. So something else implement implementation like critical request, like me favor like, appreciate it. So this like secondly, cinematic, and third and of course, like just like try to use it. Okay the most important I mean, just try to use it for your projects. It may help you may help us but probably probably Let's sit I mean we're doing this in our free time so like we don't need any money support. Yeah.

Sanyam Bhutani  48:16  
So before we can do I know you said many great advises in your blog style interview, which I encourage everyone to go check out after this interview, it has a lot of advice in there. But I also want to ask you like for all beginners who are intimidated to like contribute to open source, since you mentioned that as in your earlier interview, that as you participate in competitions, your repositories will. So what would be your best advice to like how to get used to that intimidation and be would be like, do you welcome beginner contributions to albumentations as;

Dr. Vlamidir Iglovikov  48:55  
This is good question like so on one side will definitely like welcome. On the other slide if it's like really like bad you feed back and work with this person through the school like and to like get us to the appropriate quality it may take much longer time than doing it yourself. Still still still even if you're a beginner and you gratefully request late do it as you can we will work with you it will help you to get like better court quality and like no to the court practices that we are using in our like library and also I mean it will give us motivation to like address this issue and to improve it yet to spin this. So if you intimidated just do it i mean I will not guarantee that like you know, word but like probably you learn something library will get better at this.

Sanyam Bhutani  49:47  
And maybe a general advice for people who want to contribute to open source but like even like it's sort of like Kaggle you have all these experienced people but you feel intimidated by that, some times. 

Dr. Vlamidir Iglovikov  49:58  
Don't feel intimidated. What Kaggle's like I mean so I'd like and and so what happens you have some library they like some people that work on the slide where you never met them before they'll never met you you don't even know each other's names. What's the worst case scenario will happen if you will try to contribute but you'll get rejected or something else will just, you know, not work done nothing. I mean, you'll get upset for a bit although you as an adult, you should get used to the rejection. Like different reasons. Just do a three and not a big deal. 

Sanyam Bhutani  50:31  
Okay.

Dr. Vlamidir Iglovikov  50:32  
Yeah, it's similar like the the Kaggle, you know, many people like, you know, intimidated by how to start your thing and just do it. I mean, because like, they're like, yeah, I mean, people didn't do it. So you're not like dumber. Most of them are like you.

Sanyam Bhutani  50:49  
Grand Master CPMP had given one sort of advice here that use a an alias if you if you like worried. So I think that's also useful. That this case has been;

Dr. Vlamidir Iglovikov  51:02  
I mean hard for me to imagine. I mean, I'm pretty arrogant. That's why I like put my name everywhere doesn't seem to bother me that much. But yeah, kind of like this advice is definitely like wait to work on some intimidation. You can be behind some anonymous alias. And you know realistically you can start with anonymous hours because intimidating but later on it can become a breath. 

Sanyam Bhutani  51:23  
Yeah, for sure. Alright, thanks for all the great advice and for the answers and for doing the second interview and also thanks to you and all the creators for albumentations.

Dr. Vlamidir Iglovikov  51:38  
Thank you for taking this interview. I hope like this interview will help other people like start using our library. I mean, it definitely makes every person who's using a library and makes us feel that like we spend this time just for some reason, and may the world slightly more interesting and speeded up research and legless and also I mean if another thing that I forget like you know if you want to say thank you just leave think if you like take any of this author send any problem writing like thinking the issue was not a good idea but like in some Twitter or like Linkedin and or somewhere else so just like just say thank you for the library. It would make us feel good.

Sanyam Bhutani  52:24  
Got it, alright. Thanks so much.

Sanyam Bhutani  52:38  
Thank you so much for listening to this episode. If you enjoyed the show, please be sure to give it a review or feel free to shoot me a message you can find all of the social media links in the description. If you like the show, please subscribe and tune in each week to "Chai Time Data Science."

