Sanyam Bhutani  0:13  
Hey, this is Sanyam Bhutani and you're listening to "Chai Time Data Science": a podcast for data science enthusiasts, where I interview practitioners, researchers, and Kagglers about their journey, experience, and talk all things about data science.

Sanyam Bhutani  0:45  
Hello, and welcome to another episode of the "Chai Time Data Science" show. In this episode, I interview Navdeep Gill : senior data scientist and software engineer at H2O.ai. In this interview, we talk all about the intersection of these two fields, data science and software engineering, best practices for both data science and software engineering and how much of software engineering skills should our data scientists really know, is the question that we also discuss in this interview. We talked all about Navdeep's journey into machine learning, machine learning interpretability and his journey at H2O.ai. We also talk a lot about machine learning interpretability, Navdeep's thoughts on it, as well as MLI inside of H2O's products. Quick reminder to the non native English speaking audience, please remember to enable the subtitles on YouTube or watch the interview on YouTube instead of listening. Yes, the interviews are available in video and audio format, because the subtitles have been manually uploaded and term checked for corrections. Without further ado, here's my interview with Navdeep Gill. All about machine learning, machine learning interpretability, software engineering and data science. Please enjoy the show.

Sanyam Bhutani  2:17  
Hi, everyone, I am really excited to have another if I may, great maker on this show, Navdeep, thank you so much for joining me on the "Chai Time Data Science" podcast.

Navdeep Gill  2:26  
No problem. It's an honor to be here.

Sanyam Bhutani  2:28  
Likewise, it's great to be finally talking to you on the show. So I want to start by talking about your background. You studied stats and psychology in in your during your bachelor's days. Was that your secret preparation towards machine learning? How did the dots connect for you? When did you get interested in machine learning over the years? 

Navdeep Gill  2:48  
Well, yeah, so when I started off college, I was initially just majoring in psychology and I was also studying mathematics at the time. And I thought, well, you know, I was really interested in how the mind works, I was interested in all of these things I've learned about in high school when I took like physiology, and I learned about the brain, all these types of things. So I thought, well, maybe I can make a career out of it. So initially, psychology was sort of the degree that stood out to me. And then plus, taking classes on, you know, certain, certain biology and chemistry and mathematics, all that kind of piece together, I thought it would give me a better understanding of the mind. I thought I would maybe go into clinical psychology, maybe be a clinical psychologist working on more of the experimental side, so not really beside that speaks with clients per se, but the side that runs experiments tries to understand how the human mind works in certain aspects, whether it be vision, hearing, cognition, all these type of things. So that's where I initially got started. And then I started working in the lab at my university and a lot of my work was pretty much data analysis. That's pretty much what I was doing. I mean, yeah, we would formulate problems, we would write papers and all these type of things. But a lot of my time was spent behind a computer, just analyzing data using languages like Python or uh, back then we used SPSS. So then that was sort of this GUI interface kind of like what a, like rapid miner or something like that, you know, those type of tools. So I was spending a lot of time analyzing data. And then my advisor at the time suggested that I also studied statistics because I was planning on going to graduate school like a PhD program. And he suggested to study stats cuz it sort of gives you the technical background and sort of an edge over other applicants who probably do not have that background or maybe only have one or two stats courses. And at the time, my university offered a pretty good program in which statistics could be double majoring with a lot of other majors because a lot of a lot, a lot of courses to overlap with each other. So I just decided to pick up another major then statistics so then I got my bachelor's in stats, my bachelor's and decided to minor in math, all in college, and I thought that was a pretty good package to have going into a PhD program. I thought it would give me sort of an edge over other applicants. So I went down that route, initially. Yeah.

Sanyam Bhutani  5:14  
Were you always drawn to programming and maths because that's something that if I think usually biologists or even psychologists are slightly distinct from what, were you attracted to that?

Navdeep Gill  5:25  
Yeah, I think I've been attracted to programming ever since I was a little kid. My dad taught me basic when I was younger, my little MS DOS computer back in like the mid 90s, early 90s. So I've always been attracted to that. And then, you know, both my parents, they, they study did sort of have scientific backgrounds. My dad studied chemistry, my mom studied economics. So, you know, growing up like that, you sort of get attracted to science, you get attracted to building things of you know, you know, I had like a little chemistry set and my dad got me when I was kid, I had a microscope and all these type of things. So I think all that mixed together. I sort of got into programming through my father. And then you know, then, you know, the scientific side sort of picked up from both my parents, and then you know, I have other relatives who are in the science field. So it's just been, it's just been sort of part of my life for a really long time. So I think that's just something that just happened. I don't know, I don't think I venture out and look for just something that just happened to me and I just, I just happen to enjoy it.

Sanyam Bhutani  6:24  
Okay. And talking about science, you also worked as, as a researcher, I think, contributed during your undergrad days in research labs, and you were way ahead of the curve. You were already interested in 3d perception problems. You were working on problems that were even not highlighted before the AlexNet Boom. Can you tell us more about your research ideas and experiments from your college days?

Navdeep Gill  6:48  
Yeah, sure. So um, yeah, I worked in a few labs. Just all that getting, you know, I thought I would go to a PhD program. So one of the pieces of advice that I got was, you know, you can take all the courses you want, you can get the good grades, but it's really good if you can get a couple publications out there, that's sort of that sort of showcases your skill set a little better than, let's say, an A, in some mathematic course, or psychology course or something like that. So I initially just started reaching out to different labs saying, you know, I can come in a couple times a week, you don't have to pay me, I'll just, I'll just show up and others do work with you. We can write on a paper. Or we can, you know, I can just put something on my resume that shows I've learned some new skill and you know, this is something I can potentially apply in a future graduate program. So the simplest lab to join first was the lab just that my advisor ran and he ran a lab that had to do with measuring human attention. So it was a lot of behavioral experiments that we did. So this involves a lot of MATLAB programming, which sort of helps design the experiment make the experiment because a lot of times when you do attention experiments in psychology, it just involves the participants doing some tasks on a computer. That task has to be programmed. So then so that was my sort of responsibility of the lab is to put these tasks together and make sure they run accordingly to collect the results efficiently, they produce the results in a, you know, little a quick manner, and then, you know, analyze the data. And then we would work on some manuscript together. So that's sort of where I started with those a lot of the research ideas was sort of stuff they, my advisor worked on during his PhD, and we sort of branched off from that. So we would look at things like, you know, what happens to human attention when they get distracted. So this was during a time when California was trying to pass a law about not using cell phones while driving cars. So now it's sort of a norm across the United States back then it was still being debated. So then, you know, there were people at places like the University of Utah, who were who were, you know, analyzing this type of thing, and we were analyzing it too. And basically, it's what you would expect if if you are distracted when doing some task, your performance degrades on that task. So it was we just had a simple task. You know, no one's life was on the line, of course. But if imagine driving car and you're distracted, that can cause a lot of damage to a lot of people. 

Sanyam Bhutani  9:14  
Yeah.

Navdeep Gill  9:15  
So we're able to showcase this through different conference presentations, things like that. And that was sort of my first glimpse into doing some type of research. And then you know, that's, so at the time, I thought I would go into maybe clinical psychology at that point in time, I decided to maybe shift to cognitive psychology more of a experimental psychology, which is sort of that's where, so in that field, that's where stats math and like statistics sort of overlap with each other because you're doing a lot of data analysis, a lot of experimental design, which is key and statistics. So that's sort of how I got into into that path. And then based off from there, I started working in other labs as well, like UCSF started working at Smith Kettlewell I-Research Institute, and those two places I picked up skills and MRI imaging and electrophysiological experiments. So you can see I was just trying to get my hands dirty in a lot of different aspects of neuroscience and I was trying to learn as much as I could. And I have my advisor to thank for that, because he said, You know, you're at a point where, you know, you're pretty young, and you can just try a bunch of different things. And it's not a big deal if something fails, because it failed. It's okay, you haven't invested in a PhD program of any kind, you need to sort of go out there and understand what you like, what you don't like. And based off of that you can sort of apply to the programs you like, when you apply to these PhD programs at which I thought of applying at the time. It's not really about the university or anything like that. It's just about the problem you're going to tackle in graduate school and who you will work with doesn't matter where they are. And that's sort of the key piece of information I got from doing that. So, yeah, based off of that, I was able to pick up a lot of different skills and you know, that sort of led me to figuring out what I was interested in and at the time doing all this, I sort of fell into being having interest in you know, attention, human attention, cognition, vision type research. And, you know, across all this type of research, one thing that always comes up with aging. So one thing I sort of add to when I was at UCSF is they did a lot of research the lab, I work for that a lot of research about aging and what happens to your cognitive abilities with aging and what can we do to improve these cognitive deficits that happen with aging?

Sanyam Bhutani  11:30  
Okay, I think many people miss out on this point that it your passion should really be curiosity driven and not market driven. You shouldn't go out and see hey, data science jobs are paying much higher. So Should I just go out and chase that. Who knows by the time you're an expert if that were true or not, or if you live in enjoyed.

Navdeep Gill  11:49  
Yeah, I agree. I think. I think definitely curiosity sparks a lot of passion, a lot of people's lives. And I think that's what people should really aim for when it comes to what their interests are. The cool thing about data sciences is you can say I want to do data science and apply to many different fields. So you can, you can have an interest in let's say, I don't know, medicine or biology, Bioinformatics, chemistry, all that stuff involves analyzing data and all of that. All those fields involve design, designing certain, or framing certain questions, which is what data science is really about. You know, that's what statistics is about, to you know, john tooties says, you know, famous quote, you know, the great thing about stats is I get to play in everyone's backyard. So I think that's the beauty of data science, as well as you get to sort of take the analytical skills you have and applied to some passion you're interested in based off of some curiosity you have, and I think that's sort of the key. key takeaway, I think with with how you should pursue certain things in life. [Definitely.]

Sanyam Bhutani  12:53  
How do you feel about the fact that when you working on research problems that were unsolved at the time, for example, right now, we don't even have computer vision in getting driverless AI, it will be out soon. But whatever problems you were working on back in the day can now just be solved with a click Launch experiment. And back in the day, those are unsolved. How do you feel about that?

Navdeep Gill  13:13  
I think I think they're solved in the sense of doing the task itself. So we don't have to, you know, that's a program like some network all the time or some GBM model the time or some decision tree all the time. But I think the thing that keeps science going now is all these tools enable people to tackle a lot of different problems, and it sort of lets, whether it be scientists or people working in, you know, law or anything like that, sort of lets them think about the problems a lot more, maybe come up with more questions to ask, instead of, you know, spending more time on just performing some mundane task, whether it be you know, just putting tuning parameters or anything like that. I mean, it could be interesting, but same time, it could be pretty mundane, if you rather just be thing about, you know, the proper question to ask. So I think a lot of these tools, the analytical tools we have now like driveless, ai, like all these automail type tools, they sort of enable people to ask more questions, and enables them to experiment a lot faster. And then based off of that, you can sort of solve problems better, you can probably fail quicker, and figure out what to do next. The thing I noticed was when I was in neuroscience, or these are psychology, that was spending a lot of time you know, designing experiments and then putting a lot of stuff together. But you know, the real work is done figuring out before you start anything is even worth investing in. So with the data science tools that are coming out, I think you're you're able to figure out a lot quicker if if it's worth the time to invest in a certain problem. And you might find out quickly that maybe your data set is just not well equipped for this, but you can find it out in a fast manner and then sort of make the proper steps to fix that. So I think that's sort of the benefit of these analytical tools. Coming out nowadays specifically like Driverless AI and those type of tools

Sanyam Bhutani  15:04  
Coming to where you are contributing to help humans answer the bigger questions. Can you tell us more about your current day? Like what is a day in your life currently look like and what problems are you working on at H2O.ai?

Navdeep Gill  15:17  
Yeah, so currently I'm a working with the Machine learning interpretability team, h2o. So we offer MLI as we as the acronym we use, and we offer that through driveless ai. So it's a way to, you know, sort of interpret, explain your model, debug your model. So the other things I'm working on nowadays is sort of thinking of what type of applications or algorithm that we can apply to sort of dive into the model itself and sort of piece out information that we need from the model to understand if it's doing what we expect. So you know, day to day I could be most of the time I'm like, usually programming something into the MLI module that we offer in Driverless AI, because obviously you know, sort of like a software engineer, you're still doing you know, bug fixes to Are these types of things? My colleague Patrick Hall and I, we work on a lot of different blogs together a lot of papers we've been working on recently, booklets, things like that. So that's sort of my day to day is, you know, sort of roadmapping what we think we should work on, and then, you know, just working on it throughout the day.

Sanyam Bhutani  16:20  
Yeah. Okay. Can you tell us more about machine learning interpretability? Why is getting SOTA accuracy? Not good enough? Why do you need to interpret models, Where is it required and important?

Navdeep Gill  16:32  
Yeah. So I think when it comes to machine learning itself, yeah, you can build out a great model. That's pretty accurate. But the model itself is inherently bias. There's certain biases that are that are in that model itself. And you sort of need to uncover those in sort of these applications. Specifically high stake applications. And by high stake, I mean, things like, you know, if it has to do a medical diagnosis or has to do with fraudulent activity, you're trying to predict some type of fraud or you're trying to predict somebody should get a loan for a home or someone should be denied credit or accepted for credit for a certain credit card or loan application. I think with these involves a little more due diligence from the data scientists or from an analyst or business stakeholder at that point, it's not enough to just build a model and, you know, take it, you know, as it is, and accept it for what it what it's doing. And basically trust it. I think, when anyone's building machine learning model, you should have some doubt when you build that model. And you should try to take your domain expertise or someone else's domain expertise, maybe the you know, maybe they know more than you are, which is usually the case a lot of big organizations, you sort of want to take that into heart when you build these models and make sure that the results you're seeing actually do make sense. You do need to do the due diligence of you know, exploratory data analysis, and then post hoc analysis of the model to see what the model deems important and what and what you think the moderation team is important. You sort of want to, you know, go from a global level of looking at the model Globally, and you want to look at what it does for some groups of observations. Now, these observations can be people or anything like that. But you want to see that the model is sort of looking at everything in a uniform manner. And if it's not, you want to understand why it's not looking at everything in a uniform manner. Why is it treating these groups of people differently than these groups of people, this specific row versus, you know, the entire data set? This involves a lot of work, I think, from the data scientists still. And I think you can try to automate the process itself, but at the end of the day, you need a, you need a human in the loop to sort of look at everything. And then so it's not enough just to look at accuracy. I think I think accuracy is probably like step zero, like you're just looking at, you know, is this model even good at classifying something for me? Okay, good at classifying something, but does it make sense? Is it picking out some, is there some leakages that I'm not seeing and it's not making any sense or is it picking our variables that are important that should it be? So I think sort of That's where I'm alive comes into plays. It's sort of this process of doing machine learning in a responsible manner. Especially when it comes to high stakes applications, you really want to make sure that what you're deploying is, you know, fair, what you're deploying is not biased towards anyone. Because if it is, you're going to run into a lot of issues for yourself, for whatever team you're working on, and maybe for even your organization. So I would, I would think that's where I'm alive comes into play for sure.

Sanyam Bhutani  19:28  
We've already seen such issues already recently coming up in the news where banks have had issues of the processes that they automated using machine learning. Where do you think MLI comes into the picture of auto ml? Because automl essentially, aims to replace the complete human in the loop Where would MLI come in that picture.

Navdeep Gill  19:47  
Yeah, I think I think embolize you know, it's going to be, you know, sort of the last step you do before you decide to deploy something. I think automail you know, with tools like driverless AI, and then you know, I think Google has tools coming Things like that. I think those tools sort of are doing a pretty good job at automating the model building process. So we're, you know, before grid searches and things like that people were just sort of tuning models, and then trying to figure out how to make this model pretty accurate. And that took a lot of time. I remember before, like, you know, before being introduced to grid searches, you would spend a lot of time just trying to model and figure out, you know, what parameters you should use, cross for validation, all these type of things. But a lot of that stuff is automated now, especially with tools like driverless AI. So with with automl, I think it allows people to experiment more, build more models in an efficient manner. And then that allows them to spend more time on debugging the model, explaining the model, figuring out what the model thinks is important. And, and, and looking at the stability of the model per features, you know, like partial dependence, things like that. I think spending more time on that as well. automl enables, it enables people to sort of critique the model a lot more, you know, in the past, if you spent a lot of time tuning the model, and then you spend a lot of time debugging the model explaining the model, the whole process could, you know, really hinder a project. But nowadays, I think we all know you can do the modeling aspect pretty quickly. And that sort of diminishes the time it takes to tackle a data science problem. So you can spend more time you know, in the, in the pre processing of the data, exploring the data, and then once your data is is, you know, set you can you can just turn out many experiments with a tool like driverless AI, for example. And then you could spend a lot of time trying to explain that model and see if it makes sense to you. And then you know, that that loop is sort of getting quicker for people I think, with auto no tools. So I think so I think where MLI comes in, it just comes in as another step that people could spend more time on. Now, instead of glossing over you know, the feature importance or local feature importance or you Sort of the reasons behind why a model made a decision, you can spend a lot more time on it, you can critique it a lot harder. And I think that just makes for good science in general.

Sanyam Bhutani  22:10  
Like you said, it'll allow us to look at the bigger question then have the automated process already been handled?

Navdeep Gill  22:17  
Yeah, exactly, exactly. I think it allows. It just allows people to experiment more, formulate better questions, you know, you know, it's called Data Science and, you know, sciences in the title. So I think, without our tools, we should let you know, data scientists focus on the science focus on making better questions focused on collecting better data, you know, those type of things are what data should be focused on focused on and the model tuning should be, you know, it has been shown to be automated, and then we should take advantage of that. And we should, you know, model or build as many models as possible, and then try to, you know, debug all of them and then based on that you can pick the best one that you think is whether, you know, you could think it's fair and accurate you know, so I think I think that's where I think that's where we're heading this point.

Sanyam Bhutani  23:03  
Definitely. So you mentioned a bunch of ideas or research, open research questions that are present for me, like, can you tell us what all are already being handled or are being have been solved inside of H2O products? And what can we currently do using any of the H2O products?

Navdeep Gill  23:20  
Yeah, so with the MLM module today, like using Driverless AI, you can do a lot of different things. So you know, globally, you can get things like feature importances, you can look at Shapley value, which is a more accurate way of looking at feature importances. You can look at things from a local perspective. So this is looking at clusters of data versus rows of data versus the entire data set. You can do sensitivity analysis and MLI today. So you know, that's a pretty powerful tool that basically involves you know, perturbing some raw data or collection of data and seeing how the model actually responds to that. So that goes back to what I was talking about earlier about having some stability in your model. You want to see when does your model really break down? And that's where sensitivity analysis comes in. You know, it's the topic that's been using a lot of engineering fields, you know, you simply apply to a model. So in engineering or chemistry, but there's like these physical models, and you want to see how they perform, well, now we have a machine learning model, you can do the same, same thing, you just take the data, you know, change something and see what the model does and how stable it is. And then we offer things like disparate impact analysis, which is a thing used in a lot of regulated industries. And that basically involves looking at certain metrics, like true positive rate, true negative rate, false positive rate, etc, across different demographic groups. So let's say you built a model, that's predicting whether someone should get a loan or not. And in the model you if you're in a regulated industry, obviously, you did not use demographic variables if your model is just not allowed, so you can't use something like gender or race or anything like that. But the problem is, is those variables can be correlated to certain things. So in doing disparate impact analysis, you can sort of look at your model post hoc and see Is there any significant differences in these methods? It's like true positive rate, or false negative rate across, let's say gender. And if we raise, you can sort of, you know, detect that quickly. And you can sort of remedy that. In in MLI, we also offer things like LIME. So but we call it k-LIME because it's a faster implementation, you know, this involves, you know, you have your complex model, your complex model has these has, you know, the white hat or the predictions, and we can build simpler models, which are called survey models to that complex models predictions. So you can build a global let's say GLM to the predictions of your complex model, and in K-LIME, the K is stands for the K means process. So, you can cluster the data based off of K means and then you can build subsequent TLM for each cluster. And then based off of that, you have reason codes, because dlm DB coefficient is a linear model. So, you can look at the reason codes and you can be confident that you know, if I take this, this beta times this value for my rho plus, you know, the rest of The beta times the rest of the values for the robot can get the I can get what the GLM predicted, and then you can get your predictions that way. So it's a way of coming up these reason codes, and the reason codes or something from regulated industry, specifically banking. And and basically what that has to do with is if you're denied credit, for example, in the United States, the bank sort of has to tell you why you were denied credit. So these are called reason codes for your denial. And it could be things like, you know, you know, your, your, you've been, you've defaulted on a lot of payments in the past, I don't know, two, three months. So in MLI, we can give you these type of reason codes, for any type of model doesn't have to be specifically banking, we just call them reason codes, because they're the reason or potential reasons why the prediction was the way it was. So we offer that today through through driveless. Ai and then we offer are also offer other survey models. Basically, it's the same concept, you're building a simpler model to the predictions of your complex model, and you're trying Explain what the models doing. So this is a pretty powerful tool when it looks at when we try to see if if other models can sort of try to understand what your model is doing. And it sort of helps, you know, sort of debug a little, little bit more. Because as we know, you know, there's like the data set you used to model your problem that sort of give some it's not really reality is just a sample of reality. And then your model is taking that sample. And it's building its own reality and what it thinks is going on. So we're the surrogate model comes in, and is it is trying to dive into what's the reality of your complex model that you want to deploy? What does this model think? What sort of features I think are important? these type of things, and the simpler model sort of helps humans understand it a little better. So I think that's, we offer a variety of tools today throughout my life that we worked on for the past few years. It's I think it's one of the one of the few commericial tools they're offered today. So, you know, it's pretty nice that no one has to sort of import a Python library and try to do it themselves is sort of out of the box if you have travelers there. Yeah.

Sanyam Bhutani  28:11  
So you mentioned the ideas that haven't been solved. But there's now this push towards having more complex models. We have Transformers in nlb, for example, and this is also push for interpretability. What's your take on that? How interpretable will models be as they continue getting more complex with more compute power pouring in?

Navdeep Gill  28:31  
I think I think a lot of people are going to start to sort of figure out that they need to have more interpretable models. In the future, people have already started building out interpretive bull models today. And what we're starting to see is there's so for a long time, people thought there was a trade off between accuracy interpretability, but what we're starting to see is that's not really the case anymore. So there's a lot of interpret models out there, that people can use you There's models coming from Cynthia Rutan's group in Duke Rich Corona and Microsoft, he works on a lot of interpretable models. And I think that's what people are going to start to notice this is key, especially in regulated industries, where where people, you know, do want to accurate models, but they probably stick with dlm or interminable GBM, like monotonic gpms, because it's sort of the safer way to go about things you sort of avoid. Not being able to interpret a model, and, you know, using a model like that. So I think that's where I think that's where people are going towards, right now. Even if you build a complex model, people are still trying to build tools that can explain these complex models. So a lot of people are building on tools that can explain deep neural networks. People working on tools that can explain certain NLP models. So you know, I think like every other week, I see a new NLP architecture or something like that. But yeah, you know, architectures need to be explained and I think, in tandem, people are working on those both For example, a senior senior at UC Irvine, he's, he works a lot on NLP. And he works a lot on MLI, and especially how that's applied to NLP type models. So I think groups like that, like similar things group, there's groups up at the University of Washington, like I think they're they're sort of seeing what a lot of other people will potentially see is that you can build out these complex models, but you need to understand them, especially if they're going to be used in a lot of applications that have, you know, affect a lot of people, I think they should really take a look at it. And I think they will. I think people understand that.

Sanyam Bhutani  30:38  
Now, many of the audience might be feeling intimidated by by all of this conversation, I want to mention a resource that has been authored by you. book to introduction on interpretability. It's for free so you can find the link in the description if you want to go ahead and check it out. Why Why did you decide to author the book? Were there any other resources What led you to because writing a book takes a lot of efforts. Why did you do it?

Navdeep Gill  31:05  
Yeah, so um, at the time, MLI was not being worked on at h2o when when we first wrote the booklet. So, Patrick Hall is the is the main author and I co authored with him on on the booklet. And when he joined h2o, one of the first things he was working on was machine learning interpretability. And I remember, at the time, you know, this was a few years ago, we didn't really have jobs. At the time, I was just starting out. And I was pretty intrigued by MLI, I I didn't know too much about it at the time. I knew certain methods, but just not all the methods that that were coming out in the literature. So I knew certain things like you know, there's PDP is a partial dependency, ice feature importances, all those type of generic things that we've seen. But, you know, at the time papers like the line paper came out, so that was a big deal. So I was pretty intrigued when when Patrick We're sort of building out this team. And I was asked if I would, you know, sort of join or help out. And I agreed i wanted to i was i was i was interested. So we started, we started working on MLS together. Initially, we thought it might go into HR three and and we, you know, thought, well, maybe we're going to try this guy. So we decided to just go down that route. And then one of the first things that we're doing is, you know, writing about it writing about MLI. And then Patrick had a few articles on O'Reilly, about MLI, and then he was offered a chance to write a booklet and he asked if anyone would like to co author and at the time I thought, well, I think this would be interesting because I think this would be a good way to learn about MLM because it would involve doing a lot of research reading a lot of papers. So I thought it would be beneficial to myself to do something like this. And also getting a book that out there is pretty interesting to see. You know, anyone read it, you know, I didn't know if anyone would be would be interested, but I'm glad you know that turnout has been great. So that was sort of my reasoning behind doing this booklet is I thought it would just be a great learning experience for myself sort of forced myself to read the literature, looking to the tools that are available today. And you know, from an engineering standpoint, it was it was sort of like this competitive analysis, like what's out there, and what can we use and what should be built to into our tool itself, to sort of enable customers.

Sanyam Bhutani  33:25  
Okay, now, coming back to your work and your journey at H2O: you joined, I think around a few, I think four years ago, how has your work evolved over the years even Driverless AI our flagship product didn't exist back then what problems were you working on? And how is your work your word for these years?

Navdeep Gill  33:42  
Yeah, so when I, when I first joined H2O, it was sort of the end of 2015 seems like ages ago. That's when I joined and back then, we had we obviously at age 12, three, which is the open source software and we had had sparkling wine Water, which is, you know, embedding h2o three in Spark. And at the time, we were also coming out with steam, but that was sort of in its infancy at the time, if I remember correctly. And back then what I mainly worked on was, you know, I was hired as sort of this, I think my title at the time was like hacker scientist or something. And I was basically working a lot on sort of data science problems with customers. And I was also working a lot on quality engineering. So, you know, a lot of testing was happening back then. And I was sort of helping out with that with the rest of team. And I was also working on these two or three. So the core platform, I worked on that for a couple of years in the beginning. So there's involved things like implementing new new data science methods per se, mainly like exploratory data analysis type methods involved, adding minor features to certain bottles that we offered bug fixes, testing, lots of lots of testing, and presenting at conferences, building out packages, so like, you know, one of the packages I worked on was sparkling, which was adding, you know, sparkling waters are API, things like that. So it was a lot of working on open source, a lot of the open source tools at the time I was working on. And this was before. Yeah, obviously before Driverless AI came out. But back then our flagship product was h2o three. And that's what we offered to a lot of people asked what mainly everyone worked on. At the time, I think the team was mostly engineers. Now, it's sort of a big mix of sales marketing engineers, but at the time, it was just a group of engineers in the Mountain Dew office, and we were all basically working on H2O three or sparkling water or a little bit of both. And, yeah, that's what I that's what I mainly worked on back then. And that's what mainly everyone worked on back then to be honest.

Sanyam Bhutani  35:43  
You've also been a big supporter of open source over these years. Can you tell us about your open source journey and beginners usually feel very intimidated about contributing to open source because you come and there is this huge repository of wonderful code that's been written by someone who advice would you have for them? Yeah,

Navdeep Gill  36:03  
yeah. So yeah, with the with open source equal three specifically, I got introduced to H2O-3 because of open source. And that's sort of how I got into it. So at the time I was working at, I was working at Cisco at the time, and I wanted to use some Machine Learning Library that was pretty powerful. So I'd set up a show three, and then I realized, Oh, it's open source. That's pretty interesting. I didn't I never thought about that. Before that it's a it's sort of this. I mean, I have seen open source machine learning libraries, but not one is run by a company, per se. But at the time, you know, it was like psychic learn actually boosts all these types of things. were run by a group of people from like, like academic background, and I like a university. And then here's, here's h2o three, which is backpack company, and it's completely free. And that's sort of where I got intrigued by open source software, per se. I thought, you know, it's a pretty powerful idea to sort of open up to software, let the community contribute, let the community use it for free and sort of use that to enable improvements to your public itself. So that's how I sort of got interested in open source. And I think your second question was about, you know, people are intimidated by contributing to a project, anything like that. A lot of the good projects are pretty good at having issues open that are meant for newcomers. So my suggestion would be, sort of try to find these issues. And if you can't find them, you can always talk to or you can get on Twitter or, or just contact one of the main contributors and ask, you know, what are some issues that are for newcomers, like I see you have a pretty large code base here, I don't really know where to start. And then they might point out some issues that you can work on. And then based off of that, you can just start working on issues and eventually you'll just start, just start picking up, you know what to work on. You can just start working on it at your leisure. That's the that's the other beauty of open source. I guess there's really no like deadline or anything. It's just sort of work on it. You know, for fun here you're working on because you enjoy the work that it offers. doing the work that you do. So I think that's a key piece of advice is you can always contact the contributors. I think the fact that they are open source sort of means that they are open to communicating with the, with the rest of the community itself. So there's no shame in reaching out and saying, you know, I want to contribute, how can I contribute, they might say, Oh, you can do XYZ. Or they might say, oh, here's a few issues that I've opened that are pretty straightforward, and they don't involve you to be a current member of the contributing team. So that's my little two cents there on what on what people should try to do if they're if they do want to get involved.

Sanyam Bhutani  38:38  
Okay, I'll definitely have our H2O-3 repositories Linked In case anyone of you want to go ahead and check out those who are contributing. Now coming to your title, which has a role from hacker scientist to software engineer slash data scientist. Now this is a very interesting intersection, because these are two contrasting personalities, one which writes really good code and the other is known for eating People's and sitting in the middle of a GPU cluster writing bad code. What is this intersection important to you?

Navdeep Gill  39:08  
Yeah, I think.

Navdeep Gill  39:11  
I think I think if you write pretty good software, like pretty good code, for example, that leads to a lot of a lot of reproducibility. So allows people to let especially if your codes open. So a lot of a lot of what a lot of people are doing now a lot of data scientists or people are in machine learning of sort of publishing their papers, and they also have code now that goes with the papers. That's something you didn't see in the past. You would see maybe like a paper, I think like think about like the paper by GBM from Friedman, most likely, maybe there wasn't. I don't know if there was a software pack at a time, but most likely if it wasn't for like, papers about gmms from like the 70s or 80s. Like there wasn't any open source software available that someone can just take a look at and see what this person did. Nowadays, I think I think that's, that's more of a norm, people are doing that now, especially if papers are published archive or something like that. And then Patrick Hall and myself and Kim Montgomery from h2o and an outside partner, we're working on a paper that's just like that. It's called, we're gonna hopefully be published soon. But basically, it's a paper about in trouble machine learning in Python with open source examples. Now, if you write, if you write code that's that you can't comprehend. No one can take that code and use it. If you write code can't be compiled correctly, or you need to do a lot of crazy things to make it work, people won't use it.

Navdeep Gill  40:33  
So I think if you go about

Navdeep Gill  40:36  
practicing, both of those are practicing, you know, good programming and plus data science, I think is a powerful tool to have. And that's what I, you know, try to achieve in my in my day to day work. I think it just leads to good science, particularly that that was sort of what that led to and then the reason I think my title is the way it is because I do work on a little bit of both. So like, some days, I'll be Working on maybe more of a data science problem, which might involve, you know, writing about some method that we're working on or Jupiter notebook, or implementing some data science algorithm. But other days, it might just be pure software engineering type stuff, like working on the continuous integration, or working on some improvements to the software processes. We have testing all those types of languages, more traditionally, software engineering. So I think that's where the two overlap for me. But I think in general, I think most data scientists are probably leaning towards writing better code, which could accompany their, whether they're writing a paper or work related things, and that can lead to more reproducibility of results. And based off of that, it'll just lead to better you know, practices of data science, I think,

Sanyam Bhutani  41:53  
Definitely. Some data scientists try to get away with not so good code. I am particularly not very much Proud of the code that I put out, but where should we draw the line? If for example, a data science enthusiast is more focused on learning starts and the concepts around it, how much of effort should they put in, in learning software engineering skills? For example,

Navdeep Gill  42:15  
I think you should put a lot of effort into your skills,

Navdeep Gill  42:18  
the the [ok],

Navdeep Gill  42:21  
so you can learn the fundamentals. Yes, I definitely agree, you should definitely learn the fundamentals of statistics, mathematics, you know, all those types of things. Those are important when it comes to you know, reading the papers, sort of understanding why certain methods work the way they do. It gives you the necessary background. So, I think those are definitely good to have. The thing is software engineering, I think takes a lot more time to to sort of, I don't know the correct word new master or or achieve the skill set needed to it takes a lot of practice, I think, at least in my, in my personal endeavor with with software engineering. It takes a lot of practice. So in college, you can take a programming class, which like many people do, I did you, we take the intro to class, you take the intro classes, you learn about data structures, algorithms, and those give you the necessary background to, to sort of put software together piece things together. But anyone can piece things together. I think, I think what if if those people out there who are aspiring data scientists, and let's say their stats and machine learning program where they're doing a little bit of programming, I think they should focus on more of the programming side more of the application side, because that will open up your opportunities in the future. Because every data scientists should have an understanding of, of statistics and mathematics, especially given that's that's what's going to be in their background, whether it be formally or informally, like they maybe took a online course or something, or they got a college degree in it. But the programming side I think is important because you know, a lot of companies are interested in making data products are interested in ETL processes, you know, extracting data, cleaning up the data, making applications with the data, a lot of that stuff involves software engineering skills. So I think you can be ahead of the curve a little bit compared to the rest of the people trying to get these type of jobs if you can, let's say put an application together from scratch. And not just an application, but maybe a pretty efficient application together from scratch. That's a pretty good thing to add to your resume. add to your GitHub profile, for example. Because this showcases your ability to put together data science ideas like statistics, mathematics, with software engineering skills, and like I said earlier, those take a lot more time to put together. I think,

Navdeep Gill  44:47  
compared to

Navdeep Gill  44:50  
just learning the basics of statistics or mathematics. A lot of times in the field, you'll do a lot more programming than you would and some people might like so it's Hi better to learn that initially,

Sanyam Bhutani  45:03  
you mentioned about building projects, many people now who are trying to make this shift, just learn to online courses, take human programming courses and followed by the resiliency courses, how should they find these ideas? And how should they think about what projects to build to showcase to recruiting managers or companies?

Navdeep Gill  45:23  
Yeah. So I think the first thing with like a data science profile is maybe you can sort of think of a problem you want to tackle first. Because a lot of times we get hub now, I think everyone who's in the data science field should have some type of GitHub account to showcase their work. Or they can have a website, but the GitHub accounts easier, because it's already set up for you. So you can you can set up a GitHub account, you can try to think of a problem you want to tackle, or think of a interesting data set that you want to use. And then based off of that, you can do a lot of different things to showcase your skills. So it doesn't have to be building a machine learning model per se. You can probably do a lot of great exploratory data analysis. What you see now in capital, especially with capital kernels, for example, you see a lot now where it's not really about just building the best model. But people are getting credit for building interesting findings or finding certain things in the data set. So by using a lot of great uda, so that's one thing that people can add to the resume is sort of showing how you can explore data in a in a, in a proper manner, and which is what data science is, for the most part, I think, for the most part, data science has to do with cleaning up the data properly. And also exploring the data properly. I think those are very important skills to have. And those are very important skills to showcase. Because if the modeling is you know, getting automated, or getting to the point where you can turn out a lot of different models is not as impressive to see someone just call some model on a data without doing anything to the data set itself. So you know, eta is one of the important key things I think someone should show on their resume and then we can take them any longer. Courses about that. A good one, you know, is probably anything offer about like course era, things like that maybe MIT has a lot of free stuff from their from their online courses. Stanford has a lot of online courses. And you know, another good thing is you can always look on GitHub and see what people are working on. And then sort of take some inspiration from that. I mentioned cago kernels earlier, that's a pretty interesting one to look at as well, because especially if you look at some of the top ones, that people do a lot of interesting results on there. And then based off of that, you can get a lot of ideas to apply to other data sets. So I think that's, that's where people should try to focus on this focus on, you know, the process of data science, which is a lot about dealing with data. So that's where you take like, you can improve your programming skills when it comes to dealing with data, you can sort of build out ways to explore the data, find some great insights into the data, and put that into a nice, you know, maybe GitHub project. You could just run Jupyter Notebooks or something, sort of write out what you did. And then that's something you can show someone cuz that's something you actually did. It's tangible. You can take that and take it to a hiring manager and be like, this is what I worked on. What do you think of this? And you can do this for many different things. It's not the same as just, I don't think it's the same as before, where you can just finish like a master's program, and then get a hope to get a job. You have to you have to show something. Yeah. So in addition, I mean, that's why would like software engineering, interviews, people are getting like take home assignments, or they're given maybe like a whiteboard type of thing. I think the same thing can happen with data science. So I know in data science interviews that, you know, there are the whiteboard interview, where you're given a problem and you got to explain how you would solve it. That's great. But at the same time, there's a lot of issues with that type of interview process. So if you have, let's say, You're just like a nervous person or something like that, and you don't do well. But you have a great, great profile, a great resume of projects you've worked on, that can help you At the end of the day, something worked on something you wrote together, it's something you did on your own or maybe with another person. But at the end of the day, it's something you worked on something you came up with from scratch, it shows your ability to think analytically, critically. All these things can be showcased nowadays, via the power of GitHub or Cavill, or anything like that. So I think that's where people should try to go towards this. You shouldn't think you need some formal education and this, all you need is you just need to gain the skills of like, let's say efficient programming, statistics, mathematics, you need to be able to formulate problems, need to be able to figure out how to solve these problems, and need to showcase these problems in such a way that other people can get their hands on it. And that's just, that's just a good way to go about things nowadays, especially.

Sanyam Bhutani  49:48  
I'd also like to mention, you mentioned Kaggle kernels, which are rewarding if you're a great storyteller and also, if maybe someone might not be as skilled as you to skill enough to write a book. But if you blogging about your data science projects. Those are also very big positive signal to hiring managers or even the community.

Navdeep Gill  50:08  
Yeah, definitely. I, I definitely agree that like writing blogs, maybe even, yeah, writing blogs is a great way. You don't have to go through a review processable journal, anything you can. And like the like the Cavill kernels itself are a great way to showcase your storytelling because, you know, that's what data sciences is. It's mostly storytelling, you sort of with your, I mean, you're telling a story with the data, you're trying to figure out what this data is telling you. And cowboy kernels have opened up a lot, a lot of opportunities for people. I think. GitHub is also a great way to to showcase it as well. And yeah, writing blogs, or I think some people even do like YouTube videos of how they went out went about solving the problem. A lot of times, like a lot of the cool things is when people win competitions and then they have a whole blog about what they did. I mean, that's a pretty cool thing to show someone. Because it's one thing to be in, let's say, like when you go to like an interview for a data science position, there's only a certain amount of time you have to get whatever you need to get out there. And a lot of times, it's based off of the questions the other person asked, would be really nice if you can just point to a blog that you did, or point to some some, like notebook you made or some analysis you did, and just be like, this is this is what I worked on. If you're interested, please take a look. I can answer any questions you have about this. I think that's a really powerful thing. I mean, I sort of wish I knew about this when I was applying for jobs in the beginning. But you know, back then maybe was a little different. This was maybe like six years ago, seven years ago, maybe people aren't doing that as much. But nowadays, I think that's going to become the norm like people want to see what you've worked on. So you need to sort of put it out there. I know it's hard for some people because you might be working on projects that work that you can't share with the world. Yeah, that's totally understandable. And at that point, you should probably figure out a way to explain that to people in a efficient manner. But if you do have the time to do these open source type things, these open source type of analyses, I think it's, I think, is very powerful in trying to find other positions, if you're, if you're interested in jumping into data science.

Sanyam Bhutani  52:22  
The other point is blogging or even working on kaggle competitions, sets of these chain reactions. I've seen many competition winners or maybe even the top 10 finishes writing even research papers, people very like you and you said, contributing to research opens many doors. So even if you produce a very novel solution that might not have won the competition, even that could also lead to very interesting research results.

Navdeep Gill  52:48  
Yeah, I think

Navdeep Gill  52:51  
I think it can lead to interesting research, these two disorder blogs that people put out and things like that. It's um, it's Sort of sort of opens the door for a lot of different research. And I think the blogs itself. I mean, I find them very interesting at I think they're, I think they will raise a lot of questions for a lot of people as well. Maybe they can build off of these type of blogs or maybe another competition there and or something else to work. So I think it's very powerful. Yeah.

Sanyam Bhutani  53:23  
Now coming back to is to I'd like to understand something from you, because I've only been at H2O for three months. But can you tell me more about the maker culture and makers gonna make philosophy that we follow?

Navdeep Gill  53:35  
Yeah, to me, the, to me, the maker culture, I think has to do with giving people that work in a company freedom to work on problems they're interested in. So I always found that I always loved that about each two. I was like, a lot of the problems that I've worked on here, a lot of the teams I've worked on is sort of, you know, in my own choice to a certain degree. I could, I could, I could say, you know, I'm interested in working on this. This problem. And most of the time people were okay with it ended let me do it as long. You know, as long as it wasn't too out there, you know, they would they would let you work on it. You know, with MLI like, like I was interested to work on MLO, and you know, I've been able to stick with it for for a long time. So I think that's sort of the maker culture is sort of giving, giving the employees of the company freedom to work on what they want to work on.

Sanyam Bhutani  54:24  
It could even be drinking chai and producing podcasts

Navdeep Gill  54:28  
for those who are curious. Exactly, exactly. I mean, I think I think I think the maker culture has to do with, like a lot of people has to do with having the curiosity to go after a problem. And then if you're curious enough, that leads to some type of passion. And if you're passionate about a problem, you're going to you're going to spend a lot of time with that problem. And that sort of makes for good work at the end of the day. That's why you know, it's the same concept was like, you know, people in academics were working on problems me don't know the problems they chose to work on. Why not apply that to industry as well, you know how to get the right people to work on the problems they're interested in. And then hopefully, from that, you'll get a lot of great products, a lot of great output, and a lot of interesting findings. And I think that's what the maker culture has to do with specifically h2o.

Sanyam Bhutani  55:16  
Awesome. Now, you already shared many great advisors. But if you were to give any final best advice to someone who's just starting out who just signed up for an online course or maybe just thinking of doing the first open source contribution?

Navdeep Gill  55:30  
Yeah, I think

Navdeep Gill  55:33  
what someone should do, if they're just starting out, is they should take the online course or whatever they're doing, and sort of document what you've been learning throughout the process. So document in the way that, you know, it could be something as simple as, Oh, I'm taking this online course, on machine learning, I'll make a GitHub. I'll make a GitHub project, but that's called this online course that I'm taking whatever it may be, like intro to machine learning or something. And then each you know, then you put down all these assignments you worked on things like that, in that in that project itself. And then what that catalogs will have a history of your thinking process that we've been working on. And then that's good, you can put on your profile as well. Like I mentioned before, you can also work on certain data science problems you're interested in. So you know, get your hands on a data set you're interested in or, or work on a problem you're interested in, and get the right data set to tackle that problem. And showcase your thought process through through your online profile, your online presence, whether it be if you have a personal website, showcase it there, you have a GitHub account, their capital, colonels, whatever you whatever you want. I think that's what people should be doing now from the beginning. And you see that in a lot of you see, that's how a lot of like data science boot camps work, actually, if you get things like or if you if you if you look at the assignments they do, a lot of times it has to do with something the student came up with themselves. And that's something you could say Basically going through that same process there, they're saying, you know, we're going to teach you these set of skills. But at the end, we want you to put all these skills together and make something out of it. And that's something you can do outside of a data science boot camp. Technically, I mean, you could just,

Navdeep Gill  57:13  
you know, like I said, it's just the sciences, formulate a

Navdeep Gill  57:15  
problem, collect the right data you need, and then tackle that problem. And if you if you're able to put that together in some type of document, whether it be a blog, or just something like PDF Doc, or just a very, very well annotated notebook, you know, that's something you can show someone. So that would be my piece of advice of advice to anyone getting into machine learning now is you sort of want to show what you know, and you want to show what you've been learning. So I think that's, that would be my piece of advice, especially if you're coming into industry, academics might be different. You might need to showcase like, implementing an algorithm or something like that. But when it comes to industry, I think I think that's what people are mostly interested in. This is your thinking process and how you go about solving the These type of problems that you might encounter. Yeah.

Sanyam Bhutani  58:03  
Awesome. Now, before we end the call, can you tell us what would be the best platforms to follow you and follow your work?

Navdeep Gill  58:10  
Oh, the best platform so you can follow me on Twitter. I usually am on there and LinkedIn. Let me see what else and then the of course, the booklet itself. It's in a second edition, the machine learning interpretability booklet from O'Reilly and that's available through the H2O website.

Sanyam Bhutani  58:28  
All of these will be linked in the description. So please don't be lazy. Scroll down and find these links. If you're curious. Thank you so much for joining me on the podcast and thank you for all of your contributions to the MLA community and the open source community.

Navdeep Gill  58:42  
Yeah, it's great to be on here. It's guy thank you so much for having me.

Sanyam Bhutani  58:52  
Thank you so much for listening to this episode. If you enjoyed the show, please be sure to give it a review or feel free to shoot me a message you can find All of the social media links in the description. If you like the show, please subscribe and tune in each week to "Chai Time Data Science."

