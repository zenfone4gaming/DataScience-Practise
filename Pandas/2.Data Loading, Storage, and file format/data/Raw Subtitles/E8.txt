Sanyam Bhutani  0:13  
Hey, this is Sanyam Bhutani and you're listening to "Chai Time Data Science", a podcast for data science enthusiast where I interview practitioners, researchers, and Kagglers about their journey, experience, and talk all things about data science.

Sanyam Bhutani  0:46  
Hello, and welcome to another episode of the "Chai Time Data Science" show. In this conversation with one of my peers on the fast.ai community, James Dellinger who has a background in software product management from a startup called Peel, and has been studying machine learning slash deep learning full time for the past two years, and recently fast.ai as a full time student for the past one year. We talked about James' journey into machine learning class deep learning, about image augmentations and the library and image augmentation library called DALI spelled 'dali' from NVIDIA. This conversation is based on Jame's article, titled quote unquote, "Diving into Dali": how to use NVIDIA GPU optimized image augmentation library to which you can find a link in the description of this podcast. Also, during our offline conversation, James mentioned that he's on the job market so please feel free to reach out to him from his contacts linked in the description of this podcast. Here's the conversation all about image augmentations and the Dali library. Enjoy the show.

Sanyam Bhutani  2:12  
Hi, everyone, welcome to this special episode in our podcast series. Today I'm joined by James Dellinger, who's also fast.ai fellow. I had the chance to know him through the community. We're going to be talking about image augmentation today and library from Nvidia that came out recently. It's called DALI. James has recently put out an amazing blog post a deep dive into the library. I'll have the description linked off in the description of this podcast or the YouTube video, please make sure to check it out. And thanks, James, for joining us today.

James Dellinger  2:49  
Yeah, thanks, Sanyam. Thanks, Sanyam. It's so great to be here and excited to chat about it.

Sanyam Bhutani  2:54  
Likewise. So let's let's talk about Dali, like from Nvidia's description page, it's sort of a image augmentation library specifically like they mentioned it to be a data augmentation library.

James Dellinger  3:10  
Sure. 

Sanyam Bhutani  3:11  
Can you tell us more about your understanding of it? And, like, we can also discuss why are image augmentations necessary?

James Dellinger  3:20  
Sure. Um, well I guess I'll start kind of specifically with Dali first and then we can maybe back up a bit to image augmentation in general, um, why I jumped into Dali was I was going through the fast.ai notebooks from the most recent part two, the 2019 version of fast.ai part two. And I've just been going through notebook by notebook, and I got to the image augmentation notebook. And through that, Jeremy and Sylvain, kind of walk us through how to implement kind of the basic most important image transformations such as random crop and resize or rotation or perspective warp;

Sanyam Bhutani  3:59  
That's from scratch, right? 

James Dellinger  4:01  
That's from scratch, we use, we use pillow for a lot of those. And what we found was that pillow works really fast on like, one image at a time. So it kind of like, I think, definitely how I've been doing my image augmentations before using open CV or pillow is it as far as I'm aware, it runs on the CPU, and it does image by an image in serial. And that's not an issue if you have a small enough data set or image that, but once you start trying to train like image net or a data set with, you know, potentially like a million images, and if you're going to do it from scratch, you'll start to notice over time that you're spending a whole lot of time on image augmentation and you're going to want to see is there any way to speed things up? So at the very end of that, that class notebook we went through, Jeremy actually taught us how to use Pytorch JIT to implement batch image augmentations using the GPU. And what we did was we wrote one script, and it was for image rotations. And it would take like our, my batch size was 64. So it would take 64 images rotate each image at random all at the same time on the GPU. And it ran, obviously, much faster than doing it image by image. And so that was kind of why I was and then I had been attending the in person study group up at USF during, while the course was going on. And one day in study group, Jeremy kind of mentioned that Nvidia had this library called Dali. And he showed us;

Sanyam Bhutani  5:40  
Did you attend it in person?

James Dellinger  5:43  
What?

Sanyam Bhutani  5:43  
 Did you attend the part two in person?

James Dellinger  5:45  
I did attend in person. 

Sanyam Bhutani  5:46  
Yeah. Okay. 

James Dellinger  5:48  
And so, he kind of mentioned in brief in passing, there was this library from NVIDIA, and he showed us this notebook than he and Sylvian were working on but we didn't really have time to dive in it too deeply. And then we kind of moved on to the other notebooks and wrapped up. And so a couple of weeks ago when I was here at home, going through the notebooks on my own, I said, I remembered that Dali and I was like, I want to see if I can, how does it compare with doing Pytorch JIT, was kind of what I wanted to find out. And I wanted to know like, is it is it is hard to implement? Do I have to write a script from scratch with 12 lines? Am I going to have to do a lot of research on forums? Because I don't really know how to use JIT, if I wanted to do my own, I'd have to, you know, figure it all out from scratch, or is there kind of a high level API? And so I went to NVIDIA's website for Dali, all that has the documentation for the library and stuff. And it looked really great. They were like, yeah, it obviously runs really fast. Because doing image augmentation, as a batch on the GPU is kind of a no brainer. Like, it just makes sense. To do a bunch of images at the same time, is always better. So I was definitely sold on that. And then it said that it's also like has all the image transforms at least the kind of the mainstream ones. And it's a really easy to use API. So I gave it a try. And then we can kind of go into more of the specifics of what that was like. But that was sort of how I got to the point of like, why am I exploring this.

Sanyam Bhutani  7:20  
Got it. Also uh, like you mentioned pillow, Like do you know if it's just faster because I know for sure that fast.ai bakes in all of the best practices.

James Dellinger  7:29  
Sure.

Sanyam Bhutani  7:30  
Most of the cutting edge practices. So like, people are getting imagenet it all of the time, even before Dali came out, right? So was it using just pillow or JIT like scripts?

James Dellinger  7:43  
That's a great question. I don't know if I were to guess I'd say like, given that we started off learning how to do it in pillow, and I don't remember for sure, but I think I remember Jeremy saying like most people don't do it on the GPU. 

Sanyam Bhutani  7:56  
Okay.

James Dellinger  7:57  
My impression was most people are just doing it on the CPU.

Sanyam Bhutani  8:00  
Yeah, even the fast.ai library I believe it, it's on the CPU, so far. 

James Dellinger  8:05  
So sure, yeah. Yeah. Right. 

Sanyam Bhutani  8:09  
And the main reason we want to use the GPU is because it's it's too damn fast compared to the CPU. And like, we definitely want that to remove that bottleneck of part two that comes out in June end is going to be talking about all of these cutting edge practices, like how do we remove this bottleneck? How do we make sure that we're using every performance out of our model? So I guess that's one of the reasons why we want to use this.

James Dellinger  8:37  
For Sure, For Sure.

Sanyam Bhutani  8:41  
And another point I want to ask you is like, if Dali isn't there, how is just using a JIT or just in time compiler by Pytorch like, for this? 

James Dellinger  8:52  
Yes. So that's like, so definitely, if I ever wanted to do an image rotation on on the GPU as a batch. I know I can do it because I already have that implemented. So I've got this great template from fast.ai from the course. But if I were to want to say implement warp a theme transforms from scratch. I'm sure it's possible. But it's not clear to me exactly, for someone like me, who's maybe has about two years of Python experience, how how, and maybe like, I have probably about a year of Pytorch experience, it's not clear to me how long it would take me to kind of learn the ropes of JIT and kind of use that image rotation thing as a template. And then glean as much as that as I could, but then do kind of a completely different, a different algorithm. I know Jeremy has mentioned a JIT can be kind of finicky. He says it's improving rapidly getting better like day by day. But when we saw that, that script in class, he'd, he'd kind of already gotten to the point of like, he'd already debugged it, and he and Sylvian had published it and it sort of just worked. And so it, he intimated that getting to that point of having a quote unquote, just working was not is super easy. Now how long that would be for me like I don't know, but it would definitely be an interesting, interesting exercise.

Sanyam Bhutani  10:17  
Interesting. So tell us more about Dali, what, I know you run a lot of experiments on in. And how's that been an experience compared to the JIT script you mentioned? I know that it must have been a lot better in terms of performance compared to pillow.

James Dellinger  10:37  
Sure.

Sanyam Bhutani  10:37  
How about the general interface and the general explorations? 

James Dellinger  10:44  
Sure. So I felt like the general interface of Dali was awesome. I felt like it was really straightforward. Working with Dali, you have to do two things. You have to write a pipeline class, and that basically it's a class and it's sub classes are, it inherits from a Dali class, it's actually just called pipeline. And inside your class, you just include a line for basically every transform, you want to call. And you're in that pipeline class, it has two methods that you need to define. The first is just a normal init method. And in that you just create objects or variables for each of your transforms, say, image, rotation, imagery, size, etc. Then there's one more method called defined graph. And if you've ever worked with like TensorFlow or the 1.0, this might start to feel familiar for you. You just kind of, you put in a line for each of your rotations in the order you want them to be called.

Sanyam Bhutani  11:44  
So it's more on the side of static graphs from;

James Dellinger  11:48  
Very much static graph.

Sanyam Bhutani  11:49  
Okay, so so it's more Tensorflow-ey than PyTorch-ey?

James Dellinger  11:53  
Right. And so the good another good thing about Dali is it works really well with, well, I didn't try it with TensorFlow. But all the examples on the on the website and the documentation you need to believe in works kind of just works with TensorFlow. And I felt like it kind of just works with Pytorch. It's supposed to return data loader objects that Pytorch can can understand. Getting to work with fast.ai, especially the library that we we were building from scratch during part two. 

Sanyam Bhutani  12:22  
Yeah.

James Dellinger  12:23  
That was that required a lot. Probably like 10 hours of troubleshooting time on my own, not because it was really complex, what I had to do, it just need a while to kind of go through and figure out where is it breaking? Okay, where is it breaking now? Like, what class do I need to go back and fix and it actually turned out, there were just three simple fixes I had to do to get Dali to work really great with the fast.ai that we've been working with in part two, but it was that exercise of going in and hunting down. What I had to do that took me personally a while to kind of figure out what was going on. A nice side effect of that was that because I did that, it really forced me to understand the ins and outs in the plumbing of all of those classes we are implementing from scratch during fast.ai part two.

Sanyam Bhutani  13:10  
Got it. So I assume like this would also require a lot of expertise with the library that you must have gotten to by going through the part two, and how about interfacing with it in general? So for example, I'm working on a pipeline artwork, and I want to just maybe give this library a shot, what are your thoughts on that?

James Dellinger  13:34  
So if you have, I feel like I plan to use Dali going forward, but I would use it for early prototyping when I just want to,  when speed is kind of the name of the game. And I'll get into why in a second that is. If I were kind of, if it were the final week of a Kaggle competition, and I just wanted to squeeze out that extra like, point 0001 point of performance or something like on my CV um, I probably wouldn't use it and here's why, um, Dali does support kind of all the basic transforms. Rotation, image resize or random crop and resize which Jeremy, Jeremy mentioned is the most important one because he said all the image net winners like as a rule since like 2013 I think it was, have some form of random crop and resize.

Sanyam Bhutani  14:25  
Yeah. 

James Dellinger  14:26  
Each one of them master augmentation and then some of the other ones vary. But, but my goal with working with Dali was to see like okay, does this at least have the augmentations that we implemented from scratch in part two, and so that was random prop and resize. It was image rotation. It was warp transform, and it was flips like random horizontal or vertical and so and and, and also each of these in order for me to consider Dali like quote unquote 'supporting it', it needed to support randomized up, meaning if I said rotate images, I can give you like, a range of angles. And for each and every image in the in a batch, it will kind of do it just a little bit different, because that's what we had implemented from scratch. 

Sanyam Bhutani  15:14  
And like, could you elaborate on why is the randomness useful? So if if I just give one angle why is that not enough? Or if I just put you in a few angles 30 degree 40 degree, why isn't that;

James Dellinger  15:26  
Sure. So um having randomized output will help iron the model that we trained to generalize better to the test set or to brand new unseen things. And so the reason for that is if I'm, if I'm always like, looking at, say, a truck, and the truck is always like, not tilted, not anything. Then my model can get really good at understanding what a truck looks like. But the truck better not be tilted, better not be fuzzy. It better just be like a perfect picture but in real life, you know, maybe the camera tilts a little bit or maybe like you're looking at from underneath and, and really like you could be looking at him at an image from any sort of angle. Who knows? 

Sanyam Bhutani  16:09  
Yeah. 

James Dellinger  16:10  
So that's why when we do these transforms, we like to perturb them a little bit, maybe tilt the image this way or that way, rotate it this way or that way. And kind of, at least for it's a little bit different for other image that's like satellite imagery, but at least for the image, that stuff, when it's kind of an image that's just facing you. We like to see what it's like to look at it through kind of different lens or different angles. We don't really like to, we don't want to perturb it, we don't want to squish it or stretch it in a way that would be really unnatural that like you would rarely never have an application where you needed to first take a truck and stretch it really wide and then like know that it's a truck.

Sanyam Bhutani  16:52  
Yeah. So again, I assume you'd have to have the knowledge of all of these transforms and maybe some expertise. Or atleast knowledge of your data and like, understanding of how these transforms would apply to it and give you all.

James Dellinger  17:06  
Absolutely yeah, that's definitely one thing that Dali, Dali has it all in there, but it won't, it doesn't have any sort of logic to kind of know, oh, what kind of image net or what kind of image set you have and what kind of transforms you might or might not want. It's definitely not at that level, it kind of expects you to be more, more of a practical practitioner who's kind of knows what transforms you want to use ahead of time.

Sanyam Bhutani  17:30  
So contrary to fast.ai, because fast.ai, I've done a deep dive into the existing ones, and it has good enough defaults for most of the cases so fast.ai takes care of the defaults for you, which, like you said, just works, quote unquote, for most;

James Dellinger  17:45  
For sure. 

Sanyam Bhutani  17:46  
You mentioned about bouncing out performance. For example, during the last week of Kaggle, or if you're putting this into production, you definitely want to use most of the performance. So; 

James Dellinger  17:58  
Right.

Sanyam Bhutani  17:59  
Usually we'd be using I believe C++ pipelines for that. Or some other alternative. Do you think this could be used in production as an alternative to it? Or it, is it still in the early stages for that?

James Dellinger  18:15  
If I, my understanding tells me there is no reason why it couldn't be used in production. The reason is because the although we, you know, tell Dali, what kind of pipeline we want to have, and how we and we pass hyper parameters to the different rotation transforms. We do all of that in a Python API. 

Sanyam Bhutani  18:37  
Yeah.

James Dellinger  18:37  
It's, everything is written in C++, and everything is optimized for the GPU. So at least say assuming the again, if we're talking some well, actually the the documentation on our website did seem to indicate that even if you had a bunch of like GPUs, it would work with all of them and figure out how to distribute the the augment the batch across all the GPUs.

Sanyam Bhutani  19:01  
Mentioned is that manual or that that's taken care by the library?

James Dellinger  19:07  
Good question. I don't know. I don't know, I would have to go back and double check. And again, I do remember saying that it's supported several GPUs but unfortunately for my experiment experiments, I just ran it on one GPU, so I wouldn't know if they would automatically know all of that or if you'd have to say, hey Dali, here, all my GPUs. It just knows to go in and check and see like, if like, check in your system and or your CUDA and see what you have.

Sanyam Bhutani  19:32  
Got it. Um, another thing you've done is you've really benchmarked this and you see the results in your blog post also. So I believe you benchmark this against both JIT and pillow. 

James Dellinger  19:48  
Hmm. 

Sanyam Bhutani  19:50  
How does it compare to both of these in your experience?

James Dellinger  19:53  
Sure. So it was tough for me to I guess do a true apples to apples benchmark with pillow because with pillow we kind of go image by image. And pillow on average, most of those transforms would do one image in like a couple hundred microseconds. And for when I timed Dali, I could see how long it took to run all of my transforms on one batch of 64 images. And if I did like the dimensional analysis and divided by number of transforms divided by number of images, it worked out to something like 600 microseconds per image. So in that regard, you could say it's, it's quite comparable. However, one thing we found was that with pillow, the main bottleneck is loading up those images for the first time. Once you have the image loaded and you do transforms, it's fast, but just loading up that image initially can take like five microseconds at least on my again on my AWS. And to me that just was like yeah, if I'm taking this long to load each image then, I could never, pillow would never even approach Dolly

Sanyam Bhutani  21:05  
To our listeners millisecond might sound a very small like a very small gap, but when you when you're doing billions of images, it really gets accounted for, you'd want that to be micro, if not, hopefully sometimes nanoseconds

James Dellinger  21:19  
For sure. And then as far as comparison with PyTorch JIT, I found that it was so PyTorch JIT could do the one batch of 64 image rotations, in then about 4.3 milliseconds. And it took my Dali pipeline of say, I think it was about 11 or 12 operations. Of course, not every operation was run on each image because again, these were applied randomly, some images would get rotated and some wouldn't for example, but all in all, it took one batch of with about 12 operations about, like 40 milliseconds. And if I divided that by like, say, again back of the envelope, but just divide by 10, or that's like four seconds per operation. And that's kind of reminds me of my 4.3 milliseconds it took to do my rotation on my one batch using Pytorch jet. So I felt, you know, I can just say it seems pretty comparable. That's pretty similar. And that was pretty reassuring to see. I, we should probably get to sort of like the one kind of at this point, I think folks that are listening are going, okay, great. Dali is like perfect. It's like ready to go. It's easier to implement probably than a jet script. So why wouldn't anyone use it?

Sanyam Bhutani  22:45  
Oh, from what I have kind of understood it, like we've also mentioned that it's more TensorFlow-y?. So for for our fast.ai friends or people from, people who are fans of the Pytorch approach, the TensorFlow two point approach, also how, like do do you think Dali is a good compliment to that?

James Dellinger  23:06  
Um, you know, it's it's funny, I guess like, no it's not, if you're super in on Pytorch dynamic mode. For me, I mean, I, I have pretty much been only doing Pytorch for the past year before that I had some intro experience with TensorFlow. And then kind of, I'm much more into user friendliness. So then once I got the Pytorch, I was like, okay, I don't see why I would go back to the old at least TensorFlow one point on style. But even for me, who had a little bit of that experience with static, creating the static computation graph I handled like, it was sort of a little bit of a, you call it culture shock. Getting back into doing that again with Dali, not not not surmountable, it was told are not insurmountable. It is totally doable. It just took me kind of getting through the frustration and some and some strange like runtime errors.

Sanyam Bhutani  24:04  
Got it. So, how I look at is like to conclude, Dali does have all of these great options, there are some non ironed out ends for example, you can't do randomizations. But could you envision it like if you really, if you're working with this you might be at expert level or you really, really trying to put this into production? So do you think it's a good option just for that particular use case? Maybe not for an experimental list?

James Dellinger  24:34  
Right? Um, unfortunately, maybe the what, given where the library is currently at, the one irony might be, while it's not suitable for say someone who's a domain expert and kind of picking up maybe they're a few months into their deep learning journey, you and maybe they're not quite ready to pick each and every image augmentation on their own; that kind of person, Dali is not suitable for them yet, because Dali really wants you to know exactly what you're going to do, and in what order you're going to do it, and what parameters you're going to pass to it, such as I want to rotate my images between plus or minus 30 degrees or plus or minus seven, like you, the practitioner need to know that.

Sanyam Bhutani  25:17  
It's very much.

James Dellinger  25:19  
So that kind of in my mind, that means like, okay, Dali really isn't appropriate for just kind of domain experts fresh into deep learning. Ironically, on the flip side, I would say Dali isn't really appropriate for experts who want to put a cutting edge model into production.

Sanyam Bhutani  25:36  
Okay.

James Dellinger  25:36  
What I mean by that is because and this is, was in my mind, the biggest kind of downside of Dali, not all augmentation support randomized output. So we spent some time a few moments ago talking about image rotations and, and why we want it to be random so you can see all the different angles. You know, our model can learn to look at a truck from all the way around. Well, it turns out well like horizontal flip, vertical flips, image rotations, they all support randomness.

Sanyam Bhutani  26:07  
The most common ones that are.

James Dellinger  26:09  
Sure warp a theme transforms our perspective tilt. And a warp a theme is basically something that makes it look like, imagine you have a framed image on your wall. And that image begins to kind of like or the painting begins to tilt toward you or tilt away from you.

Sanyam Bhutani  26:24  
For example, if my, if I have my Iphone, and that's getting tilted in a 2d image, got it.

James Dellinger  26:31  
For sure, and so what what that does is it's still a 2d image, but it looks kind of like oh, this truck is sort of falling, falling out of the screen. We would like those to be randomized as well. And Dali doesn't support that.

Sanyam Bhutani  26:46  
Because we're fast.ai fellows and we're talking 2019 cutting edge deep learning and these I think, most of the cutting edge practices that that are being followed everywhere in 2019, for sure.

James Dellinger  26:57  
For sure. And so it's, for that, and I think there may be one or two other transforms that Dali also doesn't support randomization. And so this was kind of this was the point at which, when I went through and built my own pipeline, I was like, okay, I want to implement each of these four transforms that we learned how to do in fast.ai. Okay. And then instead of just like, I didn't want to skip having warp transforms or perspective warps, I wanted to do it somehow and I thought, okay, fine. Dali doesn't have logic to randomize it but you know, no problem. I'll write my own, we kind of learned how to do that even in class. I'll just write my own Python method and just call it you know, at runtime from within my pipeline. No dice. Do not try that. That is not static graph programming. As I painfully found out, and what I kind of after hitting my head against that wall a few times I went to the Dali website and saw that, it was an issue, folks had even requested that the team add randomization to some of these other transforms. And the response from the team was we don't have the bandwidth to do that right now. If you want, you can create your own custom operation. And I said, okay, let me know how do I do that? And it turned out, Dali really is written in C++, which I think is a good thing, because it means it's optimized for the GPU. 

Sanyam Bhutani  28:26  
Yeah.

James Dellinger  28:26  
It's optimized for CUDA. But for me, personally, it was a bit of a bad thing because I was like, okay, I need to write this logic in C++, compile it, and then import it. So I can use it in my Dali computation graph. And then that was a little bit more than I wanted to invest.

Sanyam Bhutani  28:46  
I think that's even a stretch for most of the practioners, because the production use cases I believe, if you're talking to different frameworks, we might expect another team to be taking care of the C++ side of things.

James Dellinger  29:02  
For sure, so that was kind of the main downside.

Sanyam Bhutani  29:05  
Got it. So how I imagine this would be you already, like if you're doing experiments, mini experiments or such CPUs would be good enough for you. But if you're working on a problem that works just for and you just need the augmentations that are the inside of this library, it could be a great option for you. 

James Dellinger  29:29  
I definitely, I definitely think so. If you just went into like, you know, say you just didn't even really want that many image transforms. You just wanted to resize your images or do random crop and resize. 

Sanyam Bhutani  29:42  
Yeah. 

James Dellinger  29:43  
Then it, that might really be enough.

Sanyam Bhutani  29:46  
For most of the use cases, I believe the generic apps, hot dog, not hot dog, whatever will be the thing. These are good enough, I think.

James Dellinger  29:55  
Sure, for sure.

Sanyam Bhutani  29:57  
And maybe like, again, it's, the website is, you would want to put this on a GPU. So usually when we're talking production, everyone is putting it on a CPU not a GPU. So it's really for the practitioner who wants to train on a million images. 

James Dellinger  30:13  
Correct. 

Sanyam Bhutani  30:14  
Just these options that are currently supported I believe. In the future hopefully be might roll these out.

James Dellinger  30:21  
You know, it, uh, it was it was interesting the ticket for adding randomization to the warp of things on the GitHub, the team, definitely I could tell that they looked at it and but it seemed like at least for the time being, they made a firm decision not to do it. So, but I mean, on the bright side, they're totally welcoming contributors to come in and add it to the library. So if you know C++ and you want to learn a little C++ and you really have a passion for building image transforms from scratch, I think they would, they would really welcome it. But I didn't, the ticket was closed in February and it's not clear to me that it's coming in the next version who knows it man, but;

Sanyam Bhutani  31:03  
Maybe in a future studies, hopefully. 

James Dellinger  31:06  
Sure, sure. 

Sanyam Bhutani  31:07  
Got it. Thank you so much James for for that amazing introduction. I have James original blog post linked in all the descriptions along with his experiments. You can also find James on Twitter, or any other place where we can follow your activities?

James Dellinger  31:30  
I will just yeah, medium, Twitter mostly these two best places. Yeah.

Sanyam Bhutani  31:36  
Oh, James also has an amazing blog post blog. I encourage you to check out other blog posts as well. We'll have all of those links in the description of both the YouTube video and the podcast description.

James Dellinger  31:50  
Sounds good.

Sanyam Bhutani  31:52  
Thanks again James for joining me today.

James Dellinger  31:54  
Thank you so much Sanyam, it was awesome.

Sanyam Bhutani  32:06  
Thank you so much for listening to this episode. If you enjoyed the show, please be sure to give it a review or feel free to shoot me a message. You can find all of the social media links in the description. If you like the show, please subscribe and tune in each week to "Chai Time Data Science".

